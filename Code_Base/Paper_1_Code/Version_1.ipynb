{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install streamlit streamlit-chat langchain transformers sentence-transformers faiss-cpu pypdf2 torch pyngrok\n",
        "!pip install -q transformers einops accelerate\n",
        "!pip install pytesseract\n",
        "!apt-get install tesseract-ocr"
      ],
      "metadata": {
        "id": "uHnV6EUjWag7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wilBbMvbl7m",
        "outputId": "3b155683-c264-41c7-abf0-11679e43e57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.11)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.14)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.29)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.25.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (0.3.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (2.10.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.25.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.14 marshmallow-3.25.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pGsLS9FcTpT",
        "outputId": "a7211d8b-ce34-4dc2-995c-26adc1459490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Mount Google Drive (if needed)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBIbJJFQXIXQ",
        "outputId": "96c8c345-88f7-4502-9001-0233cd45b7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Main Application Code\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import gc\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import tempfile\n",
        "import logging\n",
        "import time\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ResearchPaperChatbot:\n",
        "    def __init__(self, model_name: str = \"distilgpt2\"):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        st.info(f\"Using device: {self.device}\")\n",
        "        self.setup_model(model_name)\n",
        "        self.setup_embeddings()\n",
        "        self.vector_store = None\n",
        "        self.chain = None\n",
        "        self.memory = None\n",
        "\n",
        "    def setup_model(self, model_name: str):\n",
        "      with st.spinner(\"Loading language model...\"):\n",
        "          try:\n",
        "              print(f\"Loading model: {model_name}\")\n",
        "              # Initialize tokenizer\n",
        "              self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "              # Load model\n",
        "              self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                  model_name,\n",
        "                  device_map=\"auto\",\n",
        "                  torch_dtype=torch.float32,\n",
        "                  low_cpu_mem_usage=True\n",
        "              )\n",
        "\n",
        "              print(\"Setting up pipeline...\")\n",
        "              # Setup pipeline\n",
        "              self.pipe = pipeline(\n",
        "                  \"text-generation\",\n",
        "                  model=self.model,\n",
        "                  tokenizer=self.tokenizer,\n",
        "                  max_new_tokens=200,\n",
        "                  do_sample=True,\n",
        "                  temperature=0.7,\n",
        "                  top_p=0.95,\n",
        "                  return_full_text=False\n",
        "              )\n",
        "\n",
        "              print(\"Creating LLM...\")\n",
        "              # Create LLM\n",
        "              self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
        "\n",
        "              st.success(\"Model loaded successfully!\")\n",
        "              print(\"Model setup complete\")\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"Model setup error: {e}\")\n",
        "              st.error(f\"Error loading model: {str(e)}\")\n",
        "              raise\n",
        "\n",
        "    def setup_embeddings(self):\n",
        "        with st.spinner(\"Setting up embeddings...\"):\n",
        "            try:\n",
        "                self.embeddings = HuggingFaceEmbeddings(\n",
        "                    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "                    model_kwargs={'device': self.device}\n",
        "                )\n",
        "                st.success(\"Embeddings setup complete!\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error in setup_embeddings: {str(e)}\")\n",
        "                st.error(f\"Error setting up embeddings: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "    def process_pdf(self, pdf_file):\n",
        "        try:\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
        "                tmp_file.write(pdf_file.getvalue())\n",
        "                tmp_file_path = tmp_file.name\n",
        "\n",
        "            loader = PyPDFLoader(tmp_file_path)\n",
        "            documents = loader.load()\n",
        "\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=500,\n",
        "                chunk_overlap=50,\n",
        "                length_function=len,\n",
        "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "            )\n",
        "\n",
        "            texts = text_splitter.split_documents(documents)\n",
        "            print(f\"Split PDF into {len(texts)} chunks\")\n",
        "\n",
        "            os.unlink(tmp_file_path)\n",
        "            return texts\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing PDF: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def initialize_chain(self, texts):\n",
        "        with st.spinner(\"Creating vector store and initializing chain...\"):\n",
        "            try:\n",
        "                print(\"Creating vector store...\")\n",
        "                self.vector_store = FAISS.from_documents(texts, self.embeddings)\n",
        "\n",
        "                retriever = self.vector_store.as_retriever(\n",
        "                    search_kwargs={\n",
        "                        \"k\": 3,\n",
        "                        \"fetch_k\": 5\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                self.memory = ConversationBufferMemory(\n",
        "                    memory_key=\"chat_history\",\n",
        "                    return_messages=True,\n",
        "                    output_key='answer'\n",
        "                )\n",
        "\n",
        "                self.chain = ConversationalRetrievalChain.from_llm(\n",
        "                    llm=self.llm,\n",
        "                    retriever=retriever,\n",
        "                    memory=self.memory,\n",
        "                    return_source_documents=True,\n",
        "                    verbose=True\n",
        "                )\n",
        "\n",
        "                print(\"Chain initialized successfully\")\n",
        "                st.success(\"Document processed successfully!\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in initialize_chain: {str(e)}\")\n",
        "                st.error(f\"Error processing document: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "    def generate_summary(self):\n",
        "      try:\n",
        "          print(\"Starting summary generation...\")\n",
        "          if not self.vector_store:\n",
        "              return \"Please upload a paper first.\"\n",
        "\n",
        "          # Get document content\n",
        "          docs = self.vector_store.similarity_search(\n",
        "              \"main points of this research paper\",\n",
        "              k=3\n",
        "          )\n",
        "\n",
        "          # Combine content\n",
        "          content = \" \".join([doc.page_content for doc in docs])\n",
        "          print(f\"Retrieved content length: {len(content)}\")\n",
        "\n",
        "          # Simple prompt\n",
        "          input_text = f\"Summarize this research paper: {content}\"\n",
        "\n",
        "          try:\n",
        "              print(\"Generating summary...\")\n",
        "              # Use pipeline directly\n",
        "              result = self.pipe(\n",
        "                  input_text,\n",
        "                  max_new_tokens=200,\n",
        "                  do_sample=True,\n",
        "                  temperature=0.7,\n",
        "                  top_p=0.95,\n",
        "                  num_return_sequences=1,\n",
        "                  return_full_text=False\n",
        "              )\n",
        "\n",
        "              print(\"Generation completed\")\n",
        "              print(f\"Result type: {type(result)}\")\n",
        "              print(f\"Result content: {result}\")\n",
        "\n",
        "              # Extract the generated text\n",
        "              if result and isinstance(result, list) and len(result) > 0:\n",
        "                  if isinstance(result[0], dict) and 'generated_text' in result[0]:\n",
        "                      return result[0]['generated_text'].strip()\n",
        "                  else:\n",
        "                      return str(result[0]).strip()\n",
        "              else:\n",
        "                  return \"Could not generate summary.\"\n",
        "\n",
        "          except Exception as gen_error:\n",
        "              print(f\"Generation error: {gen_error}\")\n",
        "              return f\"Generation error: {str(gen_error)}\"\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Summary error: {e}\")\n",
        "          return f\"Summary error: {str(e)}\"\n",
        "\n",
        "    def ask_question(self, question: str):\n",
        "        try:\n",
        "            if not self.chain:\n",
        "                return \"Please upload a paper first.\"\n",
        "\n",
        "            response = self.chain({\"question\": question})\n",
        "            return response['answer']\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {str(e)}\")\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    def format_summary(self, summary):\n",
        "        sections = [\"Objectives:\", \"Methodology:\", \"Key Findings:\", \"Conclusions:\"]\n",
        "        formatted = \"Research Paper Summary\\n\\n\"\n",
        "\n",
        "        lines = summary.split('\\n')\n",
        "        current_section = \"\"\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if any(section.lower() in line.lower() for section in sections):\n",
        "                current_section = line\n",
        "                formatted += f\"\\n{current_section}\\n\"\n",
        "            elif line:\n",
        "                formatted += f\"{line}\\n\"\n",
        "\n",
        "        return formatted\n",
        "\n",
        "\n",
        "    def save_to_drive(self, file_path: str, drive_path: str):\n",
        "        try:\n",
        "            drive_full_path = f\"/content/drive/My Drive/{drive_path}\"\n",
        "            os.makedirs(os.path.dirname(drive_full_path), exist_ok=True)\n",
        "            shutil.copy2(file_path, drive_full_path)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error saving to Drive: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def manage_memory(self):\n",
        "      \"\"\"Clean up memory and GPU cache\"\"\"\n",
        "      try:\n",
        "          # Clear CUDA cache if using GPU\n",
        "          if torch.cuda.is_available():\n",
        "              torch.cuda.empty_cache()\n",
        "\n",
        "          # Clear conversation memory if exists\n",
        "          if hasattr(self, 'memory') and self.memory is not None:\n",
        "              self.memory.clear()\n",
        "\n",
        "          # Perform garbage collection\n",
        "          gc.collect()\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error in manage_memory: {str(e)}\")\n",
        "\n",
        "def initialize_session_state():\n",
        "    if 'chatbot' not in st.session_state:\n",
        "        st.session_state.chatbot = None\n",
        "    if 'messages' not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "    if 'paper_processed' not in st.session_state:\n",
        "        st.session_state.paper_processed = False\n",
        "\n",
        "def main():\n",
        "    st.title(\"Research Paper Analysis Chatbot (Colab Version)\")\n",
        "\n",
        "    initialize_session_state()\n",
        "\n",
        "    with st.sidebar:\n",
        "        st.header(\"Settings\")\n",
        "\n",
        "        model_options = {\n",
        "            \"GPT-2\": \"gpt2\",\n",
        "            \"DistilGPT2\": \"distilgpt2\",\n",
        "            \"BERT\": \"bert-base-uncased\"\n",
        "        }\n",
        "        selected_model = st.selectbox(\n",
        "            \"Select Language Model\",\n",
        "            list(model_options.keys())\n",
        "        )\n",
        "\n",
        "        source = st.radio(\"Select source:\", [\"Upload File\", \"Google Drive\"])\n",
        "\n",
        "        if source == \"Upload File\":\n",
        "            uploaded_file = st.file_uploader(\"Upload Research Paper (PDF)\", type=\"pdf\")\n",
        "        else:\n",
        "            drive_file = st.text_input(\"Enter path in Google Drive (e.g., 'papers/research.pdf')\")\n",
        "            if drive_file:\n",
        "                full_path = f\"/content/drive/My Drive/{drive_file}\"\n",
        "                if os.path.exists(full_path):\n",
        "                    uploaded_file = open(full_path, 'rb')\n",
        "                else:\n",
        "                    st.error(\"File not found in Drive\")\n",
        "                    uploaded_file = None\n",
        "\n",
        "        if uploaded_file and not st.session_state.paper_processed:\n",
        "            try:\n",
        "                if not st.session_state.chatbot:\n",
        "                    st.session_state.chatbot = ResearchPaperChatbot(\n",
        "                        model_name=model_options[selected_model]\n",
        "                    )\n",
        "\n",
        "                with st.spinner(\"Processing paper...\"):\n",
        "                    texts = st.session_state.chatbot.process_pdf(uploaded_file)\n",
        "                    st.session_state.chatbot.initialize_chain(texts)\n",
        "                    st.session_state.paper_processed = True\n",
        "\n",
        "                st.success(\"Paper processed successfully!\")\n",
        "\n",
        "                with st.spinner(\"Generating summary...\"):\n",
        "                  print(\"\\n=== Starting Summary Process ===\")\n",
        "                  if hasattr(st.session_state.chatbot, 'memory'):\n",
        "                    st.session_state.chatbot.memory.clear()\n",
        "\n",
        "                  print(\"Calling generate_summary...\")\n",
        "                  summary = st.session_state.chatbot.generate_summary()\n",
        "\n",
        "                  print(f\"\\nSummary generation complete. Result length: {len(summary) if summary else 0}\")\n",
        "                  if summary and not isinstance(summary, str) or not summary.startswith(\"Error\"):\n",
        "                    print(\"Summary generated successfully\")\n",
        "\n",
        "                    st.session_state.messages.append({\"role\": \"assistant\",\"content\": f\"Summary of the paper:\\n\\n{summary}\"})\n",
        "                    print(\"Summary successfully added to messages\")\n",
        "                  else:\n",
        "                      print(f\"Summary generation failed: {summary}\")\n",
        "                      st.error(f\"Could not generate summary: {summary}\")\n",
        "\n",
        "            except Exception as e:\n",
        "              error_msg = f\"Error in summary generation: {str(e)}\"\n",
        "              print(error_msg)\n",
        "              st.error(error_msg)\n",
        "\n",
        "\n",
        "    st.header(\"Chat Interface\")\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "    if st.session_state.paper_processed:\n",
        "        user_question = st.chat_input(\"Ask a question about the paper\")\n",
        "\n",
        "        if user_question:\n",
        "            st.session_state.messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_question\n",
        "            })\n",
        "\n",
        "            with st.chat_message(\"user\"):\n",
        "                st.write(user_question)\n",
        "\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    response = st.session_state.chatbot.ask_question(user_question)\n",
        "                    st.write(response)\n",
        "\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": response\n",
        "                    })\n",
        "\n",
        "                    # Periodic memory cleanup\n",
        "                    if len(st.session_state.messages) % 5 == 0:\n",
        "                        st.session_state.chatbot.manage_memory()\n",
        "    else:\n",
        "        st.info(\"Please upload a research paper to start the conversation.\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"\"\"\n",
        "    ### How to use:\n",
        "    1. Select a language model from the sidebar\n",
        "    2. Choose file source (Upload or Google Drive)\n",
        "    3. Upload or select your research paper\n",
        "    4. Wait for the initial summary\n",
        "    5. Ask questions about the paper\n",
        "\n",
        "    ### Tips:\n",
        "    - Be specific in your questions\n",
        "    - You can ask about methods, results, conclusions, etc.\n",
        "    - The bot remembers conversation context\n",
        "    \"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTmV1TDWW6rE",
        "outputId": "5ec6a1f7-8b46-4281-c139-3b4c0a527c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Setup ngrok authentication\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "JoSvk67_Wly7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2riOJUDurYrJk5mxROf2umSvvMB_7itD6GvxQWtksw2VtvHLG\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az__SvgAWoG0",
        "outputId": "b17c6e24-dc65-4098-eafb-0938b2683f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def run_app():\n",
        "       !streamlit run app.py &>/content/logs.txt &\n",
        "       !killall ngrok\n",
        "       # Disconnect existing tunnels before connecting a new one\n",
        "        #  !pkill -f \"ngrok http\"  # or\n",
        "\n",
        "       # Wait for ngrok to shut down\n",
        "       time.sleep(5)\n",
        "\n",
        "       public_url = ngrok.connect(addr='8501')\n",
        "       print(f\"Public URL: {public_url}\")"
      ],
      "metadata": {
        "id": "w8ARhu8zW9QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84Igt15NhQ-M",
        "outputId": "572b3e3e-9fbe-425d-8d9e-fb35c56509c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok: no process found\n",
            "Public URL: NgrokTunnel: \"https://0048-34-143-249-121.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!killall ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg3PUR8mfs5M",
        "outputId": "27e16ad0-323a-405e-8dbe-02e9ef3d3374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok: no process found\n"
          ]
        }
      ]
    }
  ]
}