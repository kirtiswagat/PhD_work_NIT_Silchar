{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_W8SXFHVYu0"
   },
   "source": [
    "# Load Lobraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXb0Q7w8xDlS"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy\n",
    "import pandas\n",
    "from matplotlib import pyplot\n",
    "import matplotlib\n",
    "from plotly import express\n",
    "import seaborn\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection  \n",
    "from tensorflow.keras import models \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing import image \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import applications  \n",
    "from tensorflow.keras import utils \n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.applications import resnet50 \n",
    "from google.colab import drive\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNJByk37WtUG"
   },
   "source": [
    "# Load Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4LCx_iuVNER",
    "outputId": "05151ce2-7eb2-42a4-c0d2-58e4200fdce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/Lung_Cancer\n",
      "/content/drive/MyDrive/Lung_Cancer\n",
      "['Bengin cases', 'Malignant cases', 'Normal cases']\n"
     ]
    }
   ],
   "source": [
    "def loaddata():\n",
    "  drive.mount('/content/drive')\n",
    "  lngdir = '/content/drive/MyDrive/Lung_Cancer'\n",
    "  classes = ['Bengin cases','Malignant cases','Normal cases']\n",
    "  lngpath = os.path.join(lngdir)\n",
    "  return lngdir,lngpath, classes\n",
    "lngdir,lngpath, classes=loaddata()\n",
    "print(lngdir)\n",
    "print(lngpath)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkTTcT32W0ud"
   },
   "source": [
    "# Show Class Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROM60BCV_AOK",
    "outputId": "c6c38668-78b1-4c52-bc26-d09626dcc392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Images in Malignant cases ==> 561.\n",
      "Available Images in Bengin cases ==> 120.\n",
      "Available Images in Normal cases ==> 416.\n",
      "\n",
      "Count of Images:  1097\n"
     ]
    }
   ],
   "source": [
    "def class_counts():\n",
    "  dirs=os.listdir(lngdir)\n",
    "  cnt=0\n",
    "  for i in dirs:\n",
    "      presdr=os.path.join(lngdir ,i)\n",
    "      print(\"Available Images in {} ==> {}.\".format(i,len(os.listdir(presdr))))\n",
    "      cnt+=len(os.listdir(presdr))\n",
    "  print(\"\\nCount of Images: \",cnt)\n",
    "class_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDNtc61lWzuI"
   },
   "source": [
    "# Preparing Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "id": "KkKslvoJxDle",
    "outputId": "2e78dfca-339d-41df-ec1b-0ce1a3c47ad6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"aea5fc29-0efa-44c3-84fc-faac4a7ede4a\" class=\"plotly-graph-div\" style=\"height:500px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"aea5fc29-0efa-44c3-84fc-faac4a7ede4a\")) {                    Plotly.newPlot(                        \"aea5fc29-0efa-44c3-84fc-faac4a7ede4a\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Lung_Type=%{y}<br>Count=%{text}<extra></extra>\",\"legendgroup\":\"Malignant cases\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"Malignant cases\",\"offsetgroup\":\"Malignant cases\",\"orientation\":\"h\",\"showlegend\":true,\"text\":[561.0],\"textposition\":\"auto\",\"x\":[561],\"xaxis\":\"x\",\"y\":[\"Malignant cases\"],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Lung_Type=%{y}<br>Count=%{text}<extra></extra>\",\"legendgroup\":\"Normal cases\",\"marker\":{\"color\":\"#EF553B\",\"pattern\":{\"shape\":\"\"}},\"name\":\"Normal cases\",\"offsetgroup\":\"Normal cases\",\"orientation\":\"h\",\"showlegend\":true,\"text\":[416.0],\"textposition\":\"auto\",\"x\":[416],\"xaxis\":\"x\",\"y\":[\"Normal cases\"],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Lung_Type=%{y}<br>Count=%{text}<extra></extra>\",\"legendgroup\":\"Bengin cases\",\"marker\":{\"color\":\"#00cc96\",\"pattern\":{\"shape\":\"\"}},\"name\":\"Bengin cases\",\"offsetgroup\":\"Bengin cases\",\"orientation\":\"h\",\"showlegend\":true,\"text\":[120.0],\"textposition\":\"auto\",\"x\":[120],\"xaxis\":\"x\",\"y\":[\"Bengin cases\"],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Count\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Lung_Type\"},\"categoryorder\":\"array\",\"categoryarray\":[\"Bengin cases\",\"Normal cases\",\"Malignant cases\"]},\"legend\":{\"title\":{\"text\":\"Lung_Type\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Count of Clsses\"},\"barmode\":\"relative\",\"height\":500,\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('aea5fc29-0efa-44c3-84fc-faac4a7ede4a');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-b5772e1c-fc4f-42cb-8443-37d7a501164f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lung_Image</th>\n",
       "      <th>Lung_ID</th>\n",
       "      <th>Lung_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Malignant cases/Malignant case (507).jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Malignant cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malignant cases/Malignant case (551).jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Malignant cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Normal cases/Normal case (121).jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Malignant cases/Malignant case (497).jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Malignant cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Malignant cases/Malignant case (401).jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Malignant cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Malignant cases/Malignant case (39).jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Malignant cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Normal cases/Normal case (228).jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Malignant cases/Malignant case (313).jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Malignant cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Malignant cases/Malignant case (191).jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Malignant cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Malignant cases/Malignant case (75).jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Malignant cases</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5772e1c-fc4f-42cb-8443-37d7a501164f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-b5772e1c-fc4f-42cb-8443-37d7a501164f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-b5772e1c-fc4f-42cb-8443-37d7a501164f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                 Lung_Image  Lung_ID        Lung_Type\n",
       "0  Malignant cases/Malignant case (507).jpg        1  Malignant cases\n",
       "1  Malignant cases/Malignant case (551).jpg        1  Malignant cases\n",
       "2        Normal cases/Normal case (121).jpg        2     Normal cases\n",
       "3  Malignant cases/Malignant case (497).jpg        1  Malignant cases\n",
       "4  Malignant cases/Malignant case (401).jpg        1  Malignant cases\n",
       "5   Malignant cases/Malignant case (39).jpg        1  Malignant cases\n",
       "6        Normal cases/Normal case (228).jpg        2     Normal cases\n",
       "7  Malignant cases/Malignant case (313).jpg        1  Malignant cases\n",
       "8  Malignant cases/Malignant case (191).jpg        1  Malignant cases\n",
       "9   Malignant cases/Malignant case (75).jpg        1  Malignant cases"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def image_data_creater():\n",
    "  dflst = []\n",
    "  for lng_id, fle in enumerate(classes):\n",
    "      for file in os.listdir(os.path.join(lngpath, fle)):\n",
    "          dflst.append(['{}/{}'.format(fle, file), lng_id, fle])\n",
    "  lungdf = pandas.DataFrame(dflst, columns=['Lung_Image', 'Lung_ID','Lung_Type'])\n",
    "  lungdf=lungdf.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "  ltser=lungdf['Lung_Type'].value_counts()\n",
    "  lung_cnt_df=pandas.DataFrame({\"Lung_Type\":ltser.index,\"Count\":ltser.tolist()})\n",
    "  fig = express.bar(lung_cnt_df, y=\"Lung_Type\", x=\"Count\",text=\"Count\",color=\"Lung_Type\",title=\"Count of Clsses\",height=500,width=800)\n",
    "  fig.show()\n",
    "  return lungdf\n",
    "lungdf=image_data_creater()\n",
    "lungdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5v5ynnY6xDlm",
    "outputId": "295cfafe-8d7e-4f23-9801-2368896b4907"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1097it [00:10, 105.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor Shape: (1097, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "imsz = 64\n",
    "\n",
    "def img_read(path):\n",
    "    return cv2.imread(os.path.join(lngdir, path))\n",
    "def img_resize(image, image_size):\n",
    "    return cv2.resize(image.copy(), image_size, interpolation=cv2.INTER_AREA)\n",
    "def create_pred():\n",
    "  Predictor = numpy.zeros((lungdf.shape[0], imsz, imsz, 3))\n",
    "  for idx, img in tqdm(enumerate(lungdf['Lung_Image'].values)):\n",
    "      imge = img_read(img)\n",
    "      if imge is not None:\n",
    "          Predictor[idx] = img_resize(imge, (imsz, imsz))\n",
    "  Predictor = Predictor / 255.\n",
    "  print('Predictor Shape: {}'.format(Predictor.shape))\n",
    "  return Predictor\n",
    "Predictor=create_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5t2M56dxDlq"
   },
   "outputs": [],
   "source": [
    "target = lungdf['Lung_ID'].values\n",
    "target = utils.to_categorical(target, num_classes=len(classes))\n",
    "BS = 64\n",
    "X_train, X_val, Y_train, Y_val = model_selection.train_test_split(Predictor, target, test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txrZyOP7xDlu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5dkbOBwiyYY"
   },
   "outputs": [],
   "source": [
    "aug_image = image.ImageDataGenerator(rotation_range=360, \n",
    "                        width_shift_range=0.2,\n",
    "                        height_shift_range=0.2,\n",
    "                        zoom_range=0.2,\n",
    "                        horizontal_flip=True, \n",
    "                        vertical_flip=True)\n",
    "\n",
    "aug_image.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrfyvY9IxDlw"
   },
   "outputs": [],
   "source": [
    "epc=100\n",
    "chn=3\n",
    "\n",
    "modeldeep=[\n",
    "    applications.DenseNet201(weights='imagenet', include_top=False),\n",
    "    applications.Xception(weights='imagenet', include_top=False),\n",
    "    applications.VGG16(weights='imagenet', include_top=False),\n",
    "    resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "]\n",
    "deepnames=[\n",
    "    \"DenseNet201\",\n",
    "    \"Xception\",\n",
    "    \"VGG19\",\n",
    "    \"ResNet50\"\n",
    "]\n",
    "allacc,allprec,allrecall,allf1,tracc,trlss,act,lst=[],[],[],[],[],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrzqOd5Wem_p"
   },
   "outputs": [],
   "source": [
    "def model_create(deep,nm):\n",
    "    prsdeep = deep\n",
    "    deepinp = Input(shape=(imsz,imsz, chn))\n",
    "    dplayr = layers.Conv2D(3, (3, 3), padding='same')(deepinp)\n",
    "    dplayr = prsdeep(dplayr)\n",
    "    dplayr = layers.GlobalAveragePooling2D()(dplayr)\n",
    "    dplayr = layers.BatchNormalization()(dplayr)\n",
    "    dplayr = layers.Dropout(0.5)(dplayr)\n",
    "    dplayr = layers.Dense(256, activation='relu')(dplayr)     # Change 1024 to lower value to get more accuracy-> 512 or 256\n",
    "    dplayr = layers.BatchNormalization()(dplayr)\n",
    "    dplayr = layers.Dropout(0.5)(dplayr)\n",
    "    output = layers.Dense(3,activation = 'softmax', name='root')(dplayr)\n",
    "    dpmodel =models.Model(deepinp,output,name=nm)\n",
    "    optimizer = optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=0.0)\n",
    "    dpmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    dpmodel.summary()\n",
    "    \n",
    "    return dpmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pYm7lPlhxMo"
   },
   "outputs": [],
   "source": [
    "def deep_train(deep,bsz,vb,epch,nm):\n",
    "  annealer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1, min_lr=1e-3)\n",
    "  checkpoint = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
    "  histdeep = deep.fit_generator(aug_image.flow(X_train, Y_train, batch_size=bsz),\n",
    "               steps_per_epoch=X_train.shape[0] // bsz,\n",
    "               epochs=epch,\n",
    "               verbose=1,\n",
    "               callbacks=[annealer, checkpoint],\n",
    "               validation_data=(X_val, Y_val))\n",
    "  deep.save(\"{}.h5\".format(nm))\n",
    "  deeplossVAL, deepaccVAL = deep.evaluate(X_val, Y_val)\n",
    "  deeplossTR, deepaccTR = deep.evaluate(X_train, Y_train)\n",
    "  print(\"Model Performance for {}\".format(nm))\n",
    "  print('Loss(Test): {}, Accuracy(Test): {}%'.format(deeplossVAL, round(deepaccVAL,3)*100))\n",
    "  print('Loss(Train): {}, Accuracy(Train): {}%'.format(deeplossTR, round(deepaccTR,3)*100))\n",
    "  trlss.append(round(deeplossTR,4))\n",
    "  tracc.append(round(deepaccTR,4)*100)\n",
    "  act.append(round(deeplossVAL,4))\n",
    "  lst.append(round(deepaccVAL,4)*100)\n",
    "  return deep,histdeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wcvnDCAshxQO"
   },
   "outputs": [],
   "source": [
    "def deep_predict(deep,nm):\n",
    "  model_predicted = deep.predict(X_val)\n",
    "  model_predicted = numpy.argmax(model_predicted, axis=1)\n",
    "  Actual = numpy.argmax(Y_val, axis=1)\n",
    "  cm = sklearn.metrics.confusion_matrix(Actual, model_predicted)\n",
    "  pyplot.figure(figsize=(10,5))\n",
    "  pyplot.title(\"Confusion Matrix for {}\".format(nm))\n",
    "  ax = seaborn.heatmap(cm, fmt=\"d\",annot=True,cmap=\"plasma\", xticklabels=classes, yticklabels=classes)\n",
    "  ax.set_ylabel('Actual', fontsize=25)\n",
    "  ax.set_xlabel('Predicted', fontsize=25)\n",
    "  pyplot.show()\n",
    "  allacc.append(round(sklearn.metrics.accuracy_score(Actual,model_predicted),2)*100)\n",
    "  allprec.append(round(sklearn.metrics.precision_score(Actual,model_predicted,average=\"weighted\"),2)*100)\n",
    "  allrecall.append(round(sklearn.metrics.recall_score(Actual,model_predicted,average=\"weighted\"),2)*100)\n",
    "  allf1.append(round(sklearn.metrics.f1_score(Actual,model_predicted,average=\"weighted\"),2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pSI5z9qcenGI",
    "outputId": "ae35074d-1712-46bf-f43b-16183ad09f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DenseNet201\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_34 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 64, 64, 3)         84        \n",
      "                                                                 \n",
      " densenet201 (Functional)    (None, None, None, 1920)  18321984  \n",
      "                                                                 \n",
      " global_average_pooling2d_9   (None, 1920)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " batch_normalization_42 (Bat  (None, 1920)             7680      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 1920)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               491776    \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " root (Dense)                (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,823,319\n",
      "Trainable params: 18,589,911\n",
      "Non-trainable params: 233,408\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.9244 - accuracy: 0.3506\n",
      "Epoch 1: val_loss improved from inf to 1.19084, saving model to model.h5\n",
      "13/13 [==============================] - 41s 957ms/step - loss: 1.9244 - accuracy: 0.3506 - val_loss: 1.1908 - val_accuracy: 0.4227 - lr: 0.0020\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7660 - accuracy: 0.3506\n",
      "Epoch 2: val_loss did not improve from 1.19084\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 1.7660 - accuracy: 0.3506 - val_loss: 1.1909 - val_accuracy: 0.3455 - lr: 0.0020\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7002 - accuracy: 0.4059\n",
      "Epoch 3: val_loss improved from 1.19084 to 1.15993, saving model to model.h5\n",
      "13/13 [==============================] - 6s 461ms/step - loss: 1.7002 - accuracy: 0.4059 - val_loss: 1.1599 - val_accuracy: 0.4045 - lr: 0.0020\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.5567 - accuracy: 0.4440\n",
      "Epoch 4: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 1.5567 - accuracy: 0.4440 - val_loss: 1.2452 - val_accuracy: 0.4182 - lr: 0.0020\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.4018 - accuracy: 0.4871\n",
      "Epoch 5: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 1.4018 - accuracy: 0.4871 - val_loss: 1.4455 - val_accuracy: 0.3955 - lr: 0.0020\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2764 - accuracy: 0.5252\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 1.2764 - accuracy: 0.5252 - val_loss: 1.6306 - val_accuracy: 0.3864 - lr: 0.0020\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2755 - accuracy: 0.5240\n",
      "Epoch 7: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 1.2755 - accuracy: 0.5240 - val_loss: 1.8729 - val_accuracy: 0.3773 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1403 - accuracy: 0.5720\n",
      "Epoch 8: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 1.1403 - accuracy: 0.5720 - val_loss: 1.9650 - val_accuracy: 0.3818 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1541 - accuracy: 0.5720\n",
      "Epoch 9: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 1.1541 - accuracy: 0.5720 - val_loss: 1.9099 - val_accuracy: 0.4136 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0810 - accuracy: 0.6175\n",
      "Epoch 10: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 1.0810 - accuracy: 0.6175 - val_loss: 2.1436 - val_accuracy: 0.4182 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0283 - accuracy: 0.6125\n",
      "Epoch 11: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 206ms/step - loss: 1.0283 - accuracy: 0.6125 - val_loss: 2.3901 - val_accuracy: 0.4136 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0421 - accuracy: 0.6027\n",
      "Epoch 12: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 1.0421 - accuracy: 0.6027 - val_loss: 2.5669 - val_accuracy: 0.4227 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0385 - accuracy: 0.6273\n",
      "Epoch 13: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 206ms/step - loss: 1.0385 - accuracy: 0.6273 - val_loss: 2.3394 - val_accuracy: 0.4409 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9312 - accuracy: 0.6519\n",
      "Epoch 14: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.9312 - accuracy: 0.6519 - val_loss: 2.0319 - val_accuracy: 0.4727 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9692 - accuracy: 0.6544\n",
      "Epoch 15: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 206ms/step - loss: 0.9692 - accuracy: 0.6544 - val_loss: 1.8988 - val_accuracy: 0.4909 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8434 - accuracy: 0.6827\n",
      "Epoch 16: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 206ms/step - loss: 0.8434 - accuracy: 0.6827 - val_loss: 1.9539 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8803 - accuracy: 0.6851\n",
      "Epoch 17: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 205ms/step - loss: 0.8803 - accuracy: 0.6851 - val_loss: 1.6820 - val_accuracy: 0.5636 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7735 - accuracy: 0.6999\n",
      "Epoch 18: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 207ms/step - loss: 0.7735 - accuracy: 0.6999 - val_loss: 1.9903 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8165 - accuracy: 0.7146\n",
      "Epoch 19: val_loss did not improve from 1.15993\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.8165 - accuracy: 0.7146 - val_loss: 1.7708 - val_accuracy: 0.5773 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7811 - accuracy: 0.7103\n",
      "Epoch 20: val_loss improved from 1.15993 to 1.07528, saving model to model.h5\n",
      "13/13 [==============================] - 6s 460ms/step - loss: 0.7811 - accuracy: 0.7103 - val_loss: 1.0753 - val_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7610 - accuracy: 0.6925\n",
      "Epoch 21: val_loss improved from 1.07528 to 0.98485, saving model to model.h5\n",
      "13/13 [==============================] - 6s 465ms/step - loss: 0.7610 - accuracy: 0.6925 - val_loss: 0.9849 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7657 - accuracy: 0.7109\n",
      "Epoch 22: val_loss did not improve from 0.98485\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.7657 - accuracy: 0.7109 - val_loss: 0.9962 - val_accuracy: 0.6682 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7151 - accuracy: 0.7417\n",
      "Epoch 23: val_loss improved from 0.98485 to 0.96424, saving model to model.h5\n",
      "13/13 [==============================] - 6s 458ms/step - loss: 0.7151 - accuracy: 0.7417 - val_loss: 0.9642 - val_accuracy: 0.7182 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6926 - accuracy: 0.7405\n",
      "Epoch 24: val_loss improved from 0.96424 to 0.84995, saving model to model.h5\n",
      "13/13 [==============================] - 6s 454ms/step - loss: 0.6926 - accuracy: 0.7405 - val_loss: 0.8499 - val_accuracy: 0.7409 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6681 - accuracy: 0.7491\n",
      "Epoch 25: val_loss did not improve from 0.84995\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.6681 - accuracy: 0.7491 - val_loss: 1.1136 - val_accuracy: 0.7182 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6987 - accuracy: 0.7308\n",
      "Epoch 26: val_loss did not improve from 0.84995\n",
      "13/13 [==============================] - 3s 207ms/step - loss: 0.6987 - accuracy: 0.7308 - val_loss: 0.9059 - val_accuracy: 0.7409 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6479 - accuracy: 0.7651\n",
      "Epoch 27: val_loss improved from 0.84995 to 0.69922, saving model to model.h5\n",
      "13/13 [==============================] - 6s 452ms/step - loss: 0.6479 - accuracy: 0.7651 - val_loss: 0.6992 - val_accuracy: 0.7727 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6429 - accuracy: 0.7565\n",
      "Epoch 28: val_loss did not improve from 0.69922\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.6429 - accuracy: 0.7565 - val_loss: 0.7269 - val_accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6554 - accuracy: 0.7503\n",
      "Epoch 29: val_loss did not improve from 0.69922\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.6554 - accuracy: 0.7503 - val_loss: 0.7255 - val_accuracy: 0.7727 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6084 - accuracy: 0.7983\n",
      "Epoch 30: val_loss did not improve from 0.69922\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.6084 - accuracy: 0.7983 - val_loss: 0.7408 - val_accuracy: 0.7773 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5916 - accuracy: 0.7786\n",
      "Epoch 31: val_loss did not improve from 0.69922\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.5916 - accuracy: 0.7786 - val_loss: 0.7484 - val_accuracy: 0.7591 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5984 - accuracy: 0.7761\n",
      "Epoch 32: val_loss did not improve from 0.69922\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.5984 - accuracy: 0.7761 - val_loss: 0.8127 - val_accuracy: 0.7636 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6009 - accuracy: 0.7761\n",
      "Epoch 33: val_loss did not improve from 0.69922\n",
      "13/13 [==============================] - 3s 206ms/step - loss: 0.6009 - accuracy: 0.7761 - val_loss: 0.8074 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5588 - accuracy: 0.7884\n",
      "Epoch 34: val_loss did not improve from 0.69922\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 0.5588 - accuracy: 0.7884 - val_loss: 0.6998 - val_accuracy: 0.7636 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.7921\n",
      "Epoch 35: val_loss improved from 0.69922 to 0.63284, saving model to model.h5\n",
      "13/13 [==============================] - 6s 464ms/step - loss: 0.5634 - accuracy: 0.7921 - val_loss: 0.6328 - val_accuracy: 0.7818 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.7811\n",
      "Epoch 36: val_loss improved from 0.63284 to 0.54647, saving model to model.h5\n",
      "13/13 [==============================] - 6s 458ms/step - loss: 0.5554 - accuracy: 0.7811 - val_loss: 0.5465 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5474 - accuracy: 0.7663\n",
      "Epoch 37: val_loss did not improve from 0.54647\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.5474 - accuracy: 0.7663 - val_loss: 0.5605 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5363 - accuracy: 0.7872\n",
      "Epoch 38: val_loss did not improve from 0.54647\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.5363 - accuracy: 0.7872 - val_loss: 0.6113 - val_accuracy: 0.7909 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.8032\n",
      "Epoch 39: val_loss did not improve from 0.54647\n",
      "13/13 [==============================] - 3s 207ms/step - loss: 0.5025 - accuracy: 0.8032 - val_loss: 0.5779 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5641 - accuracy: 0.7897\n",
      "Epoch 40: val_loss improved from 0.54647 to 0.49403, saving model to model.h5\n",
      "13/13 [==============================] - 7s 522ms/step - loss: 0.5641 - accuracy: 0.7897 - val_loss: 0.4940 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5424 - accuracy: 0.7983\n",
      "Epoch 41: val_loss improved from 0.49403 to 0.47392, saving model to model.h5\n",
      "13/13 [==============================] - 6s 457ms/step - loss: 0.5424 - accuracy: 0.7983 - val_loss: 0.4739 - val_accuracy: 0.7955 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5242 - accuracy: 0.7909\n",
      "Epoch 42: val_loss did not improve from 0.47392\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.5242 - accuracy: 0.7909 - val_loss: 0.4746 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5447 - accuracy: 0.7934\n",
      "Epoch 43: val_loss did not improve from 0.47392\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.5447 - accuracy: 0.7934 - val_loss: 0.5852 - val_accuracy: 0.7955 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4634 - accuracy: 0.8253\n",
      "Epoch 44: val_loss did not improve from 0.47392\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.4634 - accuracy: 0.8253 - val_loss: 0.5802 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5043 - accuracy: 0.8007\n",
      "Epoch 45: val_loss did not improve from 0.47392\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 0.5043 - accuracy: 0.8007 - val_loss: 0.5906 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5002 - accuracy: 0.7934\n",
      "Epoch 46: val_loss did not improve from 0.47392\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 0.5002 - accuracy: 0.7934 - val_loss: 0.5441 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4992 - accuracy: 0.8032\n",
      "Epoch 47: val_loss did not improve from 0.47392\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 0.4992 - accuracy: 0.8032 - val_loss: 0.5044 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4605 - accuracy: 0.8180\n",
      "Epoch 48: val_loss improved from 0.47392 to 0.44922, saving model to model.h5\n",
      "13/13 [==============================] - 6s 477ms/step - loss: 0.4605 - accuracy: 0.8180 - val_loss: 0.4492 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4803 - accuracy: 0.8044\n",
      "Epoch 49: val_loss did not improve from 0.44922\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 0.4803 - accuracy: 0.8044 - val_loss: 0.4888 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4815 - accuracy: 0.8155\n",
      "Epoch 50: val_loss improved from 0.44922 to 0.42262, saving model to model.h5\n",
      "13/13 [==============================] - 6s 456ms/step - loss: 0.4815 - accuracy: 0.8155 - val_loss: 0.4226 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.8118\n",
      "Epoch 51: val_loss did not improve from 0.42262\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.4749 - accuracy: 0.8118 - val_loss: 0.4276 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4779 - accuracy: 0.8185\n",
      "Epoch 52: val_loss did not improve from 0.42262\n",
      "13/13 [==============================] - 3s 206ms/step - loss: 0.4779 - accuracy: 0.8185 - val_loss: 0.4359 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4299 - accuracy: 0.8253\n",
      "Epoch 53: val_loss improved from 0.42262 to 0.42142, saving model to model.h5\n",
      "13/13 [==============================] - 6s 451ms/step - loss: 0.4299 - accuracy: 0.8253 - val_loss: 0.4214 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4390 - accuracy: 0.8365\n",
      "Epoch 54: val_loss improved from 0.42142 to 0.41071, saving model to model.h5\n",
      "13/13 [==============================] - 6s 456ms/step - loss: 0.4390 - accuracy: 0.8365 - val_loss: 0.4107 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4390 - accuracy: 0.8266\n",
      "Epoch 55: val_loss did not improve from 0.41071\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 0.4390 - accuracy: 0.8266 - val_loss: 0.5415 - val_accuracy: 0.7955 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4097 - accuracy: 0.8303\n",
      "Epoch 56: val_loss did not improve from 0.41071\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.4097 - accuracy: 0.8303 - val_loss: 0.5634 - val_accuracy: 0.7955 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3899 - accuracy: 0.8475\n",
      "Epoch 57: val_loss did not improve from 0.41071\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.3899 - accuracy: 0.8475 - val_loss: 0.4532 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4261 - accuracy: 0.8257\n",
      "Epoch 58: val_loss improved from 0.41071 to 0.38622, saving model to model.h5\n",
      "13/13 [==============================] - 6s 454ms/step - loss: 0.4261 - accuracy: 0.8257 - val_loss: 0.3862 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4354 - accuracy: 0.8376\n",
      "Epoch 59: val_loss improved from 0.38622 to 0.36182, saving model to model.h5\n",
      "13/13 [==============================] - 6s 463ms/step - loss: 0.4354 - accuracy: 0.8376 - val_loss: 0.3618 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4076 - accuracy: 0.8352\n",
      "Epoch 60: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.4076 - accuracy: 0.8352 - val_loss: 0.3715 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4004 - accuracy: 0.8401\n",
      "Epoch 61: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 0.4004 - accuracy: 0.8401 - val_loss: 0.4017 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3945 - accuracy: 0.8327\n",
      "Epoch 62: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.3945 - accuracy: 0.8327 - val_loss: 0.4569 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.8339\n",
      "Epoch 63: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.4128 - accuracy: 0.8339 - val_loss: 0.4812 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3756 - accuracy: 0.8450\n",
      "Epoch 64: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.3756 - accuracy: 0.8450 - val_loss: 0.4768 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3847 - accuracy: 0.8512\n",
      "Epoch 65: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 206ms/step - loss: 0.3847 - accuracy: 0.8512 - val_loss: 0.5704 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3831 - accuracy: 0.8524\n",
      "Epoch 66: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.3831 - accuracy: 0.8524 - val_loss: 0.6145 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3823 - accuracy: 0.8438\n",
      "Epoch 67: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.3823 - accuracy: 0.8438 - val_loss: 0.5064 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4105 - accuracy: 0.8352\n",
      "Epoch 68: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 218ms/step - loss: 0.4105 - accuracy: 0.8352 - val_loss: 0.3761 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.8549\n",
      "Epoch 69: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.3777 - accuracy: 0.8549 - val_loss: 0.4053 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3824 - accuracy: 0.8438\n",
      "Epoch 70: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.3824 - accuracy: 0.8438 - val_loss: 0.3927 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3743 - accuracy: 0.8475\n",
      "Epoch 71: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 0.3743 - accuracy: 0.8475 - val_loss: 0.4870 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3747 - accuracy: 0.8499\n",
      "Epoch 72: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.3747 - accuracy: 0.8499 - val_loss: 0.5129 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.8659\n",
      "Epoch 73: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.3341 - accuracy: 0.8659 - val_loss: 0.6244 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3659 - accuracy: 0.8536\n",
      "Epoch 74: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.3659 - accuracy: 0.8536 - val_loss: 0.5889 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3399 - accuracy: 0.8696\n",
      "Epoch 75: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 207ms/step - loss: 0.3399 - accuracy: 0.8696 - val_loss: 0.5644 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.8450\n",
      "Epoch 76: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.4102 - accuracy: 0.8450 - val_loss: 0.4268 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8622\n",
      "Epoch 77: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.3639 - accuracy: 0.8622 - val_loss: 0.3723 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.8696\n",
      "Epoch 78: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.3387 - accuracy: 0.8696 - val_loss: 0.3805 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3404 - accuracy: 0.8672\n",
      "Epoch 79: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.3404 - accuracy: 0.8672 - val_loss: 0.4837 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3516 - accuracy: 0.8696\n",
      "Epoch 80: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.3516 - accuracy: 0.8696 - val_loss: 0.4606 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.8610\n",
      "Epoch 81: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.3323 - accuracy: 0.8610 - val_loss: 0.4504 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3480 - accuracy: 0.8622\n",
      "Epoch 82: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.3480 - accuracy: 0.8622 - val_loss: 0.4386 - val_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3264 - accuracy: 0.8610\n",
      "Epoch 83: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.3264 - accuracy: 0.8610 - val_loss: 0.5428 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3384 - accuracy: 0.8598\n",
      "Epoch 84: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.3384 - accuracy: 0.8598 - val_loss: 0.6235 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3215 - accuracy: 0.8647\n",
      "Epoch 85: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 0.3215 - accuracy: 0.8647 - val_loss: 0.4665 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3422 - accuracy: 0.8684\n",
      "Epoch 86: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 0.3422 - accuracy: 0.8684 - val_loss: 0.4436 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3292 - accuracy: 0.8684\n",
      "Epoch 87: val_loss did not improve from 0.36182\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.3292 - accuracy: 0.8684 - val_loss: 0.4787 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3136 - accuracy: 0.8782\n",
      "Epoch 88: val_loss improved from 0.36182 to 0.32167, saving model to model.h5\n",
      "13/13 [==============================] - 6s 455ms/step - loss: 0.3136 - accuracy: 0.8782 - val_loss: 0.3217 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3176 - accuracy: 0.8733\n",
      "Epoch 89: val_loss did not improve from 0.32167\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.3176 - accuracy: 0.8733 - val_loss: 0.4378 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3213 - accuracy: 0.8733\n",
      "Epoch 90: val_loss did not improve from 0.32167\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.3213 - accuracy: 0.8733 - val_loss: 0.5844 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2956 - accuracy: 0.8868\n",
      "Epoch 91: val_loss did not improve from 0.32167\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.2956 - accuracy: 0.8868 - val_loss: 0.3583 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3204 - accuracy: 0.8795\n",
      "Epoch 92: val_loss improved from 0.32167 to 0.27862, saving model to model.h5\n",
      "13/13 [==============================] - 6s 471ms/step - loss: 0.3204 - accuracy: 0.8795 - val_loss: 0.2786 - val_accuracy: 0.8818 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.8635\n",
      "Epoch 93: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.3130 - accuracy: 0.8635 - val_loss: 0.3747 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3069 - accuracy: 0.8882\n",
      "Epoch 94: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.3069 - accuracy: 0.8882 - val_loss: 0.4840 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2756 - accuracy: 0.8795\n",
      "Epoch 95: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.2756 - accuracy: 0.8795 - val_loss: 0.4267 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2867 - accuracy: 0.8745\n",
      "Epoch 96: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.2867 - accuracy: 0.8745 - val_loss: 0.4947 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2875 - accuracy: 0.8856\n",
      "Epoch 97: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.2875 - accuracy: 0.8856 - val_loss: 0.4022 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.8782\n",
      "Epoch 98: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.2850 - accuracy: 0.8782 - val_loss: 0.5789 - val_accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.8782\n",
      "Epoch 99: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.3111 - accuracy: 0.8782 - val_loss: 0.6234 - val_accuracy: 0.7545 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.9065\n",
      "Epoch 100: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 218ms/step - loss: 0.2487 - accuracy: 0.9065 - val_loss: 0.6271 - val_accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3059 - accuracy: 0.8758\n",
      "Epoch 101: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.3059 - accuracy: 0.8758 - val_loss: 0.6671 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.8807\n",
      "Epoch 102: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.2903 - accuracy: 0.8807 - val_loss: 0.6759 - val_accuracy: 0.7773 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.8844\n",
      "Epoch 103: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.2865 - accuracy: 0.8844 - val_loss: 0.4321 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2940 - accuracy: 0.8721\n",
      "Epoch 104: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.2940 - accuracy: 0.8721 - val_loss: 0.3254 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2587 - accuracy: 0.9002\n",
      "Epoch 105: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 220ms/step - loss: 0.2587 - accuracy: 0.9002 - val_loss: 0.2841 - val_accuracy: 0.8773 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.8906\n",
      "Epoch 106: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.2703 - accuracy: 0.8906 - val_loss: 0.5133 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.8795\n",
      "Epoch 107: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.2844 - accuracy: 0.8795 - val_loss: 0.5407 - val_accuracy: 0.8409 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.9041\n",
      "Epoch 108: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.2469 - accuracy: 0.9041 - val_loss: 0.5336 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2921 - accuracy: 0.8905\n",
      "Epoch 109: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 0.2921 - accuracy: 0.8905 - val_loss: 0.6365 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2652 - accuracy: 0.8954\n",
      "Epoch 110: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.2652 - accuracy: 0.8954 - val_loss: 0.3423 - val_accuracy: 0.8727 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2761 - accuracy: 0.8881\n",
      "Epoch 111: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.2761 - accuracy: 0.8881 - val_loss: 0.3601 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2997 - accuracy: 0.8905\n",
      "Epoch 112: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.2997 - accuracy: 0.8905 - val_loss: 0.3521 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 0.8979\n",
      "Epoch 113: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.2597 - accuracy: 0.8979 - val_loss: 0.4683 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2689 - accuracy: 0.8893\n",
      "Epoch 114: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.2689 - accuracy: 0.8893 - val_loss: 0.6331 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.8831\n",
      "Epoch 115: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.3065 - accuracy: 0.8831 - val_loss: 0.6637 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2821 - accuracy: 0.9016\n",
      "Epoch 116: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.2821 - accuracy: 0.9016 - val_loss: 0.5474 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.8882\n",
      "Epoch 117: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.2584 - accuracy: 0.8882 - val_loss: 0.6023 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.9114\n",
      "Epoch 118: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.2104 - accuracy: 0.9114 - val_loss: 0.4017 - val_accuracy: 0.8727 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.8967\n",
      "Epoch 119: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.2519 - accuracy: 0.8967 - val_loss: 0.4229 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2382 - accuracy: 0.9090\n",
      "Epoch 120: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.2382 - accuracy: 0.9090 - val_loss: 0.4971 - val_accuracy: 0.8727 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.8942\n",
      "Epoch 121: val_loss did not improve from 0.27862\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 0.2541 - accuracy: 0.8942 - val_loss: 0.3584 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.9114\n",
      "Epoch 122: val_loss improved from 0.27862 to 0.27305, saving model to model.h5\n",
      "13/13 [==============================] - 6s 470ms/step - loss: 0.2381 - accuracy: 0.9114 - val_loss: 0.2730 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 0.9016\n",
      "Epoch 123: val_loss did not improve from 0.27305\n",
      "13/13 [==============================] - 3s 218ms/step - loss: 0.2310 - accuracy: 0.9016 - val_loss: 0.2766 - val_accuracy: 0.8864 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2899 - accuracy: 0.8954\n",
      "Epoch 124: val_loss did not improve from 0.27305\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.2899 - accuracy: 0.8954 - val_loss: 0.3144 - val_accuracy: 0.8864 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2337 - accuracy: 0.9002\n",
      "Epoch 125: val_loss improved from 0.27305 to 0.25305, saving model to model.h5\n",
      "13/13 [==============================] - 6s 461ms/step - loss: 0.2337 - accuracy: 0.9002 - val_loss: 0.2530 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2256 - accuracy: 0.9077\n",
      "Epoch 126: val_loss improved from 0.25305 to 0.22476, saving model to model.h5\n",
      "13/13 [==============================] - 6s 459ms/step - loss: 0.2256 - accuracy: 0.9077 - val_loss: 0.2248 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9139\n",
      "Epoch 127: val_loss did not improve from 0.22476\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.2466 - accuracy: 0.9139 - val_loss: 0.2466 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.8930\n",
      "Epoch 128: val_loss did not improve from 0.22476\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.2667 - accuracy: 0.8930 - val_loss: 0.2378 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2334 - accuracy: 0.9016\n",
      "Epoch 129: val_loss did not improve from 0.22476\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.2334 - accuracy: 0.9016 - val_loss: 0.2523 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2128 - accuracy: 0.9151\n",
      "Epoch 130: val_loss did not improve from 0.22476\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.2128 - accuracy: 0.9151 - val_loss: 0.4190 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9274\n",
      "Epoch 131: val_loss did not improve from 0.22476\n",
      "13/13 [==============================] - 3s 218ms/step - loss: 0.2111 - accuracy: 0.9274 - val_loss: 0.5005 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2322 - accuracy: 0.9065\n",
      "Epoch 132: val_loss did not improve from 0.22476\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.2322 - accuracy: 0.9065 - val_loss: 0.6413 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2368 - accuracy: 0.9065\n",
      "Epoch 133: val_loss did not improve from 0.22476\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.2368 - accuracy: 0.9065 - val_loss: 0.3583 - val_accuracy: 0.8864 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2140 - accuracy: 0.9053\n",
      "Epoch 134: val_loss did not improve from 0.22476\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.2140 - accuracy: 0.9053 - val_loss: 0.4398 - val_accuracy: 0.8818 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2524 - accuracy: 0.9004\n",
      "Epoch 135: val_loss did not improve from 0.22476\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.2524 - accuracy: 0.9004 - val_loss: 0.3072 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2143 - accuracy: 0.9127\n",
      "Epoch 136: val_loss improved from 0.22476 to 0.21910, saving model to model.h5\n",
      "13/13 [==============================] - 6s 462ms/step - loss: 0.2143 - accuracy: 0.9127 - val_loss: 0.2191 - val_accuracy: 0.9227 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.9090\n",
      "Epoch 137: val_loss improved from 0.21910 to 0.20631, saving model to model.h5\n",
      "13/13 [==============================] - 6s 461ms/step - loss: 0.2326 - accuracy: 0.9090 - val_loss: 0.2063 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.9102\n",
      "Epoch 138: val_loss did not improve from 0.20631\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.2403 - accuracy: 0.9102 - val_loss: 0.3960 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2257 - accuracy: 0.9041\n",
      "Epoch 139: val_loss did not improve from 0.20631\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.2257 - accuracy: 0.9041 - val_loss: 0.3714 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2243 - accuracy: 0.9127\n",
      "Epoch 140: val_loss did not improve from 0.20631\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.2243 - accuracy: 0.9127 - val_loss: 0.5166 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2262 - accuracy: 0.9151\n",
      "Epoch 141: val_loss did not improve from 0.20631\n",
      "13/13 [==============================] - 3s 219ms/step - loss: 0.2262 - accuracy: 0.9151 - val_loss: 0.5754 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9164\n",
      "Epoch 142: val_loss did not improve from 0.20631\n",
      "13/13 [==============================] - 3s 220ms/step - loss: 0.1898 - accuracy: 0.9164 - val_loss: 0.7052 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2262 - accuracy: 0.9090\n",
      "Epoch 143: val_loss did not improve from 0.20631\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.2262 - accuracy: 0.9090 - val_loss: 0.3006 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2159 - accuracy: 0.9127\n",
      "Epoch 144: val_loss did not improve from 0.20631\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.2159 - accuracy: 0.9127 - val_loss: 0.2114 - val_accuracy: 0.9318 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2241 - accuracy: 0.9139\n",
      "Epoch 145: val_loss did not improve from 0.20631\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.2241 - accuracy: 0.9139 - val_loss: 0.2399 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2062 - accuracy: 0.9090\n",
      "Epoch 146: val_loss improved from 0.20631 to 0.19252, saving model to model.h5\n",
      "13/13 [==============================] - 6s 469ms/step - loss: 0.2062 - accuracy: 0.9090 - val_loss: 0.1925 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9336\n",
      "Epoch 147: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.1739 - accuracy: 0.9336 - val_loss: 0.2968 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.9279\n",
      "Epoch 148: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.1821 - accuracy: 0.9279 - val_loss: 0.2053 - val_accuracy: 0.9318 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1921 - accuracy: 0.9200\n",
      "Epoch 149: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.1921 - accuracy: 0.9200 - val_loss: 0.2518 - val_accuracy: 0.9227 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9262\n",
      "Epoch 150: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.1875 - accuracy: 0.9262 - val_loss: 0.5951 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.9114\n",
      "Epoch 151: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.2316 - accuracy: 0.9114 - val_loss: 0.6287 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1919 - accuracy: 0.9262\n",
      "Epoch 152: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.1919 - accuracy: 0.9262 - val_loss: 0.4222 - val_accuracy: 0.8818 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2190 - accuracy: 0.9090\n",
      "Epoch 153: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.2190 - accuracy: 0.9090 - val_loss: 0.3358 - val_accuracy: 0.8727 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1976 - accuracy: 0.9250\n",
      "Epoch 154: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 218ms/step - loss: 0.1976 - accuracy: 0.9250 - val_loss: 0.3192 - val_accuracy: 0.8727 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2032 - accuracy: 0.9311\n",
      "Epoch 155: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.2032 - accuracy: 0.9311 - val_loss: 0.3816 - val_accuracy: 0.8818 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1911 - accuracy: 0.9250\n",
      "Epoch 156: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.1911 - accuracy: 0.9250 - val_loss: 0.2241 - val_accuracy: 0.9227 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.9323\n",
      "Epoch 157: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 218ms/step - loss: 0.1916 - accuracy: 0.9323 - val_loss: 0.4412 - val_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1908 - accuracy: 0.9255\n",
      "Epoch 158: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 218ms/step - loss: 0.1908 - accuracy: 0.9255 - val_loss: 0.2586 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9250\n",
      "Epoch 159: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.1823 - accuracy: 0.9250 - val_loss: 0.7508 - val_accuracy: 0.7727 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2033 - accuracy: 0.9176\n",
      "Epoch 160: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.2033 - accuracy: 0.9176 - val_loss: 0.3734 - val_accuracy: 0.8818 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1975 - accuracy: 0.9250\n",
      "Epoch 161: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.1975 - accuracy: 0.9250 - val_loss: 0.2267 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9237\n",
      "Epoch 162: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.1925 - accuracy: 0.9237 - val_loss: 0.2685 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.9323\n",
      "Epoch 163: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.1821 - accuracy: 0.9323 - val_loss: 0.2593 - val_accuracy: 0.9227 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9348\n",
      "Epoch 164: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.1669 - accuracy: 0.9348 - val_loss: 0.2994 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9200\n",
      "Epoch 165: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.2114 - accuracy: 0.9200 - val_loss: 0.3909 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2185 - accuracy: 0.9188\n",
      "Epoch 166: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 221ms/step - loss: 0.2185 - accuracy: 0.9188 - val_loss: 0.6873 - val_accuracy: 0.7727 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1841 - accuracy: 0.9336\n",
      "Epoch 167: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.1841 - accuracy: 0.9336 - val_loss: 0.3060 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9385\n",
      "Epoch 168: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 219ms/step - loss: 0.1672 - accuracy: 0.9385 - val_loss: 0.3673 - val_accuracy: 0.8864 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1320 - accuracy: 0.9520\n",
      "Epoch 169: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 220ms/step - loss: 0.1320 - accuracy: 0.9520 - val_loss: 0.5883 - val_accuracy: 0.7955 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1783 - accuracy: 0.9250\n",
      "Epoch 170: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.1783 - accuracy: 0.9250 - val_loss: 0.4302 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.9311\n",
      "Epoch 171: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.1707 - accuracy: 0.9311 - val_loss: 0.2968 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1832 - accuracy: 0.9287\n",
      "Epoch 172: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 219ms/step - loss: 0.1832 - accuracy: 0.9287 - val_loss: 0.2312 - val_accuracy: 0.9227 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9397\n",
      "Epoch 173: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 220ms/step - loss: 0.1687 - accuracy: 0.9397 - val_loss: 0.3523 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9545\n",
      "Epoch 174: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.1299 - accuracy: 0.9545 - val_loss: 0.4936 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.9250\n",
      "Epoch 175: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 219ms/step - loss: 0.1912 - accuracy: 0.9250 - val_loss: 0.3804 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1612 - accuracy: 0.9397\n",
      "Epoch 176: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.1612 - accuracy: 0.9397 - val_loss: 0.2514 - val_accuracy: 0.9273 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9533\n",
      "Epoch 177: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 221ms/step - loss: 0.1289 - accuracy: 0.9533 - val_loss: 0.2331 - val_accuracy: 0.9318 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1664 - accuracy: 0.9397\n",
      "Epoch 178: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.1664 - accuracy: 0.9397 - val_loss: 0.3766 - val_accuracy: 0.8818 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9336\n",
      "Epoch 179: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.1673 - accuracy: 0.9336 - val_loss: 0.4964 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.9410\n",
      "Epoch 180: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.1454 - accuracy: 0.9410 - val_loss: 0.5472 - val_accuracy: 0.8409 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9446\n",
      "Epoch 181: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.1439 - accuracy: 0.9446 - val_loss: 0.3926 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9274\n",
      "Epoch 182: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 219ms/step - loss: 0.1808 - accuracy: 0.9274 - val_loss: 0.2994 - val_accuracy: 0.9227 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.9434\n",
      "Epoch 183: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.1353 - accuracy: 0.9434 - val_loss: 0.3671 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9483\n",
      "Epoch 184: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.1484 - accuracy: 0.9483 - val_loss: 0.2082 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1173 - accuracy: 0.9508\n",
      "Epoch 185: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.1173 - accuracy: 0.9508 - val_loss: 0.2155 - val_accuracy: 0.9318 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9483\n",
      "Epoch 186: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.1318 - accuracy: 0.9483 - val_loss: 0.2104 - val_accuracy: 0.9409 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9315\n",
      "Epoch 187: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.1774 - accuracy: 0.9315 - val_loss: 0.2534 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1385 - accuracy: 0.9471\n",
      "Epoch 188: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.1385 - accuracy: 0.9471 - val_loss: 0.3530 - val_accuracy: 0.8864 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9557\n",
      "Epoch 189: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 219ms/step - loss: 0.1284 - accuracy: 0.9557 - val_loss: 0.5008 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1618 - accuracy: 0.9348\n",
      "Epoch 190: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.1618 - accuracy: 0.9348 - val_loss: 0.2858 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9471\n",
      "Epoch 191: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.1420 - accuracy: 0.9471 - val_loss: 0.2195 - val_accuracy: 0.9227 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1261 - accuracy: 0.9471\n",
      "Epoch 192: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.1261 - accuracy: 0.9471 - val_loss: 0.3002 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9471\n",
      "Epoch 193: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.1246 - accuracy: 0.9471 - val_loss: 0.5358 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9579\n",
      "Epoch 194: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.1183 - accuracy: 0.9579 - val_loss: 0.3691 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9508\n",
      "Epoch 195: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.1406 - accuracy: 0.9508 - val_loss: 0.6425 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9545\n",
      "Epoch 196: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.1140 - accuracy: 0.9545 - val_loss: 1.0724 - val_accuracy: 0.7318 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9483\n",
      "Epoch 197: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 221ms/step - loss: 0.1316 - accuracy: 0.9483 - val_loss: 0.7408 - val_accuracy: 0.7909 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1395 - accuracy: 0.9533\n",
      "Epoch 198: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.1395 - accuracy: 0.9533 - val_loss: 0.2336 - val_accuracy: 0.9227 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9533\n",
      "Epoch 199: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 221ms/step - loss: 0.1299 - accuracy: 0.9533 - val_loss: 0.2057 - val_accuracy: 0.9273 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1546 - accuracy: 0.9385\n",
      "Epoch 200: val_loss did not improve from 0.19252\n",
      "13/13 [==============================] - 3s 220ms/step - loss: 0.1546 - accuracy: 0.9385 - val_loss: 0.2229 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.2229 - accuracy: 0.9182\n",
      "28/28 [==============================] - 2s 59ms/step - loss: 0.1601 - accuracy: 0.9396\n",
      "Model Performance for DenseNet201\n",
      "Loss(Test): 0.2228897511959076, Accuracy(Test): 91.8%\n",
      "Loss(Train): 0.1601419448852539, Accuracy(Train): 94.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Xception\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_35 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_34 (Conv2D)          (None, 64, 64, 3)         84        \n",
      "                                                                 \n",
      " xception (Functional)       (None, None, None, 2048)  20861480  \n",
      "                                                                 \n",
      " global_average_pooling2d_10  (None, 2048)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_44 (Bat  (None, 2048)             8192      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               524544    \n",
      "                                                                 \n",
      " batch_normalization_45 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " root (Dense)                (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,396,095\n",
      "Trainable params: 21,336,959\n",
      "Non-trainable params: 59,136\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.8104 - accuracy: 0.3813\n",
      "Epoch 1: val_loss improved from inf to 1.07226, saving model to model.h5\n",
      "13/13 [==============================] - 12s 382ms/step - loss: 1.8104 - accuracy: 0.3813 - val_loss: 1.0723 - val_accuracy: 0.4909 - lr: 0.0020\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.8786 - accuracy: 0.3309\n",
      "Epoch 2: val_loss improved from 1.07226 to 1.03237, saving model to model.h5\n",
      "13/13 [==============================] - 3s 244ms/step - loss: 1.8786 - accuracy: 0.3309 - val_loss: 1.0324 - val_accuracy: 0.5091 - lr: 0.0020\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7665 - accuracy: 0.3702\n",
      "Epoch 3: val_loss improved from 1.03237 to 1.00688, saving model to model.h5\n",
      "13/13 [==============================] - 3s 264ms/step - loss: 1.7665 - accuracy: 0.3702 - val_loss: 1.0069 - val_accuracy: 0.5136 - lr: 0.0020\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7303 - accuracy: 0.3542\n",
      "Epoch 4: val_loss improved from 1.00688 to 1.00315, saving model to model.h5\n",
      "13/13 [==============================] - 3s 240ms/step - loss: 1.7303 - accuracy: 0.3542 - val_loss: 1.0032 - val_accuracy: 0.5136 - lr: 0.0020\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6066 - accuracy: 0.3579\n",
      "Epoch 5: val_loss improved from 1.00315 to 0.99837, saving model to model.h5\n",
      "13/13 [==============================] - 3s 243ms/step - loss: 1.6066 - accuracy: 0.3579 - val_loss: 0.9984 - val_accuracy: 0.5136 - lr: 0.0020\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6491 - accuracy: 0.3752\n",
      "Epoch 6: val_loss did not improve from 0.99837\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 1.6491 - accuracy: 0.3752 - val_loss: 1.0007 - val_accuracy: 0.5136 - lr: 0.0020\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.5829 - accuracy: 0.4084\n",
      "Epoch 7: val_loss improved from 0.99837 to 0.99410, saving model to model.h5\n",
      "13/13 [==============================] - 3s 248ms/step - loss: 1.5829 - accuracy: 0.4084 - val_loss: 0.9941 - val_accuracy: 0.5136 - lr: 0.0020\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.4024 - accuracy: 0.4256\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.99410\n",
      "13/13 [==============================] - 2s 135ms/step - loss: 1.4024 - accuracy: 0.4256 - val_loss: 0.9971 - val_accuracy: 0.5136 - lr: 0.0020\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3830 - accuracy: 0.4034\n",
      "Epoch 9: val_loss did not improve from 0.99410\n",
      "13/13 [==============================] - 2s 135ms/step - loss: 1.3830 - accuracy: 0.4034 - val_loss: 0.9993 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3866 - accuracy: 0.3875\n",
      "Epoch 10: val_loss did not improve from 0.99410\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 1.3866 - accuracy: 0.3875 - val_loss: 1.0047 - val_accuracy: 0.5091 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3512 - accuracy: 0.4342\n",
      "Epoch 11: val_loss did not improve from 0.99410\n",
      "13/13 [==============================] - 2s 133ms/step - loss: 1.3512 - accuracy: 0.4342 - val_loss: 1.0059 - val_accuracy: 0.5091 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3844 - accuracy: 0.4194\n",
      "Epoch 12: val_loss did not improve from 0.99410\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 1.3844 - accuracy: 0.4194 - val_loss: 0.9984 - val_accuracy: 0.5045 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3941 - accuracy: 0.3973\n",
      "Epoch 13: val_loss improved from 0.99410 to 0.98760, saving model to model.h5\n",
      "13/13 [==============================] - 3s 243ms/step - loss: 1.3941 - accuracy: 0.3973 - val_loss: 0.9876 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3239 - accuracy: 0.4231\n",
      "Epoch 14: val_loss improved from 0.98760 to 0.98093, saving model to model.h5\n",
      "13/13 [==============================] - 3s 242ms/step - loss: 1.3239 - accuracy: 0.4231 - val_loss: 0.9809 - val_accuracy: 0.5045 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2870 - accuracy: 0.4194\n",
      "Epoch 15: val_loss improved from 0.98093 to 0.97271, saving model to model.h5\n",
      "13/13 [==============================] - 3s 243ms/step - loss: 1.2870 - accuracy: 0.4194 - val_loss: 0.9727 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2505 - accuracy: 0.4514\n",
      "Epoch 16: val_loss improved from 0.97271 to 0.96676, saving model to model.h5\n",
      "13/13 [==============================] - 3s 245ms/step - loss: 1.2505 - accuracy: 0.4514 - val_loss: 0.9668 - val_accuracy: 0.5091 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2981 - accuracy: 0.4490\n",
      "Epoch 17: val_loss improved from 0.96676 to 0.96070, saving model to model.h5\n",
      "13/13 [==============================] - 3s 246ms/step - loss: 1.2981 - accuracy: 0.4490 - val_loss: 0.9607 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1858 - accuracy: 0.4711\n",
      "Epoch 18: val_loss did not improve from 0.96070\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 1.1858 - accuracy: 0.4711 - val_loss: 0.9701 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2738 - accuracy: 0.4526\n",
      "Epoch 19: val_loss did not improve from 0.96070\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 1.2738 - accuracy: 0.4526 - val_loss: 0.9712 - val_accuracy: 0.5091 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1560 - accuracy: 0.5080\n",
      "Epoch 20: val_loss improved from 0.96070 to 0.95740, saving model to model.h5\n",
      "13/13 [==============================] - 3s 244ms/step - loss: 1.1560 - accuracy: 0.5080 - val_loss: 0.9574 - val_accuracy: 0.5091 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1810 - accuracy: 0.4908\n",
      "Epoch 21: val_loss improved from 0.95740 to 0.93210, saving model to model.h5\n",
      "13/13 [==============================] - 3s 242ms/step - loss: 1.1810 - accuracy: 0.4908 - val_loss: 0.9321 - val_accuracy: 0.5318 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2085 - accuracy: 0.4932\n",
      "Epoch 22: val_loss improved from 0.93210 to 0.92699, saving model to model.h5\n",
      "13/13 [==============================] - 3s 239ms/step - loss: 1.2085 - accuracy: 0.4932 - val_loss: 0.9270 - val_accuracy: 0.5364 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0965 - accuracy: 0.5055\n",
      "Epoch 23: val_loss improved from 0.92699 to 0.91724, saving model to model.h5\n",
      "13/13 [==============================] - 3s 247ms/step - loss: 1.0965 - accuracy: 0.5055 - val_loss: 0.9172 - val_accuracy: 0.5545 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0873 - accuracy: 0.5043\n",
      "Epoch 24: val_loss did not improve from 0.91724\n",
      "13/13 [==============================] - 2s 135ms/step - loss: 1.0873 - accuracy: 0.5043 - val_loss: 0.9182 - val_accuracy: 0.5591 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1767 - accuracy: 0.4809\n",
      "Epoch 25: val_loss did not improve from 0.91724\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 1.1767 - accuracy: 0.4809 - val_loss: 0.9223 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1194 - accuracy: 0.4920\n",
      "Epoch 26: val_loss did not improve from 0.91724\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 1.1194 - accuracy: 0.4920 - val_loss: 0.9372 - val_accuracy: 0.5636 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0763 - accuracy: 0.5154\n",
      "Epoch 27: val_loss did not improve from 0.91724\n",
      "13/13 [==============================] - 2s 133ms/step - loss: 1.0763 - accuracy: 0.5154 - val_loss: 0.9216 - val_accuracy: 0.5727 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0965 - accuracy: 0.5055\n",
      "Epoch 28: val_loss improved from 0.91724 to 0.88645, saving model to model.h5\n",
      "13/13 [==============================] - 3s 241ms/step - loss: 1.0965 - accuracy: 0.5055 - val_loss: 0.8865 - val_accuracy: 0.5955 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0815 - accuracy: 0.5117\n",
      "Epoch 29: val_loss improved from 0.88645 to 0.86393, saving model to model.h5\n",
      "13/13 [==============================] - 3s 244ms/step - loss: 1.0815 - accuracy: 0.5117 - val_loss: 0.8639 - val_accuracy: 0.6091 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0606 - accuracy: 0.5141\n",
      "Epoch 30: val_loss improved from 0.86393 to 0.85481, saving model to model.h5\n",
      "13/13 [==============================] - 3s 246ms/step - loss: 1.0606 - accuracy: 0.5141 - val_loss: 0.8548 - val_accuracy: 0.6136 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0112 - accuracy: 0.5080\n",
      "Epoch 31: val_loss did not improve from 0.85481\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 1.0112 - accuracy: 0.5080 - val_loss: 0.8660 - val_accuracy: 0.5818 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0317 - accuracy: 0.5264\n",
      "Epoch 32: val_loss did not improve from 0.85481\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 1.0317 - accuracy: 0.5264 - val_loss: 0.8678 - val_accuracy: 0.6045 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0552 - accuracy: 0.5228\n",
      "Epoch 33: val_loss did not improve from 0.85481\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 1.0552 - accuracy: 0.5228 - val_loss: 0.8811 - val_accuracy: 0.6227 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0193 - accuracy: 0.5437\n",
      "Epoch 34: val_loss did not improve from 0.85481\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 1.0193 - accuracy: 0.5437 - val_loss: 0.8625 - val_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9826 - accuracy: 0.5584\n",
      "Epoch 35: val_loss improved from 0.85481 to 0.85243, saving model to model.h5\n",
      "13/13 [==============================] - 3s 247ms/step - loss: 0.9826 - accuracy: 0.5584 - val_loss: 0.8524 - val_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9789 - accuracy: 0.5277\n",
      "Epoch 36: val_loss did not improve from 0.85243\n",
      "13/13 [==============================] - 2s 135ms/step - loss: 0.9789 - accuracy: 0.5277 - val_loss: 0.8619 - val_accuracy: 0.5864 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9535 - accuracy: 0.5437\n",
      "Epoch 37: val_loss improved from 0.85243 to 0.84689, saving model to model.h5\n",
      "13/13 [==============================] - 3s 243ms/step - loss: 0.9535 - accuracy: 0.5437 - val_loss: 0.8469 - val_accuracy: 0.5909 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9500 - accuracy: 0.5658\n",
      "Epoch 38: val_loss improved from 0.84689 to 0.84009, saving model to model.h5\n",
      "13/13 [==============================] - 3s 251ms/step - loss: 0.9500 - accuracy: 0.5658 - val_loss: 0.8401 - val_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9131 - accuracy: 0.5843\n",
      "Epoch 39: val_loss improved from 0.84009 to 0.83899, saving model to model.h5\n",
      "13/13 [==============================] - 3s 248ms/step - loss: 0.9131 - accuracy: 0.5843 - val_loss: 0.8390 - val_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0004 - accuracy: 0.5670\n",
      "Epoch 40: val_loss improved from 0.83899 to 0.82986, saving model to model.h5\n",
      "13/13 [==============================] - 3s 247ms/step - loss: 1.0004 - accuracy: 0.5670 - val_loss: 0.8299 - val_accuracy: 0.6182 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9294 - accuracy: 0.5843\n",
      "Epoch 41: val_loss improved from 0.82986 to 0.81458, saving model to model.h5\n",
      "13/13 [==============================] - 3s 245ms/step - loss: 0.9294 - accuracy: 0.5843 - val_loss: 0.8146 - val_accuracy: 0.6273 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9023 - accuracy: 0.5670\n",
      "Epoch 42: val_loss improved from 0.81458 to 0.79762, saving model to model.h5\n",
      "13/13 [==============================] - 3s 250ms/step - loss: 0.9023 - accuracy: 0.5670 - val_loss: 0.7976 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9113 - accuracy: 0.5953\n",
      "Epoch 43: val_loss improved from 0.79762 to 0.78495, saving model to model.h5\n",
      "13/13 [==============================] - 3s 243ms/step - loss: 0.9113 - accuracy: 0.5953 - val_loss: 0.7849 - val_accuracy: 0.6227 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9063 - accuracy: 0.5904\n",
      "Epoch 44: val_loss did not improve from 0.78495\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.9063 - accuracy: 0.5904 - val_loss: 0.7860 - val_accuracy: 0.6318 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8884 - accuracy: 0.6150\n",
      "Epoch 45: val_loss did not improve from 0.78495\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.8884 - accuracy: 0.6150 - val_loss: 0.7950 - val_accuracy: 0.6227 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9094 - accuracy: 0.6076\n",
      "Epoch 46: val_loss improved from 0.78495 to 0.77731, saving model to model.h5\n",
      "13/13 [==============================] - 3s 244ms/step - loss: 0.9094 - accuracy: 0.6076 - val_loss: 0.7773 - val_accuracy: 0.6591 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8794 - accuracy: 0.6089\n",
      "Epoch 47: val_loss did not improve from 0.77731\n",
      "13/13 [==============================] - 2s 135ms/step - loss: 0.8794 - accuracy: 0.6089 - val_loss: 0.7894 - val_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8872 - accuracy: 0.6064\n",
      "Epoch 48: val_loss did not improve from 0.77731\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.8872 - accuracy: 0.6064 - val_loss: 0.7871 - val_accuracy: 0.6273 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8936 - accuracy: 0.6118\n",
      "Epoch 49: val_loss improved from 0.77731 to 0.76801, saving model to model.h5\n",
      "13/13 [==============================] - 3s 249ms/step - loss: 0.8936 - accuracy: 0.6118 - val_loss: 0.7680 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9067 - accuracy: 0.6052\n",
      "Epoch 50: val_loss improved from 0.76801 to 0.75580, saving model to model.h5\n",
      "13/13 [==============================] - 3s 244ms/step - loss: 0.9067 - accuracy: 0.6052 - val_loss: 0.7558 - val_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9053 - accuracy: 0.6101\n",
      "Epoch 51: val_loss did not improve from 0.75580\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.9053 - accuracy: 0.6101 - val_loss: 0.7603 - val_accuracy: 0.6455 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8701 - accuracy: 0.6273\n",
      "Epoch 52: val_loss did not improve from 0.75580\n",
      "13/13 [==============================] - 2s 135ms/step - loss: 0.8701 - accuracy: 0.6273 - val_loss: 0.7562 - val_accuracy: 0.6682 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8654 - accuracy: 0.6285\n",
      "Epoch 53: val_loss did not improve from 0.75580\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.8654 - accuracy: 0.6285 - val_loss: 0.7689 - val_accuracy: 0.6773 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8225 - accuracy: 0.6421\n",
      "Epoch 54: val_loss did not improve from 0.75580\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.8225 - accuracy: 0.6421 - val_loss: 0.7812 - val_accuracy: 0.6636 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8277 - accuracy: 0.6261\n",
      "Epoch 55: val_loss did not improve from 0.75580\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.8277 - accuracy: 0.6261 - val_loss: 0.7786 - val_accuracy: 0.6591 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8643 - accuracy: 0.6285\n",
      "Epoch 56: val_loss did not improve from 0.75580\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.8643 - accuracy: 0.6285 - val_loss: 0.7799 - val_accuracy: 0.6591 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8205 - accuracy: 0.6581\n",
      "Epoch 57: val_loss did not improve from 0.75580\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.8205 - accuracy: 0.6581 - val_loss: 0.7580 - val_accuracy: 0.6773 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8081 - accuracy: 0.6667\n",
      "Epoch 58: val_loss improved from 0.75580 to 0.73832, saving model to model.h5\n",
      "13/13 [==============================] - 3s 246ms/step - loss: 0.8081 - accuracy: 0.6667 - val_loss: 0.7383 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7830 - accuracy: 0.6728\n",
      "Epoch 59: val_loss improved from 0.73832 to 0.72575, saving model to model.h5\n",
      "13/13 [==============================] - 3s 244ms/step - loss: 0.7830 - accuracy: 0.6728 - val_loss: 0.7258 - val_accuracy: 0.6955 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8020 - accuracy: 0.6654\n",
      "Epoch 60: val_loss improved from 0.72575 to 0.71218, saving model to model.h5\n",
      "13/13 [==============================] - 3s 244ms/step - loss: 0.8020 - accuracy: 0.6654 - val_loss: 0.7122 - val_accuracy: 0.7091 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7867 - accuracy: 0.6900\n",
      "Epoch 61: val_loss improved from 0.71218 to 0.70218, saving model to model.h5\n",
      "13/13 [==============================] - 3s 246ms/step - loss: 0.7867 - accuracy: 0.6900 - val_loss: 0.7022 - val_accuracy: 0.6864 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7663 - accuracy: 0.6654\n",
      "Epoch 62: val_loss improved from 0.70218 to 0.69465, saving model to model.h5\n",
      "13/13 [==============================] - 3s 244ms/step - loss: 0.7663 - accuracy: 0.6654 - val_loss: 0.6947 - val_accuracy: 0.6955 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7721 - accuracy: 0.6950\n",
      "Epoch 63: val_loss did not improve from 0.69465\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.7721 - accuracy: 0.6950 - val_loss: 0.6960 - val_accuracy: 0.6864 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7507 - accuracy: 0.6814\n",
      "Epoch 64: val_loss improved from 0.69465 to 0.68513, saving model to model.h5\n",
      "13/13 [==============================] - 3s 240ms/step - loss: 0.7507 - accuracy: 0.6814 - val_loss: 0.6851 - val_accuracy: 0.7136 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7515 - accuracy: 0.6925\n",
      "Epoch 65: val_loss improved from 0.68513 to 0.67195, saving model to model.h5\n",
      "13/13 [==============================] - 3s 248ms/step - loss: 0.7515 - accuracy: 0.6925 - val_loss: 0.6720 - val_accuracy: 0.7227 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.7109\n",
      "Epoch 66: val_loss improved from 0.67195 to 0.66846, saving model to model.h5\n",
      "13/13 [==============================] - 3s 245ms/step - loss: 0.6935 - accuracy: 0.7109 - val_loss: 0.6685 - val_accuracy: 0.7182 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7259 - accuracy: 0.6986\n",
      "Epoch 67: val_loss improved from 0.66846 to 0.64821, saving model to model.h5\n",
      "13/13 [==============================] - 3s 245ms/step - loss: 0.7259 - accuracy: 0.6986 - val_loss: 0.6482 - val_accuracy: 0.7227 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7389 - accuracy: 0.7109\n",
      "Epoch 68: val_loss improved from 0.64821 to 0.63289, saving model to model.h5\n",
      "13/13 [==============================] - 3s 245ms/step - loss: 0.7389 - accuracy: 0.7109 - val_loss: 0.6329 - val_accuracy: 0.7455 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7473 - accuracy: 0.6971\n",
      "Epoch 69: val_loss did not improve from 0.63289\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.7473 - accuracy: 0.6971 - val_loss: 0.6376 - val_accuracy: 0.7455 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7217 - accuracy: 0.7048\n",
      "Epoch 70: val_loss improved from 0.63289 to 0.62745, saving model to model.h5\n",
      "13/13 [==============================] - 3s 245ms/step - loss: 0.7217 - accuracy: 0.7048 - val_loss: 0.6275 - val_accuracy: 0.7364 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6999 - accuracy: 0.7220\n",
      "Epoch 71: val_loss improved from 0.62745 to 0.61639, saving model to model.h5\n",
      "13/13 [==============================] - 3s 246ms/step - loss: 0.6999 - accuracy: 0.7220 - val_loss: 0.6164 - val_accuracy: 0.7455 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7464 - accuracy: 0.7060\n",
      "Epoch 72: val_loss improved from 0.61639 to 0.59347, saving model to model.h5\n",
      "13/13 [==============================] - 3s 248ms/step - loss: 0.7464 - accuracy: 0.7060 - val_loss: 0.5935 - val_accuracy: 0.7636 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7160 - accuracy: 0.7355\n",
      "Epoch 73: val_loss improved from 0.59347 to 0.57851, saving model to model.h5\n",
      "13/13 [==============================] - 3s 244ms/step - loss: 0.7160 - accuracy: 0.7355 - val_loss: 0.5785 - val_accuracy: 0.7636 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7275 - accuracy: 0.7023\n",
      "Epoch 74: val_loss did not improve from 0.57851\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.7275 - accuracy: 0.7023 - val_loss: 0.5797 - val_accuracy: 0.7636 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7383 - accuracy: 0.7245\n",
      "Epoch 75: val_loss did not improve from 0.57851\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.7383 - accuracy: 0.7245 - val_loss: 0.5826 - val_accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6559 - accuracy: 0.7392\n",
      "Epoch 76: val_loss did not improve from 0.57851\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.6559 - accuracy: 0.7392 - val_loss: 0.5879 - val_accuracy: 0.7636 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.7306\n",
      "Epoch 77: val_loss did not improve from 0.57851\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.6934 - accuracy: 0.7306 - val_loss: 0.5869 - val_accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6818 - accuracy: 0.7355\n",
      "Epoch 78: val_loss did not improve from 0.57851\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.6818 - accuracy: 0.7355 - val_loss: 0.5792 - val_accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6776 - accuracy: 0.7417\n",
      "Epoch 79: val_loss improved from 0.57851 to 0.54450, saving model to model.h5\n",
      "13/13 [==============================] - 3s 248ms/step - loss: 0.6776 - accuracy: 0.7417 - val_loss: 0.5445 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6698 - accuracy: 0.7442\n",
      "Epoch 80: val_loss improved from 0.54450 to 0.52643, saving model to model.h5\n",
      "13/13 [==============================] - 3s 244ms/step - loss: 0.6698 - accuracy: 0.7442 - val_loss: 0.5264 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6694 - accuracy: 0.7552\n",
      "Epoch 81: val_loss improved from 0.52643 to 0.52471, saving model to model.h5\n",
      "13/13 [==============================] - 3s 249ms/step - loss: 0.6694 - accuracy: 0.7552 - val_loss: 0.5247 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6531 - accuracy: 0.7454\n",
      "Epoch 82: val_loss did not improve from 0.52471\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.6531 - accuracy: 0.7454 - val_loss: 0.5276 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6301 - accuracy: 0.7663\n",
      "Epoch 83: val_loss improved from 0.52471 to 0.52345, saving model to model.h5\n",
      "13/13 [==============================] - 3s 245ms/step - loss: 0.6301 - accuracy: 0.7663 - val_loss: 0.5235 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6545 - accuracy: 0.7515\n",
      "Epoch 84: val_loss improved from 0.52345 to 0.51893, saving model to model.h5\n",
      "13/13 [==============================] - 3s 252ms/step - loss: 0.6545 - accuracy: 0.7515 - val_loss: 0.5189 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6549 - accuracy: 0.7392\n",
      "Epoch 85: val_loss improved from 0.51893 to 0.51622, saving model to model.h5\n",
      "13/13 [==============================] - 3s 248ms/step - loss: 0.6549 - accuracy: 0.7392 - val_loss: 0.5162 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.7589\n",
      "Epoch 86: val_loss did not improve from 0.51622\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.6192 - accuracy: 0.7589 - val_loss: 0.5180 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6287 - accuracy: 0.7614\n",
      "Epoch 87: val_loss improved from 0.51622 to 0.51432, saving model to model.h5\n",
      "13/13 [==============================] - 3s 245ms/step - loss: 0.6287 - accuracy: 0.7614 - val_loss: 0.5143 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6301 - accuracy: 0.7589\n",
      "Epoch 88: val_loss improved from 0.51432 to 0.50378, saving model to model.h5\n",
      "13/13 [==============================] - 3s 248ms/step - loss: 0.6301 - accuracy: 0.7589 - val_loss: 0.5038 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6207 - accuracy: 0.7700\n",
      "Epoch 89: val_loss improved from 0.50378 to 0.49689, saving model to model.h5\n",
      "13/13 [==============================] - 3s 248ms/step - loss: 0.6207 - accuracy: 0.7700 - val_loss: 0.4969 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.7675\n",
      "Epoch 90: val_loss did not improve from 0.49689\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.6093 - accuracy: 0.7675 - val_loss: 0.5027 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6102 - accuracy: 0.7626\n",
      "Epoch 91: val_loss did not improve from 0.49689\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.6102 - accuracy: 0.7626 - val_loss: 0.4996 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.7774\n",
      "Epoch 92: val_loss improved from 0.49689 to 0.49462, saving model to model.h5\n",
      "13/13 [==============================] - 3s 249ms/step - loss: 0.5822 - accuracy: 0.7774 - val_loss: 0.4946 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6371 - accuracy: 0.7478\n",
      "Epoch 93: val_loss did not improve from 0.49462\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.6371 - accuracy: 0.7478 - val_loss: 0.5062 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6235 - accuracy: 0.7700\n",
      "Epoch 94: val_loss did not improve from 0.49462\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.6235 - accuracy: 0.7700 - val_loss: 0.5244 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6054 - accuracy: 0.7749\n",
      "Epoch 95: val_loss did not improve from 0.49462\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.6054 - accuracy: 0.7749 - val_loss: 0.5483 - val_accuracy: 0.7818 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5880 - accuracy: 0.7688\n",
      "Epoch 96: val_loss did not improve from 0.49462\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.5880 - accuracy: 0.7688 - val_loss: 0.5237 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5615 - accuracy: 0.7811\n",
      "Epoch 97: val_loss improved from 0.49462 to 0.48387, saving model to model.h5\n",
      "13/13 [==============================] - 3s 245ms/step - loss: 0.5615 - accuracy: 0.7811 - val_loss: 0.4839 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5403 - accuracy: 0.8032\n",
      "Epoch 98: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.5403 - accuracy: 0.8032 - val_loss: 0.4840 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5893 - accuracy: 0.7835\n",
      "Epoch 99: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.5893 - accuracy: 0.7835 - val_loss: 0.4926 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5725 - accuracy: 0.7825\n",
      "Epoch 100: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.5725 - accuracy: 0.7825 - val_loss: 0.4991 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5723 - accuracy: 0.7774\n",
      "Epoch 101: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.5723 - accuracy: 0.7774 - val_loss: 0.4916 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5741 - accuracy: 0.7798\n",
      "Epoch 102: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.5741 - accuracy: 0.7798 - val_loss: 0.4976 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.7909\n",
      "Epoch 103: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.5563 - accuracy: 0.7909 - val_loss: 0.5037 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5705 - accuracy: 0.7872\n",
      "Epoch 104: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.5705 - accuracy: 0.7872 - val_loss: 0.4931 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5711 - accuracy: 0.7909\n",
      "Epoch 105: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.5711 - accuracy: 0.7909 - val_loss: 0.5084 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5722 - accuracy: 0.7847\n",
      "Epoch 106: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.5722 - accuracy: 0.7847 - val_loss: 0.4883 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5625 - accuracy: 0.7921\n",
      "Epoch 107: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.5625 - accuracy: 0.7921 - val_loss: 0.5365 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5703 - accuracy: 0.7909\n",
      "Epoch 108: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 135ms/step - loss: 0.5703 - accuracy: 0.7909 - val_loss: 0.5845 - val_accuracy: 0.7818 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5665 - accuracy: 0.7811\n",
      "Epoch 109: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.5665 - accuracy: 0.7811 - val_loss: 0.5344 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5914 - accuracy: 0.7728\n",
      "Epoch 110: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.5914 - accuracy: 0.7728 - val_loss: 0.5221 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5441 - accuracy: 0.7909\n",
      "Epoch 111: val_loss did not improve from 0.48387\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.5441 - accuracy: 0.7909 - val_loss: 0.4909 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5660 - accuracy: 0.7958\n",
      "Epoch 112: val_loss improved from 0.48387 to 0.47129, saving model to model.h5\n",
      "13/13 [==============================] - 3s 247ms/step - loss: 0.5660 - accuracy: 0.7958 - val_loss: 0.4713 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5197 - accuracy: 0.8093\n",
      "Epoch 113: val_loss improved from 0.47129 to 0.45813, saving model to model.h5\n",
      "13/13 [==============================] - 3s 244ms/step - loss: 0.5197 - accuracy: 0.8093 - val_loss: 0.4581 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5336 - accuracy: 0.7983\n",
      "Epoch 114: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.5336 - accuracy: 0.7983 - val_loss: 0.4629 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5282 - accuracy: 0.8007\n",
      "Epoch 115: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.5282 - accuracy: 0.8007 - val_loss: 0.4640 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5459 - accuracy: 0.8007\n",
      "Epoch 116: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.5459 - accuracy: 0.8007 - val_loss: 0.4760 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5555 - accuracy: 0.7946\n",
      "Epoch 117: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.5555 - accuracy: 0.7946 - val_loss: 0.4836 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5305 - accuracy: 0.8106\n",
      "Epoch 118: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.5305 - accuracy: 0.8106 - val_loss: 0.4933 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5292 - accuracy: 0.7995\n",
      "Epoch 119: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.5292 - accuracy: 0.7995 - val_loss: 0.4887 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5304 - accuracy: 0.8032\n",
      "Epoch 120: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.5304 - accuracy: 0.8032 - val_loss: 0.5266 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5444 - accuracy: 0.8032\n",
      "Epoch 121: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.5444 - accuracy: 0.8032 - val_loss: 0.5614 - val_accuracy: 0.7727 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5326 - accuracy: 0.7958\n",
      "Epoch 122: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.5326 - accuracy: 0.7958 - val_loss: 0.5704 - val_accuracy: 0.7818 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5348 - accuracy: 0.8041\n",
      "Epoch 123: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.5348 - accuracy: 0.8041 - val_loss: 0.5849 - val_accuracy: 0.7727 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5304 - accuracy: 0.8106\n",
      "Epoch 124: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.5304 - accuracy: 0.8106 - val_loss: 0.5583 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.8007\n",
      "Epoch 125: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.5356 - accuracy: 0.8007 - val_loss: 0.5112 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5144 - accuracy: 0.7909\n",
      "Epoch 126: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.5144 - accuracy: 0.7909 - val_loss: 0.5214 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5031 - accuracy: 0.8007\n",
      "Epoch 127: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.5031 - accuracy: 0.8007 - val_loss: 0.5397 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5219 - accuracy: 0.8032\n",
      "Epoch 128: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.5219 - accuracy: 0.8032 - val_loss: 0.5041 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4924 - accuracy: 0.8245\n",
      "Epoch 129: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4924 - accuracy: 0.8245 - val_loss: 0.4832 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5012 - accuracy: 0.8180\n",
      "Epoch 130: val_loss did not improve from 0.45813\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.5012 - accuracy: 0.8180 - val_loss: 0.4673 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4892 - accuracy: 0.8241\n",
      "Epoch 131: val_loss improved from 0.45813 to 0.45175, saving model to model.h5\n",
      "13/13 [==============================] - 3s 250ms/step - loss: 0.4892 - accuracy: 0.8241 - val_loss: 0.4517 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5268 - accuracy: 0.7995\n",
      "Epoch 132: val_loss improved from 0.45175 to 0.45151, saving model to model.h5\n",
      "13/13 [==============================] - 3s 263ms/step - loss: 0.5268 - accuracy: 0.7995 - val_loss: 0.4515 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.8281\n",
      "Epoch 133: val_loss did not improve from 0.45151\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4752 - accuracy: 0.8281 - val_loss: 0.5178 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.8106\n",
      "Epoch 134: val_loss did not improve from 0.45151\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.5053 - accuracy: 0.8106 - val_loss: 0.5047 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.8041\n",
      "Epoch 135: val_loss did not improve from 0.45151\n",
      "13/13 [==============================] - 2s 146ms/step - loss: 0.5127 - accuracy: 0.8041 - val_loss: 0.4734 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5177 - accuracy: 0.8093\n",
      "Epoch 136: val_loss did not improve from 0.45151\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.5177 - accuracy: 0.8093 - val_loss: 0.4610 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5174 - accuracy: 0.8069\n",
      "Epoch 137: val_loss improved from 0.45151 to 0.44292, saving model to model.h5\n",
      "13/13 [==============================] - 3s 251ms/step - loss: 0.5174 - accuracy: 0.8069 - val_loss: 0.4429 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5281 - accuracy: 0.8057\n",
      "Epoch 138: val_loss improved from 0.44292 to 0.44133, saving model to model.h5\n",
      "13/13 [==============================] - 3s 251ms/step - loss: 0.5281 - accuracy: 0.8057 - val_loss: 0.4413 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4998 - accuracy: 0.8069\n",
      "Epoch 139: val_loss did not improve from 0.44133\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.4998 - accuracy: 0.8069 - val_loss: 0.4509 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5067 - accuracy: 0.8143\n",
      "Epoch 140: val_loss did not improve from 0.44133\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.5067 - accuracy: 0.8143 - val_loss: 0.4605 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5006 - accuracy: 0.8143\n",
      "Epoch 141: val_loss did not improve from 0.44133\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.5006 - accuracy: 0.8143 - val_loss: 0.4490 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.8143\n",
      "Epoch 142: val_loss improved from 0.44133 to 0.43277, saving model to model.h5\n",
      "13/13 [==============================] - 3s 248ms/step - loss: 0.4952 - accuracy: 0.8143 - val_loss: 0.4328 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5237 - accuracy: 0.7970\n",
      "Epoch 143: val_loss improved from 0.43277 to 0.42928, saving model to model.h5\n",
      "13/13 [==============================] - 3s 248ms/step - loss: 0.5237 - accuracy: 0.7970 - val_loss: 0.4293 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5081 - accuracy: 0.8118\n",
      "Epoch 144: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.5081 - accuracy: 0.8118 - val_loss: 0.4589 - val_accuracy: 0.8409 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.8143\n",
      "Epoch 145: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4990 - accuracy: 0.8143 - val_loss: 0.4752 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4911 - accuracy: 0.8192\n",
      "Epoch 146: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.4911 - accuracy: 0.8192 - val_loss: 0.4614 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4760 - accuracy: 0.8192\n",
      "Epoch 147: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.4760 - accuracy: 0.8192 - val_loss: 0.4877 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4804 - accuracy: 0.8241\n",
      "Epoch 148: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4804 - accuracy: 0.8241 - val_loss: 0.5049 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4979 - accuracy: 0.8155\n",
      "Epoch 149: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.4979 - accuracy: 0.8155 - val_loss: 0.5145 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4763 - accuracy: 0.8216\n",
      "Epoch 150: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.4763 - accuracy: 0.8216 - val_loss: 0.5358 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5068 - accuracy: 0.8069\n",
      "Epoch 151: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.5068 - accuracy: 0.8069 - val_loss: 0.4917 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4697 - accuracy: 0.8229\n",
      "Epoch 152: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.4697 - accuracy: 0.8229 - val_loss: 0.4911 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4605 - accuracy: 0.8266\n",
      "Epoch 153: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.4605 - accuracy: 0.8266 - val_loss: 0.4632 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4861 - accuracy: 0.8106\n",
      "Epoch 154: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4861 - accuracy: 0.8106 - val_loss: 0.4850 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4522 - accuracy: 0.8315\n",
      "Epoch 155: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.4522 - accuracy: 0.8315 - val_loss: 0.4650 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4696 - accuracy: 0.8266\n",
      "Epoch 156: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4696 - accuracy: 0.8266 - val_loss: 0.4784 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4617 - accuracy: 0.8143\n",
      "Epoch 157: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.4617 - accuracy: 0.8143 - val_loss: 0.4714 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4751 - accuracy: 0.8180\n",
      "Epoch 158: val_loss did not improve from 0.42928\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.4751 - accuracy: 0.8180 - val_loss: 0.4321 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4478 - accuracy: 0.8352\n",
      "Epoch 159: val_loss improved from 0.42928 to 0.41706, saving model to model.h5\n",
      "13/13 [==============================] - 3s 252ms/step - loss: 0.4478 - accuracy: 0.8352 - val_loss: 0.4171 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5067 - accuracy: 0.8093\n",
      "Epoch 160: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.5067 - accuracy: 0.8093 - val_loss: 0.4668 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4400 - accuracy: 0.8352\n",
      "Epoch 161: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4400 - accuracy: 0.8352 - val_loss: 0.5101 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4801 - accuracy: 0.8130\n",
      "Epoch 162: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.4801 - accuracy: 0.8130 - val_loss: 0.5172 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4610 - accuracy: 0.8290\n",
      "Epoch 163: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.4610 - accuracy: 0.8290 - val_loss: 0.4833 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4617 - accuracy: 0.8266\n",
      "Epoch 164: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4617 - accuracy: 0.8266 - val_loss: 0.4514 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4427 - accuracy: 0.8327\n",
      "Epoch 165: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.4427 - accuracy: 0.8327 - val_loss: 0.4941 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4753 - accuracy: 0.8253\n",
      "Epoch 166: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.4753 - accuracy: 0.8253 - val_loss: 0.4754 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4624 - accuracy: 0.8241\n",
      "Epoch 167: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.4624 - accuracy: 0.8241 - val_loss: 0.4503 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4547 - accuracy: 0.8229\n",
      "Epoch 168: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.4547 - accuracy: 0.8229 - val_loss: 0.4687 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4716 - accuracy: 0.8143\n",
      "Epoch 169: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4716 - accuracy: 0.8143 - val_loss: 0.4478 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4428 - accuracy: 0.8339\n",
      "Epoch 170: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4428 - accuracy: 0.8339 - val_loss: 0.4482 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4773 - accuracy: 0.8118\n",
      "Epoch 171: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.4773 - accuracy: 0.8118 - val_loss: 0.4797 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4545 - accuracy: 0.8266\n",
      "Epoch 172: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.4545 - accuracy: 0.8266 - val_loss: 0.4943 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4428 - accuracy: 0.8266\n",
      "Epoch 173: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.4428 - accuracy: 0.8266 - val_loss: 0.5339 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4562 - accuracy: 0.8229\n",
      "Epoch 174: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4562 - accuracy: 0.8229 - val_loss: 0.5717 - val_accuracy: 0.7955 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.8426\n",
      "Epoch 175: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.4204 - accuracy: 0.8426 - val_loss: 0.5958 - val_accuracy: 0.7909 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4525 - accuracy: 0.8269\n",
      "Epoch 176: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4525 - accuracy: 0.8269 - val_loss: 0.5696 - val_accuracy: 0.7955 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4309 - accuracy: 0.8450\n",
      "Epoch 177: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.4309 - accuracy: 0.8450 - val_loss: 0.5411 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4438 - accuracy: 0.8257\n",
      "Epoch 178: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4438 - accuracy: 0.8257 - val_loss: 0.4941 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4734 - accuracy: 0.8167\n",
      "Epoch 179: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4734 - accuracy: 0.8167 - val_loss: 0.5498 - val_accuracy: 0.7955 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4593 - accuracy: 0.8327\n",
      "Epoch 180: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4593 - accuracy: 0.8327 - val_loss: 0.5658 - val_accuracy: 0.7909 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.8180\n",
      "Epoch 181: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.4660 - accuracy: 0.8180 - val_loss: 0.5207 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4180 - accuracy: 0.8426\n",
      "Epoch 182: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 146ms/step - loss: 0.4180 - accuracy: 0.8426 - val_loss: 0.4946 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4530 - accuracy: 0.8327\n",
      "Epoch 183: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4530 - accuracy: 0.8327 - val_loss: 0.5375 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4349 - accuracy: 0.8315\n",
      "Epoch 184: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.4349 - accuracy: 0.8315 - val_loss: 0.5005 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4223 - accuracy: 0.8303\n",
      "Epoch 185: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.4223 - accuracy: 0.8303 - val_loss: 0.5042 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4124 - accuracy: 0.8438\n",
      "Epoch 186: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.4124 - accuracy: 0.8438 - val_loss: 0.4984 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4258 - accuracy: 0.8376\n",
      "Epoch 187: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4258 - accuracy: 0.8376 - val_loss: 0.4880 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4078 - accuracy: 0.8376\n",
      "Epoch 188: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.4078 - accuracy: 0.8376 - val_loss: 0.5403 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4472 - accuracy: 0.8290\n",
      "Epoch 189: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4472 - accuracy: 0.8290 - val_loss: 0.5380 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4000 - accuracy: 0.8487\n",
      "Epoch 190: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4000 - accuracy: 0.8487 - val_loss: 0.5152 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4393 - accuracy: 0.8364\n",
      "Epoch 191: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.4393 - accuracy: 0.8364 - val_loss: 0.4885 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4408 - accuracy: 0.8364\n",
      "Epoch 192: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.4408 - accuracy: 0.8364 - val_loss: 0.4988 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4540 - accuracy: 0.8241\n",
      "Epoch 193: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.4540 - accuracy: 0.8241 - val_loss: 0.4512 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4095 - accuracy: 0.8486\n",
      "Epoch 194: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.4095 - accuracy: 0.8486 - val_loss: 0.4265 - val_accuracy: 0.8409 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4254 - accuracy: 0.8413\n",
      "Epoch 195: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.4254 - accuracy: 0.8413 - val_loss: 0.4199 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4236 - accuracy: 0.8401\n",
      "Epoch 196: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4236 - accuracy: 0.8401 - val_loss: 0.4690 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4584 - accuracy: 0.8278\n",
      "Epoch 197: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.4584 - accuracy: 0.8278 - val_loss: 0.4893 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.8339\n",
      "Epoch 198: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.4241 - accuracy: 0.8339 - val_loss: 0.4932 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4052 - accuracy: 0.8499\n",
      "Epoch 199: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.4052 - accuracy: 0.8499 - val_loss: 0.4576 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4221 - accuracy: 0.8413\n",
      "Epoch 200: val_loss did not improve from 0.41706\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.4221 - accuracy: 0.8413 - val_loss: 0.4459 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.4459 - accuracy: 0.8364\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 0.4426 - accuracy: 0.8278\n",
      "Model Performance for Xception\n",
      "Loss(Test): 0.44587206840515137, Accuracy(Test): 83.6%\n",
      "Loss(Train): 0.4426253139972687, Accuracy(Train): 82.8%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VGG19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_36 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 64, 64, 3)         84        \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, None, None, 512)   14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_11  (None, 512)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_46 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_47 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " root (Dense)                (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,849,943\n",
      "Trainable params: 14,848,407\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.8178 - accuracy: 0.3456\n",
      "Epoch 1: val_loss improved from inf to 1.30879, saving model to model.h5\n",
      "13/13 [==============================] - 6s 254ms/step - loss: 1.8178 - accuracy: 0.3456 - val_loss: 1.3088 - val_accuracy: 0.3727 - lr: 0.0020\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7552 - accuracy: 0.3788\n",
      "Epoch 2: val_loss did not improve from 1.30879\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 1.7552 - accuracy: 0.3788 - val_loss: 2.0306 - val_accuracy: 0.1182 - lr: 0.0020\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.4560 - accuracy: 0.4600\n",
      "Epoch 3: val_loss did not improve from 1.30879\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 1.4560 - accuracy: 0.4600 - val_loss: 2.5842 - val_accuracy: 0.1136 - lr: 0.0020\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.4495 - accuracy: 0.4822\n",
      "Epoch 4: val_loss did not improve from 1.30879\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 1.4495 - accuracy: 0.4822 - val_loss: 2.1736 - val_accuracy: 0.1136 - lr: 0.0020\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3441 - accuracy: 0.5092\n",
      "Epoch 5: val_loss did not improve from 1.30879\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 1.3441 - accuracy: 0.5092 - val_loss: 4.2139 - val_accuracy: 0.1136 - lr: 0.0020\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2327 - accuracy: 0.5732\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.30879\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 1.2327 - accuracy: 0.5732 - val_loss: 4.2617 - val_accuracy: 0.1136 - lr: 0.0020\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1747 - accuracy: 0.5683\n",
      "Epoch 7: val_loss did not improve from 1.30879\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 1.1747 - accuracy: 0.5683 - val_loss: 2.9339 - val_accuracy: 0.1955 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0728 - accuracy: 0.6162\n",
      "Epoch 8: val_loss did not improve from 1.30879\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 1.0728 - accuracy: 0.6162 - val_loss: 3.8845 - val_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9552 - accuracy: 0.6408\n",
      "Epoch 9: val_loss did not improve from 1.30879\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.9552 - accuracy: 0.6408 - val_loss: 2.6758 - val_accuracy: 0.3682 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9241 - accuracy: 0.6704\n",
      "Epoch 10: val_loss improved from 1.30879 to 1.25694, saving model to model.h5\n",
      "13/13 [==============================] - 2s 171ms/step - loss: 0.9241 - accuracy: 0.6704 - val_loss: 1.2569 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8995 - accuracy: 0.6716\n",
      "Epoch 11: val_loss did not improve from 1.25694\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 0.8995 - accuracy: 0.6716 - val_loss: 1.8673 - val_accuracy: 0.4409 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9084 - accuracy: 0.6765\n",
      "Epoch 12: val_loss improved from 1.25694 to 0.87105, saving model to model.h5\n",
      "13/13 [==============================] - 2s 170ms/step - loss: 0.9084 - accuracy: 0.6765 - val_loss: 0.8710 - val_accuracy: 0.5364 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8620 - accuracy: 0.6802\n",
      "Epoch 13: val_loss did not improve from 0.87105\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.8620 - accuracy: 0.6802 - val_loss: 4.2468 - val_accuracy: 0.3727 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9061 - accuracy: 0.7048\n",
      "Epoch 14: val_loss did not improve from 0.87105\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.9061 - accuracy: 0.7048 - val_loss: 3.0955 - val_accuracy: 0.3727 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8283 - accuracy: 0.7220\n",
      "Epoch 15: val_loss did not improve from 0.87105\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.8283 - accuracy: 0.7220 - val_loss: 1.9544 - val_accuracy: 0.4591 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8454 - accuracy: 0.7134\n",
      "Epoch 16: val_loss did not improve from 0.87105\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.8454 - accuracy: 0.7134 - val_loss: 1.2238 - val_accuracy: 0.5227 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8281 - accuracy: 0.7011\n",
      "Epoch 17: val_loss improved from 0.87105 to 0.77072, saving model to model.h5\n",
      "13/13 [==============================] - 2s 172ms/step - loss: 0.8281 - accuracy: 0.7011 - val_loss: 0.7707 - val_accuracy: 0.6045 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8292 - accuracy: 0.6962\n",
      "Epoch 18: val_loss did not improve from 0.77072\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.8292 - accuracy: 0.6962 - val_loss: 4.6941 - val_accuracy: 0.3727 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7998 - accuracy: 0.7036\n",
      "Epoch 19: val_loss did not improve from 0.77072\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.7998 - accuracy: 0.7036 - val_loss: 1.7560 - val_accuracy: 0.5636 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7375 - accuracy: 0.7208\n",
      "Epoch 20: val_loss did not improve from 0.77072\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.7375 - accuracy: 0.7208 - val_loss: 3.1255 - val_accuracy: 0.4455 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7456 - accuracy: 0.7466\n",
      "Epoch 21: val_loss did not improve from 0.77072\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 0.7456 - accuracy: 0.7466 - val_loss: 8.9282 - val_accuracy: 0.3727 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7190 - accuracy: 0.7417\n",
      "Epoch 22: val_loss did not improve from 0.77072\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 0.7190 - accuracy: 0.7417 - val_loss: 0.9970 - val_accuracy: 0.7227 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7039 - accuracy: 0.7429\n",
      "Epoch 23: val_loss did not improve from 0.77072\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.7039 - accuracy: 0.7429 - val_loss: 1.0097 - val_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7070 - accuracy: 0.7466\n",
      "Epoch 24: val_loss improved from 0.77072 to 0.76494, saving model to model.h5\n",
      "13/13 [==============================] - 2s 170ms/step - loss: 0.7070 - accuracy: 0.7466 - val_loss: 0.7649 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7052 - accuracy: 0.7442\n",
      "Epoch 25: val_loss did not improve from 0.76494\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 0.7052 - accuracy: 0.7442 - val_loss: 1.7914 - val_accuracy: 0.5773 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7003 - accuracy: 0.7577\n",
      "Epoch 26: val_loss improved from 0.76494 to 0.60337, saving model to model.h5\n",
      "13/13 [==============================] - 2s 170ms/step - loss: 0.7003 - accuracy: 0.7577 - val_loss: 0.6034 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6612 - accuracy: 0.7577\n",
      "Epoch 27: val_loss did not improve from 0.60337\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.6612 - accuracy: 0.7577 - val_loss: 2.0931 - val_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6366 - accuracy: 0.7601\n",
      "Epoch 28: val_loss did not improve from 0.60337\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 0.6366 - accuracy: 0.7601 - val_loss: 1.7927 - val_accuracy: 0.5864 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6110 - accuracy: 0.7724\n",
      "Epoch 29: val_loss improved from 0.60337 to 0.53139, saving model to model.h5\n",
      "13/13 [==============================] - 2s 171ms/step - loss: 0.6110 - accuracy: 0.7724 - val_loss: 0.5314 - val_accuracy: 0.7818 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6337 - accuracy: 0.7512\n",
      "Epoch 30: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.6337 - accuracy: 0.7512 - val_loss: 7.9840 - val_accuracy: 0.3773 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6509 - accuracy: 0.7724\n",
      "Epoch 31: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.6509 - accuracy: 0.7724 - val_loss: 0.6038 - val_accuracy: 0.7727 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5925 - accuracy: 0.7811\n",
      "Epoch 32: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 0.5925 - accuracy: 0.7811 - val_loss: 0.7920 - val_accuracy: 0.7591 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5989 - accuracy: 0.7921\n",
      "Epoch 33: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 0.5989 - accuracy: 0.7921 - val_loss: 2.3086 - val_accuracy: 0.5409 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6887 - accuracy: 0.7417\n",
      "Epoch 34: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 0.6887 - accuracy: 0.7417 - val_loss: 1.9647 - val_accuracy: 0.5636 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5964 - accuracy: 0.7737\n",
      "Epoch 35: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.5964 - accuracy: 0.7737 - val_loss: 1.6081 - val_accuracy: 0.5955 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5394 - accuracy: 0.7970\n",
      "Epoch 36: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.5394 - accuracy: 0.7970 - val_loss: 1.0744 - val_accuracy: 0.6773 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5891 - accuracy: 0.7860\n",
      "Epoch 37: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.5891 - accuracy: 0.7860 - val_loss: 1.6419 - val_accuracy: 0.5955 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.7946\n",
      "Epoch 38: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.5288 - accuracy: 0.7946 - val_loss: 3.1299 - val_accuracy: 0.5318 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6218 - accuracy: 0.7749\n",
      "Epoch 39: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.6218 - accuracy: 0.7749 - val_loss: 0.8826 - val_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5876 - accuracy: 0.7847\n",
      "Epoch 40: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.5876 - accuracy: 0.7847 - val_loss: 1.6255 - val_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.7983\n",
      "Epoch 41: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.5621 - accuracy: 0.7983 - val_loss: 0.9818 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5557 - accuracy: 0.7921\n",
      "Epoch 42: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 0.5557 - accuracy: 0.7921 - val_loss: 0.6191 - val_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4928 - accuracy: 0.8118\n",
      "Epoch 43: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.4928 - accuracy: 0.8118 - val_loss: 0.5340 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5298 - accuracy: 0.7946\n",
      "Epoch 44: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 0.5298 - accuracy: 0.7946 - val_loss: 0.5346 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4825 - accuracy: 0.8241\n",
      "Epoch 45: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.4825 - accuracy: 0.8241 - val_loss: 0.8338 - val_accuracy: 0.7136 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5657 - accuracy: 0.7786\n",
      "Epoch 46: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.5657 - accuracy: 0.7786 - val_loss: 0.5646 - val_accuracy: 0.7818 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5054 - accuracy: 0.8118\n",
      "Epoch 47: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 128ms/step - loss: 0.5054 - accuracy: 0.8118 - val_loss: 0.9003 - val_accuracy: 0.7409 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5363 - accuracy: 0.7958\n",
      "Epoch 48: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.5363 - accuracy: 0.7958 - val_loss: 0.5448 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5001 - accuracy: 0.7909\n",
      "Epoch 49: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.5001 - accuracy: 0.7909 - val_loss: 0.6511 - val_accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5392 - accuracy: 0.7983\n",
      "Epoch 50: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.5392 - accuracy: 0.7983 - val_loss: 2.2482 - val_accuracy: 0.5727 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5662 - accuracy: 0.7860\n",
      "Epoch 51: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 0.5662 - accuracy: 0.7860 - val_loss: 7.6549 - val_accuracy: 0.3727 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5290 - accuracy: 0.8007\n",
      "Epoch 52: val_loss did not improve from 0.53139\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.5290 - accuracy: 0.8007 - val_loss: 2.7701 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4894 - accuracy: 0.8229\n",
      "Epoch 53: val_loss improved from 0.53139 to 0.49692, saving model to model.h5\n",
      "13/13 [==============================] - 2s 172ms/step - loss: 0.4894 - accuracy: 0.8229 - val_loss: 0.4969 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.8081\n",
      "Epoch 54: val_loss improved from 0.49692 to 0.46106, saving model to model.h5\n",
      "13/13 [==============================] - 2s 171ms/step - loss: 0.5037 - accuracy: 0.8081 - val_loss: 0.4611 - val_accuracy: 0.7773 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4930 - accuracy: 0.8155\n",
      "Epoch 55: val_loss did not improve from 0.46106\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.4930 - accuracy: 0.8155 - val_loss: 1.1274 - val_accuracy: 0.6318 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4593 - accuracy: 0.8278\n",
      "Epoch 56: val_loss did not improve from 0.46106\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.4593 - accuracy: 0.8278 - val_loss: 4.4198 - val_accuracy: 0.1455 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5203 - accuracy: 0.7921\n",
      "Epoch 57: val_loss did not improve from 0.46106\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.5203 - accuracy: 0.7921 - val_loss: 0.5941 - val_accuracy: 0.7318 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5038 - accuracy: 0.8081\n",
      "Epoch 58: val_loss improved from 0.46106 to 0.39049, saving model to model.h5\n",
      "13/13 [==============================] - 2s 173ms/step - loss: 0.5038 - accuracy: 0.8081 - val_loss: 0.3905 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4645 - accuracy: 0.8290\n",
      "Epoch 59: val_loss did not improve from 0.39049\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.4645 - accuracy: 0.8290 - val_loss: 1.3816 - val_accuracy: 0.6045 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4617 - accuracy: 0.8327\n",
      "Epoch 60: val_loss did not improve from 0.39049\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.4617 - accuracy: 0.8327 - val_loss: 0.6563 - val_accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4471 - accuracy: 0.8327\n",
      "Epoch 61: val_loss did not improve from 0.39049\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.4471 - accuracy: 0.8327 - val_loss: 0.7799 - val_accuracy: 0.7591 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4414 - accuracy: 0.8216\n",
      "Epoch 62: val_loss did not improve from 0.39049\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.4414 - accuracy: 0.8216 - val_loss: 2.9794 - val_accuracy: 0.4136 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4391 - accuracy: 0.8241\n",
      "Epoch 63: val_loss did not improve from 0.39049\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.4391 - accuracy: 0.8241 - val_loss: 0.9992 - val_accuracy: 0.6864 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4668 - accuracy: 0.8204\n",
      "Epoch 64: val_loss did not improve from 0.39049\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.4668 - accuracy: 0.8204 - val_loss: 2.6704 - val_accuracy: 0.5591 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4164 - accuracy: 0.8315\n",
      "Epoch 65: val_loss did not improve from 0.39049\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.4164 - accuracy: 0.8315 - val_loss: 2.1284 - val_accuracy: 0.5227 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4451 - accuracy: 0.8253\n",
      "Epoch 66: val_loss did not improve from 0.39049\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.4451 - accuracy: 0.8253 - val_loss: 1.2389 - val_accuracy: 0.6409 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4588 - accuracy: 0.8216\n",
      "Epoch 67: val_loss improved from 0.39049 to 0.37984, saving model to model.h5\n",
      "13/13 [==============================] - 2s 177ms/step - loss: 0.4588 - accuracy: 0.8216 - val_loss: 0.3798 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4217 - accuracy: 0.8204\n",
      "Epoch 68: val_loss did not improve from 0.37984\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.4217 - accuracy: 0.8204 - val_loss: 0.3978 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4477 - accuracy: 0.8192\n",
      "Epoch 69: val_loss did not improve from 0.37984\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.4477 - accuracy: 0.8192 - val_loss: 3.3466 - val_accuracy: 0.2091 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4271 - accuracy: 0.8413\n",
      "Epoch 70: val_loss did not improve from 0.37984\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.4271 - accuracy: 0.8413 - val_loss: 0.6744 - val_accuracy: 0.7091 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3922 - accuracy: 0.8281\n",
      "Epoch 71: val_loss did not improve from 0.37984\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.3922 - accuracy: 0.8281 - val_loss: 0.7031 - val_accuracy: 0.7136 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4351 - accuracy: 0.8352\n",
      "Epoch 72: val_loss did not improve from 0.37984\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.4351 - accuracy: 0.8352 - val_loss: 0.6414 - val_accuracy: 0.6545 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.8376\n",
      "Epoch 73: val_loss did not improve from 0.37984\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.3876 - accuracy: 0.8376 - val_loss: 0.6752 - val_accuracy: 0.6273 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4758 - accuracy: 0.7934\n",
      "Epoch 74: val_loss improved from 0.37984 to 0.34159, saving model to model.h5\n",
      "13/13 [==============================] - 2s 175ms/step - loss: 0.4758 - accuracy: 0.7934 - val_loss: 0.3416 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4227 - accuracy: 0.8253\n",
      "Epoch 75: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.4227 - accuracy: 0.8253 - val_loss: 0.4136 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4191 - accuracy: 0.8192\n",
      "Epoch 76: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.4191 - accuracy: 0.8192 - val_loss: 0.6855 - val_accuracy: 0.7227 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4396 - accuracy: 0.8253\n",
      "Epoch 77: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.4396 - accuracy: 0.8253 - val_loss: 0.3753 - val_accuracy: 0.8727 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3875 - accuracy: 0.8389\n",
      "Epoch 78: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.3875 - accuracy: 0.8389 - val_loss: 0.4751 - val_accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4733 - accuracy: 0.8101\n",
      "Epoch 79: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.4733 - accuracy: 0.8101 - val_loss: 1.9138 - val_accuracy: 0.5909 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4254 - accuracy: 0.8216\n",
      "Epoch 80: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.4254 - accuracy: 0.8216 - val_loss: 1.2501 - val_accuracy: 0.6318 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.8462\n",
      "Epoch 81: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.4183 - accuracy: 0.8462 - val_loss: 0.5152 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4283 - accuracy: 0.8376\n",
      "Epoch 82: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.4283 - accuracy: 0.8376 - val_loss: 0.8216 - val_accuracy: 0.7318 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.8462\n",
      "Epoch 83: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.3678 - accuracy: 0.8462 - val_loss: 0.5132 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3899 - accuracy: 0.8401\n",
      "Epoch 84: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 0.3899 - accuracy: 0.8401 - val_loss: 0.8080 - val_accuracy: 0.6773 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.8573\n",
      "Epoch 85: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.3577 - accuracy: 0.8573 - val_loss: 0.6517 - val_accuracy: 0.7545 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3981 - accuracy: 0.8462\n",
      "Epoch 86: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.3981 - accuracy: 0.8462 - val_loss: 0.4413 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8462\n",
      "Epoch 87: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.3639 - accuracy: 0.8462 - val_loss: 0.4040 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3705 - accuracy: 0.8413\n",
      "Epoch 88: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.3705 - accuracy: 0.8413 - val_loss: 0.4320 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3953 - accuracy: 0.8364\n",
      "Epoch 89: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.3953 - accuracy: 0.8364 - val_loss: 1.6860 - val_accuracy: 0.4409 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4160 - accuracy: 0.8512\n",
      "Epoch 90: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.4160 - accuracy: 0.8512 - val_loss: 0.9601 - val_accuracy: 0.6636 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3623 - accuracy: 0.8558\n",
      "Epoch 91: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.3623 - accuracy: 0.8558 - val_loss: 1.3652 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3784 - accuracy: 0.8438\n",
      "Epoch 92: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.3784 - accuracy: 0.8438 - val_loss: 2.7827 - val_accuracy: 0.4636 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3619 - accuracy: 0.8524\n",
      "Epoch 93: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.3619 - accuracy: 0.8524 - val_loss: 0.3741 - val_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3624 - accuracy: 0.8598\n",
      "Epoch 94: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.3624 - accuracy: 0.8598 - val_loss: 0.4655 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4017 - accuracy: 0.8315\n",
      "Epoch 95: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.4017 - accuracy: 0.8315 - val_loss: 0.6412 - val_accuracy: 0.7545 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3736 - accuracy: 0.8450\n",
      "Epoch 96: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.3736 - accuracy: 0.8450 - val_loss: 0.3518 - val_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3253 - accuracy: 0.8499\n",
      "Epoch 97: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.3253 - accuracy: 0.8499 - val_loss: 0.3816 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3571 - accuracy: 0.8475\n",
      "Epoch 98: val_loss did not improve from 0.34159\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 0.3571 - accuracy: 0.8475 - val_loss: 0.3938 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3295 - accuracy: 0.8672\n",
      "Epoch 99: val_loss improved from 0.34159 to 0.31378, saving model to model.h5\n",
      "13/13 [==============================] - 2s 179ms/step - loss: 0.3295 - accuracy: 0.8672 - val_loss: 0.3138 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3533 - accuracy: 0.8696\n",
      "Epoch 100: val_loss did not improve from 0.31378\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.3533 - accuracy: 0.8696 - val_loss: 0.5636 - val_accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3135 - accuracy: 0.8708\n",
      "Epoch 101: val_loss did not improve from 0.31378\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.3135 - accuracy: 0.8708 - val_loss: 0.4345 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.8524\n",
      "Epoch 102: val_loss did not improve from 0.31378\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.3454 - accuracy: 0.8524 - val_loss: 1.1213 - val_accuracy: 0.5682 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.8462\n",
      "Epoch 103: val_loss did not improve from 0.31378\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.3495 - accuracy: 0.8462 - val_loss: 5.6300 - val_accuracy: 0.1727 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3230 - accuracy: 0.8696\n",
      "Epoch 104: val_loss did not improve from 0.31378\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.3230 - accuracy: 0.8696 - val_loss: 1.6318 - val_accuracy: 0.4864 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.8782\n",
      "Epoch 105: val_loss did not improve from 0.31378\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.2973 - accuracy: 0.8782 - val_loss: 1.5495 - val_accuracy: 0.4409 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3037 - accuracy: 0.8733\n",
      "Epoch 106: val_loss did not improve from 0.31378\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.3037 - accuracy: 0.8733 - val_loss: 0.7624 - val_accuracy: 0.7318 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3297 - accuracy: 0.8708\n",
      "Epoch 107: val_loss did not improve from 0.31378\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.3297 - accuracy: 0.8708 - val_loss: 0.7368 - val_accuracy: 0.7773 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.8684\n",
      "Epoch 108: val_loss improved from 0.31378 to 0.30413, saving model to model.h5\n",
      "13/13 [==============================] - 2s 175ms/step - loss: 0.3046 - accuracy: 0.8684 - val_loss: 0.3041 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2994 - accuracy: 0.8868\n",
      "Epoch 109: val_loss did not improve from 0.30413\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.2994 - accuracy: 0.8868 - val_loss: 0.3589 - val_accuracy: 0.8409 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3205 - accuracy: 0.8672\n",
      "Epoch 110: val_loss did not improve from 0.30413\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.3205 - accuracy: 0.8672 - val_loss: 0.5277 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3155 - accuracy: 0.8758\n",
      "Epoch 111: val_loss did not improve from 0.30413\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.3155 - accuracy: 0.8758 - val_loss: 0.3951 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3366 - accuracy: 0.8745\n",
      "Epoch 112: val_loss did not improve from 0.30413\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.3366 - accuracy: 0.8745 - val_loss: 0.6941 - val_accuracy: 0.7636 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3676 - accuracy: 0.8558\n",
      "Epoch 113: val_loss did not improve from 0.30413\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.3676 - accuracy: 0.8558 - val_loss: 0.4381 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3077 - accuracy: 0.8745\n",
      "Epoch 114: val_loss did not improve from 0.30413\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.3077 - accuracy: 0.8745 - val_loss: 0.4218 - val_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3657 - accuracy: 0.8549\n",
      "Epoch 115: val_loss did not improve from 0.30413\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.3657 - accuracy: 0.8549 - val_loss: 0.8070 - val_accuracy: 0.7591 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3430 - accuracy: 0.8598\n",
      "Epoch 116: val_loss did not improve from 0.30413\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.3430 - accuracy: 0.8598 - val_loss: 1.7558 - val_accuracy: 0.5909 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.8487\n",
      "Epoch 117: val_loss did not improve from 0.30413\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.3410 - accuracy: 0.8487 - val_loss: 0.7158 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3225 - accuracy: 0.8610\n",
      "Epoch 118: val_loss did not improve from 0.30413\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.3225 - accuracy: 0.8610 - val_loss: 1.0405 - val_accuracy: 0.7909 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3231 - accuracy: 0.8622\n",
      "Epoch 119: val_loss did not improve from 0.30413\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.3231 - accuracy: 0.8622 - val_loss: 0.5513 - val_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3228 - accuracy: 0.8672\n",
      "Epoch 120: val_loss did not improve from 0.30413\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.3228 - accuracy: 0.8672 - val_loss: 0.3710 - val_accuracy: 0.8409 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3147 - accuracy: 0.8647\n",
      "Epoch 121: val_loss improved from 0.30413 to 0.28456, saving model to model.h5\n",
      "13/13 [==============================] - 2s 175ms/step - loss: 0.3147 - accuracy: 0.8647 - val_loss: 0.2846 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2830 - accuracy: 0.8906\n",
      "Epoch 122: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 129ms/step - loss: 0.2830 - accuracy: 0.8906 - val_loss: 0.4386 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3125 - accuracy: 0.8831\n",
      "Epoch 123: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.3125 - accuracy: 0.8831 - val_loss: 0.3143 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.8782\n",
      "Epoch 124: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2847 - accuracy: 0.8782 - val_loss: 0.4934 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2920 - accuracy: 0.8870\n",
      "Epoch 125: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2920 - accuracy: 0.8870 - val_loss: 0.3577 - val_accuracy: 0.8727 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.8647\n",
      "Epoch 126: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.3094 - accuracy: 0.8647 - val_loss: 0.3733 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.8967\n",
      "Epoch 127: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2854 - accuracy: 0.8967 - val_loss: 0.3045 - val_accuracy: 0.8818 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.8745\n",
      "Epoch 128: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 130ms/step - loss: 0.3098 - accuracy: 0.8745 - val_loss: 0.7724 - val_accuracy: 0.7409 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.8795\n",
      "Epoch 129: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.2808 - accuracy: 0.8795 - val_loss: 0.3986 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.8819\n",
      "Epoch 130: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.2938 - accuracy: 0.8819 - val_loss: 0.4392 - val_accuracy: 0.7955 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2993 - accuracy: 0.8721\n",
      "Epoch 131: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.2993 - accuracy: 0.8721 - val_loss: 0.3083 - val_accuracy: 0.8727 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.8882\n",
      "Epoch 132: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 131ms/step - loss: 0.2714 - accuracy: 0.8882 - val_loss: 0.3522 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.8831\n",
      "Epoch 133: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.2855 - accuracy: 0.8831 - val_loss: 0.5794 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2761 - accuracy: 0.8868\n",
      "Epoch 134: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2761 - accuracy: 0.8868 - val_loss: 0.6526 - val_accuracy: 0.6682 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3103 - accuracy: 0.8856\n",
      "Epoch 135: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 129ms/step - loss: 0.3103 - accuracy: 0.8856 - val_loss: 0.5995 - val_accuracy: 0.7909 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3244 - accuracy: 0.8702\n",
      "Epoch 136: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.3244 - accuracy: 0.8702 - val_loss: 0.7423 - val_accuracy: 0.7318 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2797 - accuracy: 0.8905\n",
      "Epoch 137: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.2797 - accuracy: 0.8905 - val_loss: 0.5356 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.8868\n",
      "Epoch 138: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.2822 - accuracy: 0.8868 - val_loss: 0.4129 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.8758\n",
      "Epoch 139: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2904 - accuracy: 0.8758 - val_loss: 1.4147 - val_accuracy: 0.5545 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3239 - accuracy: 0.8831\n",
      "Epoch 140: val_loss did not improve from 0.28456\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.3239 - accuracy: 0.8831 - val_loss: 0.3801 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.8868\n",
      "Epoch 141: val_loss improved from 0.28456 to 0.27779, saving model to model.h5\n",
      "13/13 [==============================] - 2s 174ms/step - loss: 0.2629 - accuracy: 0.8868 - val_loss: 0.2778 - val_accuracy: 0.8864 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.8672\n",
      "Epoch 142: val_loss did not improve from 0.27779\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.3049 - accuracy: 0.8672 - val_loss: 1.2872 - val_accuracy: 0.6455 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2729 - accuracy: 0.8819\n",
      "Epoch 143: val_loss did not improve from 0.27779\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2729 - accuracy: 0.8819 - val_loss: 0.2944 - val_accuracy: 0.8773 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2809 - accuracy: 0.8831\n",
      "Epoch 144: val_loss did not improve from 0.27779\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.2809 - accuracy: 0.8831 - val_loss: 0.3286 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.8893\n",
      "Epoch 145: val_loss did not improve from 0.27779\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.2763 - accuracy: 0.8893 - val_loss: 0.5697 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.9004\n",
      "Epoch 146: val_loss improved from 0.27779 to 0.25324, saving model to model.h5\n",
      "13/13 [==============================] - 2s 179ms/step - loss: 0.2594 - accuracy: 0.9004 - val_loss: 0.2532 - val_accuracy: 0.8727 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2788 - accuracy: 0.8906\n",
      "Epoch 147: val_loss did not improve from 0.25324\n",
      "13/13 [==============================] - 2s 131ms/step - loss: 0.2788 - accuracy: 0.8906 - val_loss: 0.3384 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9065\n",
      "Epoch 148: val_loss did not improve from 0.25324\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2576 - accuracy: 0.9065 - val_loss: 0.4186 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.8930\n",
      "Epoch 149: val_loss did not improve from 0.25324\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.2749 - accuracy: 0.8930 - val_loss: 0.4028 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.8786\n",
      "Epoch 150: val_loss did not improve from 0.25324\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.2942 - accuracy: 0.8786 - val_loss: 0.3146 - val_accuracy: 0.8818 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2809 - accuracy: 0.8868\n",
      "Epoch 151: val_loss did not improve from 0.25324\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.2809 - accuracy: 0.8868 - val_loss: 0.4266 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.8991\n",
      "Epoch 152: val_loss did not improve from 0.25324\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 0.2436 - accuracy: 0.8991 - val_loss: 0.3888 - val_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.8954\n",
      "Epoch 153: val_loss did not improve from 0.25324\n",
      "13/13 [==============================] - 2s 128ms/step - loss: 0.2629 - accuracy: 0.8954 - val_loss: 0.5313 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.8868\n",
      "Epoch 154: val_loss improved from 0.25324 to 0.21654, saving model to model.h5\n",
      "13/13 [==============================] - 2s 174ms/step - loss: 0.2665 - accuracy: 0.8868 - val_loss: 0.2165 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.8721\n",
      "Epoch 155: val_loss did not improve from 0.21654\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2890 - accuracy: 0.8721 - val_loss: 1.0414 - val_accuracy: 0.5818 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.8795\n",
      "Epoch 156: val_loss did not improve from 0.21654\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.2779 - accuracy: 0.8795 - val_loss: 0.3195 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.8870\n",
      "Epoch 157: val_loss did not improve from 0.21654\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.2757 - accuracy: 0.8870 - val_loss: 0.5642 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 0.8856\n",
      "Epoch 158: val_loss did not improve from 0.21654\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2815 - accuracy: 0.8856 - val_loss: 0.4098 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2683 - accuracy: 0.9004\n",
      "Epoch 159: val_loss did not improve from 0.21654\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2683 - accuracy: 0.9004 - val_loss: 0.6740 - val_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.8942\n",
      "Epoch 160: val_loss did not improve from 0.21654\n",
      "13/13 [==============================] - 2s 131ms/step - loss: 0.2472 - accuracy: 0.8942 - val_loss: 0.3523 - val_accuracy: 0.8773 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8893\n",
      "Epoch 161: val_loss did not improve from 0.21654\n",
      "13/13 [==============================] - 2s 133ms/step - loss: 0.2913 - accuracy: 0.8893 - val_loss: 0.4369 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2357 - accuracy: 0.9102\n",
      "Epoch 162: val_loss did not improve from 0.21654\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.2357 - accuracy: 0.9102 - val_loss: 0.3151 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2634 - accuracy: 0.8954\n",
      "Epoch 163: val_loss improved from 0.21654 to 0.20397, saving model to model.h5\n",
      "13/13 [==============================] - 2s 179ms/step - loss: 0.2634 - accuracy: 0.8954 - val_loss: 0.2040 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2286 - accuracy: 0.9151\n",
      "Epoch 164: val_loss did not improve from 0.20397\n",
      "13/13 [==============================] - 2s 134ms/step - loss: 0.2286 - accuracy: 0.9151 - val_loss: 0.2050 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.8942\n",
      "Epoch 165: val_loss did not improve from 0.20397\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.2635 - accuracy: 0.8942 - val_loss: 1.2142 - val_accuracy: 0.6045 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.9053\n",
      "Epoch 166: val_loss did not improve from 0.20397\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.2477 - accuracy: 0.9053 - val_loss: 0.2145 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.9050\n",
      "Epoch 167: val_loss did not improve from 0.20397\n",
      "13/13 [==============================] - 2s 131ms/step - loss: 0.2594 - accuracy: 0.9050 - val_loss: 2.7005 - val_accuracy: 0.4409 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2371 - accuracy: 0.9090\n",
      "Epoch 168: val_loss did not improve from 0.20397\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2371 - accuracy: 0.9090 - val_loss: 0.3279 - val_accuracy: 0.8773 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.8979\n",
      "Epoch 169: val_loss improved from 0.20397 to 0.19745, saving model to model.h5\n",
      "13/13 [==============================] - 2s 176ms/step - loss: 0.2594 - accuracy: 0.8979 - val_loss: 0.1975 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2360 - accuracy: 0.9028\n",
      "Epoch 170: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 129ms/step - loss: 0.2360 - accuracy: 0.9028 - val_loss: 1.4195 - val_accuracy: 0.6182 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.8967\n",
      "Epoch 171: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 128ms/step - loss: 0.2540 - accuracy: 0.8967 - val_loss: 0.4660 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.9077\n",
      "Epoch 172: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.2430 - accuracy: 0.9077 - val_loss: 0.4889 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.8967\n",
      "Epoch 173: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 134ms/step - loss: 0.2477 - accuracy: 0.8967 - val_loss: 0.2063 - val_accuracy: 0.9227 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.8967\n",
      "Epoch 174: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.2717 - accuracy: 0.8967 - val_loss: 0.5185 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.9053\n",
      "Epoch 175: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 132ms/step - loss: 0.2449 - accuracy: 0.9053 - val_loss: 0.2841 - val_accuracy: 0.8818 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.8930\n",
      "Epoch 176: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 128ms/step - loss: 0.2504 - accuracy: 0.8930 - val_loss: 0.3473 - val_accuracy: 0.8727 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2230 - accuracy: 0.9123\n",
      "Epoch 177: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 133ms/step - loss: 0.2230 - accuracy: 0.9123 - val_loss: 0.2274 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2202 - accuracy: 0.9102\n",
      "Epoch 178: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 0.2202 - accuracy: 0.9102 - val_loss: 0.3750 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.9164\n",
      "Epoch 179: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.2134 - accuracy: 0.9164 - val_loss: 0.4286 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.9164\n",
      "Epoch 180: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 130ms/step - loss: 0.2193 - accuracy: 0.9164 - val_loss: 0.2433 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2195 - accuracy: 0.9114\n",
      "Epoch 181: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2195 - accuracy: 0.9114 - val_loss: 1.1402 - val_accuracy: 0.6682 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2210 - accuracy: 0.9114\n",
      "Epoch 182: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 128ms/step - loss: 0.2210 - accuracy: 0.9114 - val_loss: 0.2757 - val_accuracy: 0.8773 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2116 - accuracy: 0.9114\n",
      "Epoch 183: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2116 - accuracy: 0.9114 - val_loss: 0.8561 - val_accuracy: 0.7318 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2334 - accuracy: 0.9041\n",
      "Epoch 184: val_loss did not improve from 0.19745\n",
      "13/13 [==============================] - 2s 128ms/step - loss: 0.2334 - accuracy: 0.9041 - val_loss: 0.2890 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2249 - accuracy: 0.9127\n",
      "Epoch 185: val_loss improved from 0.19745 to 0.19258, saving model to model.h5\n",
      "13/13 [==============================] - 2s 176ms/step - loss: 0.2249 - accuracy: 0.9127 - val_loss: 0.1926 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1892 - accuracy: 0.9336\n",
      "Epoch 186: val_loss improved from 0.19258 to 0.16657, saving model to model.h5\n",
      "13/13 [==============================] - 2s 175ms/step - loss: 0.1892 - accuracy: 0.9336 - val_loss: 0.1666 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.8991\n",
      "Epoch 187: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 130ms/step - loss: 0.2483 - accuracy: 0.8991 - val_loss: 0.4746 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.8991\n",
      "Epoch 188: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.2321 - accuracy: 0.8991 - val_loss: 0.2002 - val_accuracy: 0.9318 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.9114\n",
      "Epoch 189: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.2299 - accuracy: 0.9114 - val_loss: 0.1957 - val_accuracy: 0.9273 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.9151\n",
      "Epoch 190: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 128ms/step - loss: 0.2196 - accuracy: 0.9151 - val_loss: 0.4995 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2261 - accuracy: 0.9114\n",
      "Epoch 191: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 132ms/step - loss: 0.2261 - accuracy: 0.9114 - val_loss: 0.3968 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9274\n",
      "Epoch 192: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.1929 - accuracy: 0.9274 - val_loss: 1.1492 - val_accuracy: 0.6591 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2240 - accuracy: 0.9127\n",
      "Epoch 193: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2240 - accuracy: 0.9127 - val_loss: 0.3639 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2210 - accuracy: 0.9213\n",
      "Epoch 194: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 128ms/step - loss: 0.2210 - accuracy: 0.9213 - val_loss: 0.6028 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2184 - accuracy: 0.9176\n",
      "Epoch 195: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.2184 - accuracy: 0.9176 - val_loss: 0.2169 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2317 - accuracy: 0.9053\n",
      "Epoch 196: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 128ms/step - loss: 0.2317 - accuracy: 0.9053 - val_loss: 0.2168 - val_accuracy: 0.8864 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2212 - accuracy: 0.9102\n",
      "Epoch 197: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.2212 - accuracy: 0.9102 - val_loss: 0.4095 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2374 - accuracy: 0.9028\n",
      "Epoch 198: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2374 - accuracy: 0.9028 - val_loss: 0.7826 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2447 - accuracy: 0.8954\n",
      "Epoch 199: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.2447 - accuracy: 0.8954 - val_loss: 1.5463 - val_accuracy: 0.5864 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2249 - accuracy: 0.9065\n",
      "Epoch 200: val_loss did not improve from 0.16657\n",
      "13/13 [==============================] - 2s 127ms/step - loss: 0.2249 - accuracy: 0.9065 - val_loss: 0.3530 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.3530 - accuracy: 0.8455\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.2958 - accuracy: 0.8677\n",
      "Model Performance for VGG19\n",
      "Loss(Test): 0.35302039980888367, Accuracy(Test): 84.5%\n",
      "Loss(Train): 0.2957742512226105, Accuracy(Train): 86.8%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet50\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_37 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, 64, 64, 3)         84        \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, None, None, 2048)  23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d_12  (None, 2048)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 2048)             8192      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               524544    \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " root (Dense)                (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,122,327\n",
      "Trainable params: 24,064,599\n",
      "Non-trainable params: 57,728\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 2.0125 - accuracy: 0.3389\n",
      "Epoch 1: val_loss improved from inf to 0.96728, saving model to model.h5\n",
      "13/13 [==============================] - 15s 408ms/step - loss: 2.0125 - accuracy: 0.3389 - val_loss: 0.9673 - val_accuracy: 0.5136 - lr: 0.0020\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.8378 - accuracy: 0.3641\n",
      "Epoch 2: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 162ms/step - loss: 1.8378 - accuracy: 0.3641 - val_loss: 0.9846 - val_accuracy: 0.5136 - lr: 0.0020\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.8111 - accuracy: 0.3801\n",
      "Epoch 3: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 1.8111 - accuracy: 0.3801 - val_loss: 0.9906 - val_accuracy: 0.5136 - lr: 0.0020\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6719 - accuracy: 0.4010\n",
      "Epoch 4: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 1.6719 - accuracy: 0.4010 - val_loss: 0.9758 - val_accuracy: 0.5136 - lr: 0.0020\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.5436 - accuracy: 0.4490\n",
      "Epoch 5: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 1.5436 - accuracy: 0.4490 - val_loss: 1.0295 - val_accuracy: 0.5136 - lr: 0.0020\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.4642 - accuracy: 0.4932\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 1.4642 - accuracy: 0.4932 - val_loss: 1.2688 - val_accuracy: 0.5136 - lr: 0.0020\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.5295 - accuracy: 0.4662\n",
      "Epoch 7: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 1.5295 - accuracy: 0.4662 - val_loss: 1.4845 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.4181 - accuracy: 0.4994\n",
      "Epoch 8: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 1.4181 - accuracy: 0.4994 - val_loss: 1.5298 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2709 - accuracy: 0.5215\n",
      "Epoch 9: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 1.2709 - accuracy: 0.5215 - val_loss: 1.6588 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2817 - accuracy: 0.5006\n",
      "Epoch 10: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 1.2817 - accuracy: 0.5006 - val_loss: 1.9132 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2825 - accuracy: 0.5314\n",
      "Epoch 11: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 1.2825 - accuracy: 0.5314 - val_loss: 1.9787 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1854 - accuracy: 0.5437\n",
      "Epoch 12: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 1.1854 - accuracy: 0.5437 - val_loss: 2.2851 - val_accuracy: 0.4591 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1636 - accuracy: 0.6015\n",
      "Epoch 13: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 1.1636 - accuracy: 0.6015 - val_loss: 2.5243 - val_accuracy: 0.1136 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0963 - accuracy: 0.5707\n",
      "Epoch 14: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 1.0963 - accuracy: 0.5707 - val_loss: 2.7211 - val_accuracy: 0.1136 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1411 - accuracy: 0.5966\n",
      "Epoch 15: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 1.1411 - accuracy: 0.5966 - val_loss: 2.6476 - val_accuracy: 0.1136 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0743 - accuracy: 0.6261\n",
      "Epoch 16: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 1.0743 - accuracy: 0.6261 - val_loss: 2.5353 - val_accuracy: 0.1136 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0455 - accuracy: 0.6064\n",
      "Epoch 17: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 1.0455 - accuracy: 0.6064 - val_loss: 2.6142 - val_accuracy: 0.1136 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9712 - accuracy: 0.6175\n",
      "Epoch 18: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.9712 - accuracy: 0.6175 - val_loss: 2.8613 - val_accuracy: 0.1136 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9150 - accuracy: 0.6458\n",
      "Epoch 19: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.9150 - accuracy: 0.6458 - val_loss: 2.2155 - val_accuracy: 0.1136 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9168 - accuracy: 0.6458\n",
      "Epoch 20: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.9168 - accuracy: 0.6458 - val_loss: 1.6660 - val_accuracy: 0.2591 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9524 - accuracy: 0.6421\n",
      "Epoch 21: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.9524 - accuracy: 0.6421 - val_loss: 1.4496 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8667 - accuracy: 0.6617\n",
      "Epoch 22: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.8667 - accuracy: 0.6617 - val_loss: 1.4135 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7951 - accuracy: 0.7109\n",
      "Epoch 23: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.7951 - accuracy: 0.7109 - val_loss: 1.6966 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8945 - accuracy: 0.6556\n",
      "Epoch 24: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.8945 - accuracy: 0.6556 - val_loss: 1.7918 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8204 - accuracy: 0.6851\n",
      "Epoch 25: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 135ms/step - loss: 0.8204 - accuracy: 0.6851 - val_loss: 1.7960 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8206 - accuracy: 0.6851\n",
      "Epoch 26: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.8206 - accuracy: 0.6851 - val_loss: 1.8525 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7861 - accuracy: 0.7011\n",
      "Epoch 27: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.7861 - accuracy: 0.7011 - val_loss: 1.4702 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7713 - accuracy: 0.6814\n",
      "Epoch 28: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.7713 - accuracy: 0.6814 - val_loss: 1.3422 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7911 - accuracy: 0.6876\n",
      "Epoch 29: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 151ms/step - loss: 0.7911 - accuracy: 0.6876 - val_loss: 1.2046 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8601 - accuracy: 0.6851\n",
      "Epoch 30: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.8601 - accuracy: 0.6851 - val_loss: 1.2751 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7511 - accuracy: 0.7208\n",
      "Epoch 31: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.7511 - accuracy: 0.7208 - val_loss: 1.1211 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7419 - accuracy: 0.7196\n",
      "Epoch 32: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.7419 - accuracy: 0.7196 - val_loss: 1.1496 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7237 - accuracy: 0.7200\n",
      "Epoch 33: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.7237 - accuracy: 0.7200 - val_loss: 1.2712 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6657 - accuracy: 0.7380\n",
      "Epoch 34: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.6657 - accuracy: 0.7380 - val_loss: 1.1938 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7196 - accuracy: 0.7257\n",
      "Epoch 35: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.7196 - accuracy: 0.7257 - val_loss: 1.1277 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6630 - accuracy: 0.7503\n",
      "Epoch 36: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 150ms/step - loss: 0.6630 - accuracy: 0.7503 - val_loss: 1.0788 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6657 - accuracy: 0.7491\n",
      "Epoch 37: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.6657 - accuracy: 0.7491 - val_loss: 1.0987 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6815 - accuracy: 0.7245\n",
      "Epoch 38: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.6815 - accuracy: 0.7245 - val_loss: 1.1346 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6357 - accuracy: 0.7515\n",
      "Epoch 39: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.6357 - accuracy: 0.7515 - val_loss: 1.1702 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6334 - accuracy: 0.7528\n",
      "Epoch 40: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.6334 - accuracy: 0.7528 - val_loss: 1.1434 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6236 - accuracy: 0.7638\n",
      "Epoch 41: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.6236 - accuracy: 0.7638 - val_loss: 1.1283 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5979 - accuracy: 0.7528\n",
      "Epoch 42: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.5979 - accuracy: 0.7528 - val_loss: 1.0854 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5933 - accuracy: 0.7823\n",
      "Epoch 43: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.5933 - accuracy: 0.7823 - val_loss: 1.1198 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5360 - accuracy: 0.7800\n",
      "Epoch 44: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.5360 - accuracy: 0.7800 - val_loss: 1.1071 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5813 - accuracy: 0.7749\n",
      "Epoch 45: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 134ms/step - loss: 0.5813 - accuracy: 0.7749 - val_loss: 1.1136 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5777 - accuracy: 0.7872\n",
      "Epoch 46: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.5777 - accuracy: 0.7872 - val_loss: 1.0934 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5629 - accuracy: 0.7774\n",
      "Epoch 47: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.5629 - accuracy: 0.7774 - val_loss: 1.0985 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5791 - accuracy: 0.7909\n",
      "Epoch 48: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.5791 - accuracy: 0.7909 - val_loss: 1.0747 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5477 - accuracy: 0.7860\n",
      "Epoch 49: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.5477 - accuracy: 0.7860 - val_loss: 1.0852 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5870 - accuracy: 0.7675\n",
      "Epoch 50: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.5870 - accuracy: 0.7675 - val_loss: 1.0797 - val_accuracy: 0.5045 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5936 - accuracy: 0.7589\n",
      "Epoch 51: val_loss did not improve from 0.96728\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.5936 - accuracy: 0.7589 - val_loss: 1.0016 - val_accuracy: 0.4909 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5913 - accuracy: 0.7503\n",
      "Epoch 52: val_loss improved from 0.96728 to 0.96114, saving model to model.h5\n",
      "13/13 [==============================] - 3s 272ms/step - loss: 0.5913 - accuracy: 0.7503 - val_loss: 0.9611 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5027 - accuracy: 0.8020\n",
      "Epoch 53: val_loss improved from 0.96114 to 0.92818, saving model to model.h5\n",
      "13/13 [==============================] - 3s 269ms/step - loss: 0.5027 - accuracy: 0.8020 - val_loss: 0.9282 - val_accuracy: 0.5273 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5530 - accuracy: 0.7786\n",
      "Epoch 54: val_loss improved from 0.92818 to 0.90137, saving model to model.h5\n",
      "13/13 [==============================] - 3s 265ms/step - loss: 0.5530 - accuracy: 0.7786 - val_loss: 0.9014 - val_accuracy: 0.5864 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5394 - accuracy: 0.7921\n",
      "Epoch 55: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.5394 - accuracy: 0.7921 - val_loss: 0.9154 - val_accuracy: 0.5909 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.7995\n",
      "Epoch 56: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.5125 - accuracy: 0.7995 - val_loss: 0.9516 - val_accuracy: 0.5318 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5234 - accuracy: 0.8007\n",
      "Epoch 57: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.5234 - accuracy: 0.8007 - val_loss: 1.0240 - val_accuracy: 0.4955 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5164 - accuracy: 0.8020\n",
      "Epoch 58: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.5164 - accuracy: 0.8020 - val_loss: 1.0907 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4992 - accuracy: 0.7981\n",
      "Epoch 59: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4992 - accuracy: 0.7981 - val_loss: 1.2260 - val_accuracy: 0.4773 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4880 - accuracy: 0.8167\n",
      "Epoch 60: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4880 - accuracy: 0.8167 - val_loss: 1.2644 - val_accuracy: 0.4955 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.8081\n",
      "Epoch 61: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4976 - accuracy: 0.8081 - val_loss: 1.6795 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.8044\n",
      "Epoch 62: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.5059 - accuracy: 0.8044 - val_loss: 2.0380 - val_accuracy: 0.4409 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5287 - accuracy: 0.7946\n",
      "Epoch 63: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.5287 - accuracy: 0.7946 - val_loss: 2.3552 - val_accuracy: 0.4273 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4930 - accuracy: 0.8081\n",
      "Epoch 64: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.4930 - accuracy: 0.8081 - val_loss: 2.9567 - val_accuracy: 0.4318 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5044 - accuracy: 0.8020\n",
      "Epoch 65: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.5044 - accuracy: 0.8020 - val_loss: 3.3589 - val_accuracy: 0.4136 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4738 - accuracy: 0.8057\n",
      "Epoch 66: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.4738 - accuracy: 0.8057 - val_loss: 4.3423 - val_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5252 - accuracy: 0.7774\n",
      "Epoch 67: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.5252 - accuracy: 0.7774 - val_loss: 3.9439 - val_accuracy: 0.4182 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5168 - accuracy: 0.7970\n",
      "Epoch 68: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.5168 - accuracy: 0.7970 - val_loss: 4.0470 - val_accuracy: 0.4364 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4700 - accuracy: 0.8143\n",
      "Epoch 69: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.4700 - accuracy: 0.8143 - val_loss: 3.3156 - val_accuracy: 0.4455 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4844 - accuracy: 0.8143\n",
      "Epoch 70: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.4844 - accuracy: 0.8143 - val_loss: 3.1497 - val_accuracy: 0.4364 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4927 - accuracy: 0.8020\n",
      "Epoch 71: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.4927 - accuracy: 0.8020 - val_loss: 4.0235 - val_accuracy: 0.4273 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4860 - accuracy: 0.7983\n",
      "Epoch 72: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.4860 - accuracy: 0.7983 - val_loss: 3.5457 - val_accuracy: 0.4455 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5428 - accuracy: 0.7847\n",
      "Epoch 73: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.5428 - accuracy: 0.7847 - val_loss: 2.0906 - val_accuracy: 0.5227 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4884 - accuracy: 0.7995\n",
      "Epoch 74: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4884 - accuracy: 0.7995 - val_loss: 2.5522 - val_accuracy: 0.4727 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4267 - accuracy: 0.8253\n",
      "Epoch 75: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.4267 - accuracy: 0.8253 - val_loss: 2.5615 - val_accuracy: 0.4682 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4745 - accuracy: 0.8180\n",
      "Epoch 76: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.4745 - accuracy: 0.8180 - val_loss: 2.2823 - val_accuracy: 0.4773 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4291 - accuracy: 0.8229\n",
      "Epoch 77: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4291 - accuracy: 0.8229 - val_loss: 2.1513 - val_accuracy: 0.4864 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4321 - accuracy: 0.8149\n",
      "Epoch 78: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.4321 - accuracy: 0.8149 - val_loss: 1.7203 - val_accuracy: 0.5364 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4308 - accuracy: 0.8438\n",
      "Epoch 79: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4308 - accuracy: 0.8438 - val_loss: 1.7860 - val_accuracy: 0.5545 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4404 - accuracy: 0.8266\n",
      "Epoch 80: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4404 - accuracy: 0.8266 - val_loss: 1.7837 - val_accuracy: 0.5818 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4273 - accuracy: 0.8229\n",
      "Epoch 81: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.4273 - accuracy: 0.8229 - val_loss: 1.4506 - val_accuracy: 0.6636 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4379 - accuracy: 0.8155\n",
      "Epoch 82: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4379 - accuracy: 0.8155 - val_loss: 1.3122 - val_accuracy: 0.6864 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4064 - accuracy: 0.8462\n",
      "Epoch 83: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4064 - accuracy: 0.8462 - val_loss: 1.4493 - val_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4050 - accuracy: 0.8339\n",
      "Epoch 84: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.4050 - accuracy: 0.8339 - val_loss: 1.2906 - val_accuracy: 0.6636 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4385 - accuracy: 0.8293\n",
      "Epoch 85: val_loss did not improve from 0.90137\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4385 - accuracy: 0.8293 - val_loss: 0.9853 - val_accuracy: 0.7182 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4083 - accuracy: 0.8303\n",
      "Epoch 86: val_loss improved from 0.90137 to 0.82719, saving model to model.h5\n",
      "13/13 [==============================] - 3s 270ms/step - loss: 0.4083 - accuracy: 0.8303 - val_loss: 0.8272 - val_accuracy: 0.7409 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4386 - accuracy: 0.8352\n",
      "Epoch 87: val_loss improved from 0.82719 to 0.74638, saving model to model.h5\n",
      "13/13 [==============================] - 3s 277ms/step - loss: 0.4386 - accuracy: 0.8352 - val_loss: 0.7464 - val_accuracy: 0.7591 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4302 - accuracy: 0.8204\n",
      "Epoch 88: val_loss did not improve from 0.74638\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4302 - accuracy: 0.8204 - val_loss: 0.7494 - val_accuracy: 0.7364 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4158 - accuracy: 0.8266\n",
      "Epoch 89: val_loss improved from 0.74638 to 0.64487, saving model to model.h5\n",
      "13/13 [==============================] - 3s 272ms/step - loss: 0.4158 - accuracy: 0.8266 - val_loss: 0.6449 - val_accuracy: 0.7545 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4299 - accuracy: 0.8401\n",
      "Epoch 90: val_loss did not improve from 0.64487\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.4299 - accuracy: 0.8401 - val_loss: 0.6797 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.8475\n",
      "Epoch 91: val_loss did not improve from 0.64487\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.3769 - accuracy: 0.8475 - val_loss: 0.8405 - val_accuracy: 0.6909 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4563 - accuracy: 0.8229\n",
      "Epoch 92: val_loss did not improve from 0.64487\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.4563 - accuracy: 0.8229 - val_loss: 0.7943 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4077 - accuracy: 0.8462\n",
      "Epoch 93: val_loss improved from 0.64487 to 0.55785, saving model to model.h5\n",
      "13/13 [==============================] - 3s 267ms/step - loss: 0.4077 - accuracy: 0.8462 - val_loss: 0.5578 - val_accuracy: 0.7818 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4094 - accuracy: 0.8290\n",
      "Epoch 94: val_loss improved from 0.55785 to 0.52637, saving model to model.h5\n",
      "13/13 [==============================] - 3s 270ms/step - loss: 0.4094 - accuracy: 0.8290 - val_loss: 0.5264 - val_accuracy: 0.7818 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3839 - accuracy: 0.8475\n",
      "Epoch 95: val_loss did not improve from 0.52637\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.3839 - accuracy: 0.8475 - val_loss: 0.5777 - val_accuracy: 0.7591 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4000 - accuracy: 0.8389\n",
      "Epoch 96: val_loss did not improve from 0.52637\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.4000 - accuracy: 0.8389 - val_loss: 0.5654 - val_accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3693 - accuracy: 0.8475\n",
      "Epoch 97: val_loss did not improve from 0.52637\n",
      "13/13 [==============================] - 2s 155ms/step - loss: 0.3693 - accuracy: 0.8475 - val_loss: 0.6300 - val_accuracy: 0.7409 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3943 - accuracy: 0.8549\n",
      "Epoch 98: val_loss improved from 0.52637 to 0.43287, saving model to model.h5\n",
      "13/13 [==============================] - 3s 268ms/step - loss: 0.3943 - accuracy: 0.8549 - val_loss: 0.4329 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4053 - accuracy: 0.8290\n",
      "Epoch 99: val_loss improved from 0.43287 to 0.41055, saving model to model.h5\n",
      "13/13 [==============================] - 3s 269ms/step - loss: 0.4053 - accuracy: 0.8290 - val_loss: 0.4105 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3773 - accuracy: 0.8352\n",
      "Epoch 100: val_loss improved from 0.41055 to 0.36760, saving model to model.h5\n",
      "13/13 [==============================] - 3s 270ms/step - loss: 0.3773 - accuracy: 0.8352 - val_loss: 0.3676 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3568 - accuracy: 0.8536\n",
      "Epoch 101: val_loss improved from 0.36760 to 0.34851, saving model to model.h5\n",
      "13/13 [==============================] - 3s 268ms/step - loss: 0.3568 - accuracy: 0.8536 - val_loss: 0.3485 - val_accuracy: 0.8409 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3512 - accuracy: 0.8475\n",
      "Epoch 102: val_loss improved from 0.34851 to 0.33833, saving model to model.h5\n",
      "13/13 [==============================] - 3s 271ms/step - loss: 0.3512 - accuracy: 0.8475 - val_loss: 0.3383 - val_accuracy: 0.8409 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3752 - accuracy: 0.8487\n",
      "Epoch 103: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.3752 - accuracy: 0.8487 - val_loss: 0.3683 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3889 - accuracy: 0.8426\n",
      "Epoch 104: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.3889 - accuracy: 0.8426 - val_loss: 0.3672 - val_accuracy: 0.8409 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3822 - accuracy: 0.8512\n",
      "Epoch 105: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.3822 - accuracy: 0.8512 - val_loss: 0.3685 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3742 - accuracy: 0.8413\n",
      "Epoch 106: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.3742 - accuracy: 0.8413 - val_loss: 0.3784 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3560 - accuracy: 0.8512\n",
      "Epoch 107: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.3560 - accuracy: 0.8512 - val_loss: 0.3760 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3821 - accuracy: 0.8475\n",
      "Epoch 108: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.3821 - accuracy: 0.8475 - val_loss: 0.4040 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.8622\n",
      "Epoch 109: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.3375 - accuracy: 0.8622 - val_loss: 0.3620 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3669 - accuracy: 0.8499\n",
      "Epoch 110: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.3669 - accuracy: 0.8499 - val_loss: 0.3774 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3723 - accuracy: 0.8536\n",
      "Epoch 111: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.3723 - accuracy: 0.8536 - val_loss: 0.3441 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3786 - accuracy: 0.8475\n",
      "Epoch 112: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.3786 - accuracy: 0.8475 - val_loss: 0.3988 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3830 - accuracy: 0.8487\n",
      "Epoch 113: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.3830 - accuracy: 0.8487 - val_loss: 0.5475 - val_accuracy: 0.7773 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3695 - accuracy: 0.8549\n",
      "Epoch 114: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.3695 - accuracy: 0.8549 - val_loss: 0.4767 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3584 - accuracy: 0.8549\n",
      "Epoch 115: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.3584 - accuracy: 0.8549 - val_loss: 0.4488 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3516 - accuracy: 0.8413\n",
      "Epoch 116: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.3516 - accuracy: 0.8413 - val_loss: 0.5028 - val_accuracy: 0.7818 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3484 - accuracy: 0.8462\n",
      "Epoch 117: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.3484 - accuracy: 0.8462 - val_loss: 0.4060 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3316 - accuracy: 0.8635\n",
      "Epoch 118: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.3316 - accuracy: 0.8635 - val_loss: 0.3821 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3459 - accuracy: 0.8598\n",
      "Epoch 119: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 0.3459 - accuracy: 0.8598 - val_loss: 0.3704 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3533 - accuracy: 0.8573\n",
      "Epoch 120: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.3533 - accuracy: 0.8573 - val_loss: 0.4021 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3539 - accuracy: 0.8610\n",
      "Epoch 121: val_loss did not improve from 0.33833\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.3539 - accuracy: 0.8610 - val_loss: 0.3491 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.8721\n",
      "Epoch 122: val_loss improved from 0.33833 to 0.33013, saving model to model.h5\n",
      "13/13 [==============================] - 3s 273ms/step - loss: 0.3052 - accuracy: 0.8721 - val_loss: 0.3301 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3459 - accuracy: 0.8721\n",
      "Epoch 123: val_loss improved from 0.33013 to 0.32508, saving model to model.h5\n",
      "13/13 [==============================] - 3s 275ms/step - loss: 0.3459 - accuracy: 0.8721 - val_loss: 0.3251 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3351 - accuracy: 0.8647\n",
      "Epoch 124: val_loss did not improve from 0.32508\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.3351 - accuracy: 0.8647 - val_loss: 0.3394 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3319 - accuracy: 0.8721\n",
      "Epoch 125: val_loss did not improve from 0.32508\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.3319 - accuracy: 0.8721 - val_loss: 0.3276 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3498 - accuracy: 0.8770\n",
      "Epoch 126: val_loss did not improve from 0.32508\n",
      "13/13 [==============================] - 2s 146ms/step - loss: 0.3498 - accuracy: 0.8770 - val_loss: 0.4966 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3423 - accuracy: 0.8721\n",
      "Epoch 127: val_loss did not improve from 0.32508\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.3423 - accuracy: 0.8721 - val_loss: 0.6363 - val_accuracy: 0.7545 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3300 - accuracy: 0.8610\n",
      "Epoch 128: val_loss did not improve from 0.32508\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.3300 - accuracy: 0.8610 - val_loss: 0.6996 - val_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3256 - accuracy: 0.8798\n",
      "Epoch 129: val_loss did not improve from 0.32508\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.3256 - accuracy: 0.8798 - val_loss: 0.7723 - val_accuracy: 0.6682 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3326 - accuracy: 0.8622\n",
      "Epoch 130: val_loss did not improve from 0.32508\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.3326 - accuracy: 0.8622 - val_loss: 0.5593 - val_accuracy: 0.7727 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3244 - accuracy: 0.8696\n",
      "Epoch 131: val_loss did not improve from 0.32508\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.3244 - accuracy: 0.8696 - val_loss: 0.3699 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3148 - accuracy: 0.8721\n",
      "Epoch 132: val_loss did not improve from 0.32508\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.3148 - accuracy: 0.8721 - val_loss: 0.4182 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3485 - accuracy: 0.8622\n",
      "Epoch 133: val_loss did not improve from 0.32508\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.3485 - accuracy: 0.8622 - val_loss: 0.4516 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3350 - accuracy: 0.8635\n",
      "Epoch 134: val_loss did not improve from 0.32508\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.3350 - accuracy: 0.8635 - val_loss: 0.4298 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.8659\n",
      "Epoch 135: val_loss improved from 0.32508 to 0.31552, saving model to model.h5\n",
      "13/13 [==============================] - 3s 274ms/step - loss: 0.3270 - accuracy: 0.8659 - val_loss: 0.3155 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3381 - accuracy: 0.8561\n",
      "Epoch 136: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.3381 - accuracy: 0.8561 - val_loss: 0.3455 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 0.8696\n",
      "Epoch 137: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.3078 - accuracy: 0.8696 - val_loss: 0.4977 - val_accuracy: 0.7773 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.8647\n",
      "Epoch 138: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.3024 - accuracy: 0.8647 - val_loss: 0.5607 - val_accuracy: 0.7591 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3148 - accuracy: 0.8795\n",
      "Epoch 139: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.3148 - accuracy: 0.8795 - val_loss: 0.5381 - val_accuracy: 0.7364 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3148 - accuracy: 0.8844\n",
      "Epoch 140: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.3148 - accuracy: 0.8844 - val_loss: 0.3732 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3087 - accuracy: 0.8758\n",
      "Epoch 141: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 0.3087 - accuracy: 0.8758 - val_loss: 0.5201 - val_accuracy: 0.7636 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3115 - accuracy: 0.8659\n",
      "Epoch 142: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.3115 - accuracy: 0.8659 - val_loss: 0.5672 - val_accuracy: 0.7364 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3251 - accuracy: 0.8745\n",
      "Epoch 143: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.3251 - accuracy: 0.8745 - val_loss: 0.5598 - val_accuracy: 0.7182 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3253 - accuracy: 0.8635\n",
      "Epoch 144: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.3253 - accuracy: 0.8635 - val_loss: 0.4077 - val_accuracy: 0.8409 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3042 - accuracy: 0.8684\n",
      "Epoch 145: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.3042 - accuracy: 0.8684 - val_loss: 0.5071 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.8856\n",
      "Epoch 146: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 149ms/step - loss: 0.2887 - accuracy: 0.8856 - val_loss: 0.3747 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2959 - accuracy: 0.8770\n",
      "Epoch 147: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.2959 - accuracy: 0.8770 - val_loss: 0.6472 - val_accuracy: 0.7318 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.8745\n",
      "Epoch 148: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.2951 - accuracy: 0.8745 - val_loss: 0.5001 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.8831\n",
      "Epoch 149: val_loss did not improve from 0.31552\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.2852 - accuracy: 0.8831 - val_loss: 0.3437 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2940 - accuracy: 0.8782\n",
      "Epoch 150: val_loss improved from 0.31552 to 0.29094, saving model to model.h5\n",
      "13/13 [==============================] - 3s 273ms/step - loss: 0.2940 - accuracy: 0.8782 - val_loss: 0.2909 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.8770\n",
      "Epoch 151: val_loss improved from 0.29094 to 0.27625, saving model to model.h5\n",
      "13/13 [==============================] - 4s 296ms/step - loss: 0.2891 - accuracy: 0.8770 - val_loss: 0.2762 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3037 - accuracy: 0.8819\n",
      "Epoch 152: val_loss improved from 0.27625 to 0.26493, saving model to model.h5\n",
      "13/13 [==============================] - 3s 274ms/step - loss: 0.3037 - accuracy: 0.8819 - val_loss: 0.2649 - val_accuracy: 0.8864 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3200 - accuracy: 0.8822\n",
      "Epoch 153: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.3200 - accuracy: 0.8822 - val_loss: 0.2714 - val_accuracy: 0.8773 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2916 - accuracy: 0.8819\n",
      "Epoch 154: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.2916 - accuracy: 0.8819 - val_loss: 0.3337 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.8831\n",
      "Epoch 155: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.2567 - accuracy: 0.8831 - val_loss: 0.3805 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.8758\n",
      "Epoch 156: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 146ms/step - loss: 0.3195 - accuracy: 0.8758 - val_loss: 0.3302 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.8721\n",
      "Epoch 157: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.3098 - accuracy: 0.8721 - val_loss: 0.2924 - val_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.8844\n",
      "Epoch 158: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.2850 - accuracy: 0.8844 - val_loss: 0.4018 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2718 - accuracy: 0.8942\n",
      "Epoch 159: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.2718 - accuracy: 0.8942 - val_loss: 0.6140 - val_accuracy: 0.6909 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2958 - accuracy: 0.8774\n",
      "Epoch 160: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.2958 - accuracy: 0.8774 - val_loss: 0.4900 - val_accuracy: 0.7545 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2980 - accuracy: 0.8819\n",
      "Epoch 161: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.2980 - accuracy: 0.8819 - val_loss: 0.7502 - val_accuracy: 0.6864 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2946 - accuracy: 0.8807\n",
      "Epoch 162: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.2946 - accuracy: 0.8807 - val_loss: 0.3518 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.8930\n",
      "Epoch 163: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.2703 - accuracy: 0.8930 - val_loss: 0.5219 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2907 - accuracy: 0.8782\n",
      "Epoch 164: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.2907 - accuracy: 0.8782 - val_loss: 0.6850 - val_accuracy: 0.7318 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.8684\n",
      "Epoch 165: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.3066 - accuracy: 0.8684 - val_loss: 0.5602 - val_accuracy: 0.7773 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.8758\n",
      "Epoch 166: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 146ms/step - loss: 0.2925 - accuracy: 0.8758 - val_loss: 0.4319 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.8893\n",
      "Epoch 167: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.2769 - accuracy: 0.8893 - val_loss: 0.5117 - val_accuracy: 0.7955 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.8905\n",
      "Epoch 168: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.2843 - accuracy: 0.8905 - val_loss: 0.4120 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2598 - accuracy: 0.8954\n",
      "Epoch 169: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.2598 - accuracy: 0.8954 - val_loss: 0.6121 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2650 - accuracy: 0.8844\n",
      "Epoch 170: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.2650 - accuracy: 0.8844 - val_loss: 0.4343 - val_accuracy: 0.8318 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.9004\n",
      "Epoch 171: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.2443 - accuracy: 0.9004 - val_loss: 0.4463 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.8954\n",
      "Epoch 172: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.2475 - accuracy: 0.8954 - val_loss: 0.8326 - val_accuracy: 0.6045 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.9077\n",
      "Epoch 173: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.2608 - accuracy: 0.9077 - val_loss: 0.4362 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.8918\n",
      "Epoch 174: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.2540 - accuracy: 0.8918 - val_loss: 0.4389 - val_accuracy: 0.7955 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3070 - accuracy: 0.8831\n",
      "Epoch 175: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.3070 - accuracy: 0.8831 - val_loss: 0.5475 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.8893\n",
      "Epoch 176: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.2736 - accuracy: 0.8893 - val_loss: 1.3183 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2617 - accuracy: 0.8844\n",
      "Epoch 177: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.2617 - accuracy: 0.8844 - val_loss: 1.6062 - val_accuracy: 0.5091 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.8831\n",
      "Epoch 178: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.2864 - accuracy: 0.8831 - val_loss: 0.8701 - val_accuracy: 0.6682 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.8819\n",
      "Epoch 179: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.2849 - accuracy: 0.8819 - val_loss: 0.3546 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2851 - accuracy: 0.8807\n",
      "Epoch 180: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.2851 - accuracy: 0.8807 - val_loss: 0.3926 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2364 - accuracy: 0.9004\n",
      "Epoch 181: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.2364 - accuracy: 0.9004 - val_loss: 0.3296 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.8967\n",
      "Epoch 182: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.2629 - accuracy: 0.8967 - val_loss: 0.3264 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2318 - accuracy: 0.9053\n",
      "Epoch 183: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.2318 - accuracy: 0.9053 - val_loss: 0.3553 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.8918\n",
      "Epoch 184: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.2625 - accuracy: 0.8918 - val_loss: 0.3711 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 0.9016\n",
      "Epoch 185: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 0.2375 - accuracy: 0.9016 - val_loss: 0.5504 - val_accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2404 - accuracy: 0.8991\n",
      "Epoch 186: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.2404 - accuracy: 0.8991 - val_loss: 0.4717 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.8893\n",
      "Epoch 187: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.2696 - accuracy: 0.8893 - val_loss: 0.4762 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.9050\n",
      "Epoch 188: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.2477 - accuracy: 0.9050 - val_loss: 0.4741 - val_accuracy: 0.7909 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2229 - accuracy: 0.8991\n",
      "Epoch 189: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.2229 - accuracy: 0.8991 - val_loss: 0.3696 - val_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.9004\n",
      "Epoch 190: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.2628 - accuracy: 0.9004 - val_loss: 0.5000 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.8881\n",
      "Epoch 191: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.2677 - accuracy: 0.8881 - val_loss: 0.5360 - val_accuracy: 0.8045 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.8905\n",
      "Epoch 192: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 146ms/step - loss: 0.2522 - accuracy: 0.8905 - val_loss: 0.5945 - val_accuracy: 0.7364 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.8844\n",
      "Epoch 193: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.2642 - accuracy: 0.8844 - val_loss: 1.0807 - val_accuracy: 0.6182 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2247 - accuracy: 0.9139\n",
      "Epoch 194: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.2247 - accuracy: 0.9139 - val_loss: 1.8502 - val_accuracy: 0.5091 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.9053\n",
      "Epoch 195: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.2488 - accuracy: 0.9053 - val_loss: 0.7836 - val_accuracy: 0.6636 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2330 - accuracy: 0.9014\n",
      "Epoch 196: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 148ms/step - loss: 0.2330 - accuracy: 0.9014 - val_loss: 0.6084 - val_accuracy: 0.7364 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 0.9127\n",
      "Epoch 197: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.2123 - accuracy: 0.9127 - val_loss: 0.3528 - val_accuracy: 0.7955 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2292 - accuracy: 0.9200\n",
      "Epoch 198: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.2292 - accuracy: 0.9200 - val_loss: 0.5231 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.9041\n",
      "Epoch 199: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.2486 - accuracy: 0.9041 - val_loss: 1.0167 - val_accuracy: 0.6182 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.9114\n",
      "Epoch 200: val_loss did not improve from 0.26493\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.2409 - accuracy: 0.9114 - val_loss: 0.3822 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3822 - accuracy: 0.8136\n",
      "28/28 [==============================] - 1s 30ms/step - loss: 0.3806 - accuracy: 0.8141\n",
      "Model Performance for ResNet50\n",
      "Loss(Test): 0.38221925497055054, Accuracy(Test): 81.39999999999999%\n",
      "Loss(Train): 0.38058748841285706, Accuracy(Train): 81.39999999999999%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "allmodels=[]\n",
    "for i in range(len(modeldeep)):\n",
    "  currmodel=model_create(modeldeep[i],deepnames[i])\n",
    "  currmodel,hst=deep_train(currmodel,64,1,200,deepnames[i])\n",
    "  allmodels.append(currmodel)\n",
    "  currmodel.save(\"{}.h5\".format(deepnames[i]))\n",
    "  deep_predict(currmodel,deepnames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5y0-YqHInnIw",
    "outputId": "ad1e17ca-a1f0-4e49-868d-5015e4785a4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"2823dbaf-362f-42d2-b0a4-3e87fea2f0cd\" class=\"plotly-graph-div\" style=\"height:400px; width:700px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2823dbaf-362f-42d2-b0a4-3e87fea2f0cd\")) {                    Plotly.newPlot(                        \"2823dbaf-362f-42d2-b0a4-3e87fea2f0cd\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Model Accuracy=%{text}<extra></extra>\",\"legendgroup\":\"DenseNet201\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"DenseNet201\",\"offsetgroup\":\"DenseNet201\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[93.96],\"textposition\":\"auto\",\"x\":[\"DenseNet201\"],\"xaxis\":\"x\",\"y\":[93.96],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Model Accuracy=%{text}<extra></extra>\",\"legendgroup\":\"VGG19\",\"marker\":{\"color\":\"#EF553B\",\"pattern\":{\"shape\":\"\"}},\"name\":\"VGG19\",\"offsetgroup\":\"VGG19\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[86.77],\"textposition\":\"auto\",\"x\":[\"VGG19\"],\"xaxis\":\"x\",\"y\":[86.77],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Model Accuracy=%{text}<extra></extra>\",\"legendgroup\":\"Xception\",\"marker\":{\"color\":\"#00cc96\",\"pattern\":{\"shape\":\"\"}},\"name\":\"Xception\",\"offsetgroup\":\"Xception\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[82.78],\"textposition\":\"auto\",\"x\":[\"Xception\"],\"xaxis\":\"x\",\"y\":[82.78],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Model Accuracy=%{text}<extra></extra>\",\"legendgroup\":\"ResNet50\",\"marker\":{\"color\":\"#ab63fa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"ResNet50\",\"offsetgroup\":\"ResNet50\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[81.41000000000001],\"textposition\":\"auto\",\"x\":[\"ResNet50\"],\"xaxis\":\"x\",\"y\":[81.41000000000001],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Model\"},\"categoryorder\":\"array\",\"categoryarray\":[\"DenseNet201\",\"VGG19\",\"Xception\",\"ResNet50\"]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Model Accuracy\"}},\"legend\":{\"title\":{\"text\":\"Model\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Comparison of Model Accuracy\"},\"barmode\":\"relative\",\"height\":400,\"width\":700,\"font\":{\"family\":\"Times New Roman, Bold\",\"size\":20,\"color\":\"black\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('2823dbaf-362f-42d2-b0a4-3e87fea2f0cd');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"a4c998fc-527c-4c90-b5ad-1e767441e273\" class=\"plotly-graph-div\" style=\"height:400px; width:700px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a4c998fc-527c-4c90-b5ad-1e767441e273\")) {                    Plotly.newPlot(                        \"a4c998fc-527c-4c90-b5ad-1e767441e273\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Model Loss=%{text}<extra></extra>\",\"legendgroup\":\"Xception\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"Xception\",\"offsetgroup\":\"Xception\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[0.4426],\"textposition\":\"auto\",\"x\":[\"Xception\"],\"xaxis\":\"x\",\"y\":[0.4426],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Model Loss=%{text}<extra></extra>\",\"legendgroup\":\"ResNet50\",\"marker\":{\"color\":\"#EF553B\",\"pattern\":{\"shape\":\"\"}},\"name\":\"ResNet50\",\"offsetgroup\":\"ResNet50\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[0.3806],\"textposition\":\"auto\",\"x\":[\"ResNet50\"],\"xaxis\":\"x\",\"y\":[0.3806],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Model Loss=%{text}<extra></extra>\",\"legendgroup\":\"VGG19\",\"marker\":{\"color\":\"#00cc96\",\"pattern\":{\"shape\":\"\"}},\"name\":\"VGG19\",\"offsetgroup\":\"VGG19\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[0.2958],\"textposition\":\"auto\",\"x\":[\"VGG19\"],\"xaxis\":\"x\",\"y\":[0.2958],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Model Loss=%{text}<extra></extra>\",\"legendgroup\":\"DenseNet201\",\"marker\":{\"color\":\"#ab63fa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"DenseNet201\",\"offsetgroup\":\"DenseNet201\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[0.1601],\"textposition\":\"auto\",\"x\":[\"DenseNet201\"],\"xaxis\":\"x\",\"y\":[0.1601],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Model\"},\"categoryorder\":\"array\",\"categoryarray\":[\"Xception\",\"ResNet50\",\"VGG19\",\"DenseNet201\"]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Model Loss\"}},\"legend\":{\"title\":{\"text\":\"Model\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Comparison of Model Loss\"},\"barmode\":\"relative\",\"height\":400,\"width\":700,\"font\":{\"family\":\"Times New Roman, Bold\",\"size\":20,\"color\":\"black\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a4c998fc-527c-4c90-b5ad-1e767441e273');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"c7fd2127-ba6e-4239-bfd2-db75011bd91c\" class=\"plotly-graph-div\" style=\"height:400px; width:700px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c7fd2127-ba6e-4239-bfd2-db75011bd91c\")) {                    Plotly.newPlot(                        \"c7fd2127-ba6e-4239-bfd2-db75011bd91c\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Accuracies=%{text}<extra></extra>\",\"legendgroup\":\"DenseNet201\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"DenseNet201\",\"offsetgroup\":\"DenseNet201\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[92.0],\"textposition\":\"auto\",\"x\":[\"DenseNet201\"],\"xaxis\":\"x\",\"y\":[92.0],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Accuracies=%{text}<extra></extra>\",\"legendgroup\":\"VGG19\",\"marker\":{\"color\":\"#EF553B\",\"pattern\":{\"shape\":\"\"}},\"name\":\"VGG19\",\"offsetgroup\":\"VGG19\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[85.0],\"textposition\":\"auto\",\"x\":[\"VGG19\"],\"xaxis\":\"x\",\"y\":[85.0],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Accuracies=%{text}<extra></extra>\",\"legendgroup\":\"Xception\",\"marker\":{\"color\":\"#00cc96\",\"pattern\":{\"shape\":\"\"}},\"name\":\"Xception\",\"offsetgroup\":\"Xception\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[84.0],\"textposition\":\"auto\",\"x\":[\"Xception\"],\"xaxis\":\"x\",\"y\":[84.0],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Accuracies=%{text}<extra></extra>\",\"legendgroup\":\"ResNet50\",\"marker\":{\"color\":\"#ab63fa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"ResNet50\",\"offsetgroup\":\"ResNet50\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[81.0],\"textposition\":\"auto\",\"x\":[\"ResNet50\"],\"xaxis\":\"x\",\"y\":[81.0],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Model\"},\"categoryorder\":\"array\",\"categoryarray\":[\"DenseNet201\",\"VGG19\",\"Xception\",\"ResNet50\"]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Accuracies\"}},\"legend\":{\"title\":{\"text\":\"Model\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Comparison of Accuracies\"},\"barmode\":\"relative\",\"height\":400,\"width\":700,\"font\":{\"family\":\"Times New Roman, Bold\",\"size\":20,\"color\":\"black\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('c7fd2127-ba6e-4239-bfd2-db75011bd91c');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"db90f5e6-1083-4348-9bac-ea79962d6491\" class=\"plotly-graph-div\" style=\"height:400px; width:700px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"db90f5e6-1083-4348-9bac-ea79962d6491\")) {                    Plotly.newPlot(                        \"db90f5e6-1083-4348-9bac-ea79962d6491\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Precision=%{text}<extra></extra>\",\"legendgroup\":\"DenseNet201\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"DenseNet201\",\"offsetgroup\":\"DenseNet201\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[93.0],\"textposition\":\"auto\",\"x\":[\"DenseNet201\"],\"xaxis\":\"x\",\"y\":[93.0],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Precision=%{text}<extra></extra>\",\"legendgroup\":\"VGG19\",\"marker\":{\"color\":\"#EF553B\",\"pattern\":{\"shape\":\"\"}},\"name\":\"VGG19\",\"offsetgroup\":\"VGG19\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[89.0],\"textposition\":\"auto\",\"x\":[\"VGG19\"],\"xaxis\":\"x\",\"y\":[89.0],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Precision=%{text}<extra></extra>\",\"legendgroup\":\"ResNet50\",\"marker\":{\"color\":\"#00cc96\",\"pattern\":{\"shape\":\"\"}},\"name\":\"ResNet50\",\"offsetgroup\":\"ResNet50\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[87.0],\"textposition\":\"auto\",\"x\":[\"ResNet50\"],\"xaxis\":\"x\",\"y\":[87.0],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Precision=%{text}<extra></extra>\",\"legendgroup\":\"Xception\",\"marker\":{\"color\":\"#ab63fa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"Xception\",\"offsetgroup\":\"Xception\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[77.0],\"textposition\":\"auto\",\"x\":[\"Xception\"],\"xaxis\":\"x\",\"y\":[77.0],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Model\"},\"categoryorder\":\"array\",\"categoryarray\":[\"DenseNet201\",\"VGG19\",\"ResNet50\",\"Xception\"]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Precision\"}},\"legend\":{\"title\":{\"text\":\"Model\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Comparison of Precision\"},\"barmode\":\"relative\",\"height\":400,\"width\":700,\"font\":{\"family\":\"Times New Roman, Bold\",\"size\":20,\"color\":\"black\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('db90f5e6-1083-4348-9bac-ea79962d6491');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"9826430d-0624-477f-a4a0-b4c46f87772d\" class=\"plotly-graph-div\" style=\"height:400px; width:700px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9826430d-0624-477f-a4a0-b4c46f87772d\")) {                    Plotly.newPlot(                        \"9826430d-0624-477f-a4a0-b4c46f87772d\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Recall=%{text}<extra></extra>\",\"legendgroup\":\"DenseNet201\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"DenseNet201\",\"offsetgroup\":\"DenseNet201\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[92.0],\"textposition\":\"auto\",\"x\":[\"DenseNet201\"],\"xaxis\":\"x\",\"y\":[92.0],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Recall=%{text}<extra></extra>\",\"legendgroup\":\"VGG19\",\"marker\":{\"color\":\"#EF553B\",\"pattern\":{\"shape\":\"\"}},\"name\":\"VGG19\",\"offsetgroup\":\"VGG19\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[85.0],\"textposition\":\"auto\",\"x\":[\"VGG19\"],\"xaxis\":\"x\",\"y\":[85.0],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Recall=%{text}<extra></extra>\",\"legendgroup\":\"Xception\",\"marker\":{\"color\":\"#00cc96\",\"pattern\":{\"shape\":\"\"}},\"name\":\"Xception\",\"offsetgroup\":\"Xception\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[84.0],\"textposition\":\"auto\",\"x\":[\"Xception\"],\"xaxis\":\"x\",\"y\":[84.0],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>Recall=%{text}<extra></extra>\",\"legendgroup\":\"ResNet50\",\"marker\":{\"color\":\"#ab63fa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"ResNet50\",\"offsetgroup\":\"ResNet50\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[81.0],\"textposition\":\"auto\",\"x\":[\"ResNet50\"],\"xaxis\":\"x\",\"y\":[81.0],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Model\"},\"categoryorder\":\"array\",\"categoryarray\":[\"DenseNet201\",\"VGG19\",\"Xception\",\"ResNet50\"]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Recall\"}},\"legend\":{\"title\":{\"text\":\"Model\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Comparison of Recall\"},\"barmode\":\"relative\",\"height\":400,\"width\":700,\"font\":{\"family\":\"Times New Roman, Bold\",\"size\":20,\"color\":\"black\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('9826430d-0624-477f-a4a0-b4c46f87772d');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"a71d3a04-8d83-43e7-87a8-2122cc2cc504\" class=\"plotly-graph-div\" style=\"height:400px; width:700px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a71d3a04-8d83-43e7-87a8-2122cc2cc504\")) {                    Plotly.newPlot(                        \"a71d3a04-8d83-43e7-87a8-2122cc2cc504\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>F1-Score=%{text}<extra></extra>\",\"legendgroup\":\"DenseNet201\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"DenseNet201\",\"offsetgroup\":\"DenseNet201\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[92.0],\"textposition\":\"auto\",\"x\":[\"DenseNet201\"],\"xaxis\":\"x\",\"y\":[92.0],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>F1-Score=%{text}<extra></extra>\",\"legendgroup\":\"VGG19\",\"marker\":{\"color\":\"#EF553B\",\"pattern\":{\"shape\":\"\"}},\"name\":\"VGG19\",\"offsetgroup\":\"VGG19\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[86.0],\"textposition\":\"auto\",\"x\":[\"VGG19\"],\"xaxis\":\"x\",\"y\":[86.0],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>F1-Score=%{text}<extra></extra>\",\"legendgroup\":\"ResNet50\",\"marker\":{\"color\":\"#00cc96\",\"pattern\":{\"shape\":\"\"}},\"name\":\"ResNet50\",\"offsetgroup\":\"ResNet50\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[82.0],\"textposition\":\"auto\",\"x\":[\"ResNet50\"],\"xaxis\":\"x\",\"y\":[82.0],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Model=%{x}<br>F1-Score=%{text}<extra></extra>\",\"legendgroup\":\"Xception\",\"marker\":{\"color\":\"#ab63fa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"Xception\",\"offsetgroup\":\"Xception\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[79.0],\"textposition\":\"auto\",\"x\":[\"Xception\"],\"xaxis\":\"x\",\"y\":[79.0],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Model\"},\"categoryorder\":\"array\",\"categoryarray\":[\"DenseNet201\",\"VGG19\",\"ResNet50\",\"Xception\"]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"F1-Score\"}},\"legend\":{\"title\":{\"text\":\"Model\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Comparison of F1-Score\"},\"barmode\":\"relative\",\"height\":400,\"width\":700,\"font\":{\"family\":\"Times New Roman, Bold\",\"size\":20,\"color\":\"black\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a71d3a04-8d83-43e7-87a8-2122cc2cc504');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accdf=pandas.DataFrame({\"Model\":deepnames,\"Model Accuracy\":tracc,\"Model Loss\":trlss,\"Accuracies\":allacc,\"Precision\":allprec,\"Recall\":allrecall,\"F1-Score\":allf1})\n",
    "\n",
    "accdf=accdf.sort_values(by=\"Accuracies\",ascending=False)\n",
    "for i in accdf.columns.tolist()[1:]:\n",
    "    accout=accdf.sort_values(by=i,ascending=False)\n",
    "    fig = express.bar(accout, y=i, x=\"Model\",color=\"Model\",text=i,title=\"Comparison of {}\".format(i),height=400,width=700)\n",
    "    fig.update_layout(\n",
    "        font=dict(\n",
    "            family=\"Times New Roman, Bold\",\n",
    "            size=20,\n",
    "            color=\"black\"\n",
    "        )\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "zHvs-m1_nnRe",
    "outputId": "6b03006f-a9c8-43b1-a94c-cea5e0befd92"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c96c2b46-f119-425d-9ce4-07fd1ba3ad7f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Model Accuracy</th>\n",
       "      <th>Model Loss</th>\n",
       "      <th>Accuracies</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DenseNet201</td>\n",
       "      <td>93.96</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>92.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VGG19</td>\n",
       "      <td>86.77</td>\n",
       "      <td>0.2958</td>\n",
       "      <td>85.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xception</td>\n",
       "      <td>82.78</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>84.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ResNet50</td>\n",
       "      <td>81.41</td>\n",
       "      <td>0.3806</td>\n",
       "      <td>81.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c96c2b46-f119-425d-9ce4-07fd1ba3ad7f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c96c2b46-f119-425d-9ce4-07fd1ba3ad7f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c96c2b46-f119-425d-9ce4-07fd1ba3ad7f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         Model  Model Accuracy  Model Loss  Accuracies  Precision  Recall  \\\n",
       "0  DenseNet201           93.96      0.1601        92.0       93.0    92.0   \n",
       "2        VGG19           86.77      0.2958        85.0       89.0    85.0   \n",
       "1     Xception           82.78      0.4426        84.0       77.0    84.0   \n",
       "3     ResNet50           81.41      0.3806        81.0       87.0    81.0   \n",
       "\n",
       "   F1-Score  \n",
       "0      92.0  \n",
       "2      86.0  \n",
       "1      79.0  \n",
       "3      82.0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accdf.to_csv(\"Model_Perform.csv\")\n",
    "accdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwdSU6t4xDl1"
   },
   "source": [
    "# Testing Disease Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "ZXK8VdtSxDl4",
    "outputId": "5fff711a-bdba-433b-bae6-e3ca4de22eb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.2549568e-06 9.9999774e-01 1.1219906e-11]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9WYxk13km+N3Y9z0zcq3M2lnFrbiYq2y2IHBkDwRJDdpGy3aj0TAsPYwNDDAPbfhhZjBPxqB7jBkM0LIaFqZt96inBY/RliBpTNiyqJUiKRaLpdr3yso9IyMyY8lY7zxEfSf/OHluxI3MKjJJ5g8kMuLGveeee+6/b8eybRsHcAAH8MkFz4c9gQM4gAP4cOGACRzAAXzC4YAJHMABfMLhgAkcwAF8wuGACRzAAXzC4YAJHMABfMLhoTEBy7J+3bKsy5ZlXbMs648f1n0O4AAOYG9gPYw8AcuyvACuAHgVwByAtwB8ybbtCw/8ZgdwAAewJ3hYmsBzAK7Ztn3Dtu0GgP8M4AsP6V4HcAAHsAfwPaRxJwHcFd/nADzvdLLX67X9fv9DmsoBDAOWZT3Q8Q4yUvcP1Ov1Vdu2R/TjD4sJDATLsr4M4MsA4PP5MDMz82FN5RMHlmXB4/HsOOb2WknY/NzvetM57Xb7gEF8wHDlypXbpuMPiwncAzAtvk/dP6bAtu2vAfgaAIRCoQNseAjg9Xr7/q4Trk7gJtB/l2M4XW9iEF6v1/HcVqvVdw4H8GDhYTGBtwActyzrMLrE/y8A/M5DutcB3AcS/TAqvSTcvUpmNwxEZxom8Hq9PfNqt9t7mtcB9IeHwgRs225ZlvWHAP4/AF4AX7dt+5cP416fVDCp87ux5x+0Sm7SBnTidzOG/K9rDbZtH5gSDxAemk/Atu3vAPjOwxr/kwIS2UkMJHhJWPuFKPrNY7dORxOD63Q6xs8HMDx8aI7BA3AHUgp6PJ4eJ9sgp5wbp90gPwCvHWTruyV+N3PqB7xOroX8vF+Y4UcJDpjAhwwmopDqsK4KOxHPXkN7+jyocbghqg+S8HRi17UE6T84YAju4IAJ7DPweDw9TGAYCTvI279XKazfaxgik2bMgwR9HjIi0ul0DkwFF3DABD5A6Cf1gS4DkKrtMNDPJHBDeCYGMuy9PkhwMwc6Tw8YQX84YAIfEDgRNsNhEvZCZLpk1P0Ibuf1IMDJxBhGixhW49CBjJX3p7nwoLSijwMcMIEPGGTY62EgoNtkHTfjSPV9r9qJyTk4TLbhbn/X5+PzdVG+2WweMID7cMAEPkCQSAh01VQ93v9hw4PWDPo5NR8WEbphLj6f7yA78T4cMIEPAEj8upNuvzEAYFv93k1KsdN4HzS4maueiPRJ9hscMIGHBCRy6a0eliB0YnTr5BskffsRiOme/c7bi7nwMMG0Tqa5SmYAdLWz/fYsDxsOmMBDABnmA7qIRkmjp8SaoJ80dgPDFPL0A7c2u5siot2M7RZ2kxTllGwlNYNPCjPYf/roRxycHH4P0zM/iJCkI86tmszPwyYnfRiEY5qn/l3PbehH5JZl9UQVPu5woAk8IGAobhCxO4XNnM53A3IMadu60TqcxuN1Tuq+KX15GHjYvoJ+RUxufQZyjh9nn8Eng9U9ZJCSwy1BmDLo9mPIaj/E04cNA5qu3w2z4nvl/48rfHyf7AMAEjERZFBa7G4k8zDnOknr3SC/ft2w1YGm0t8PIjHJ5PRzMg3cmlGSGexHRr1XOGACLsCEwJbVLe6RPoBBiD4oW24vxTqm+Q17Dz3TsJ/n/4MiiGHrEwZdO6xvRncaer3ej51W8PF6mg8IyAAAd4RP6OeIGgbc3k9nTsMwGTfMrN98TI66Qc/pxqHaz1G51xRjE5jG06M/H3U4YAIuQCKwZAD8bjIHJDENiqc7hQN1pNbHciJG/TqpyvYj3ocl3d1KX7daSr9zBzGJYa5xOoc48HHRCA6iAy6hH/fvdDpGIh4mVi8ltVtC7GcCmHINnJiE6Xu/cfv5DEzPIR1zbpmg0/13C3vVEEzPSmfwR71keddMwLKsaQB/CSAPwAbwNdu2/3fLsv5nAH8AYOX+qX9id1uNfWRBZwAmYtpLcs9uz3cDTqExHpPebxlmZBzdtm1jow59LPlH4tDNERKKnBN/2+uzm5huP+3rQYxP4LN+VBui7kUTaAH4H2zb/oVlWXEA71iW9fr93/7Mtu1/u/fpfXjAF86CHyc1+mHZof0Id9C1PFd3alF9bbfbaLfb6HQ6aLfbO0wMHdxoAvL+kuDlMQKZhKz3l8k7w6rnci5O2k+/cUyanOm5BoHP5/tIFiTtmgnYtr0AYOH+503Lsi6iu/PQxwb6MYB+av2DkmomYnOyr01Mw+fzwePxoN1uo9Vq9SC7lNy7ZWJun1M+DxmEbduKYGhje71exZxMzzgs7Cba4oYRmdab3/kMHyV4IJ4Ny7JmATwF4M37h/7QsqxzlmV93bKs9IO4xwcNOgPop7Y6OZ10NZnQz6mln+dGSsk5czu3druNZrOJRqOBVqvV41Q0qe+6za4znr0wNv3euqOSqnSz2VQ7E9m2rZiYaQ5uPsv766D7K4Z9FtNY/CzLxT8KsGcmYFlWDMDfAPjvbdveAPDvARwFcAZdTeHfOVz3Zcuy3rYs6+39xjkZ+3dCDl16Oknq3XjAnexap/l4PB74/X54PB5F9Dwu7+fEkPr9Noxj0w0Mikbov7daLbTbbVWNqZ87SENyuo9+Lydzx2ncfvMgfJQYwZ5malmWH10G8J9s2/5/AcC27SXx+38A8G3TtfY+3IbMlBWmq7JORCzP4XU8bjqX93MiNBOTkSontQQpOfupsvp8TOZAP9PAZGsPa0f3M2WcfpN/1Iqk9mC6zzCwm1RvN34GYPs9cZ57NRMfFuxaE7C6T/QXAC7atv2/iePj4rR/DuD87qf3wYF0Vg3jWALcSQ/9NyepLyW/0xylXU0kM6n7/UBnMm6ZiLzWLTE4jWkaz2SG6M/Lc4dJ2tkNo+jHyAddA2y/r4fVSu5BwV40gZcB/EsA71uWdfb+sT8B8CXLss6gGza8BeAre5rhBwB6849BILUC3bZ087JN0qef6WFZvR5s2vjydycC1iVXP1Om3zyGPcfpfhJM93ZiRvIzHYfS5Nltl2Z9Ldw+m2kdnYTHfu96vJfowI8AmFbsI5UTsJs2307qs44I/SSJE7KZENHn86HRaPQ0x+ynrZiISGcag+ajM7p+BGIygZzWwTSWG83J9Dx62bTJzh/k0O3nV3B6zybGRhxyupdkBMM6Ix82fDzyHncJMla925fiZMvq50ik7XeNtCc9Hg9arRYajQYsy10l2yCCdPsMPG5ymvXzjTjNx834TnPpZzpJ/4Bb7ajf3OScdMLXna395mx6tv1ac/CJZQJ67rduU5tA/81EYG5s7X6E4fF44PP5VLhM2pNOUrHfHNw8h7zGyWfR7979fCL9JK0b6Ge6SJDzNiUqmeZsun5YMDHbfs/s9Xr3XeRgf83mA4RhVLJ+6qsu3QcRoOl3Ikar1XJU+eW1kmGZfBOSuZkkou7Y62da6OfpHvJBHnMpPZ2eS8JeJKW8VnrkTetoYujyvxzT9Ju+9k5r0G+8/QKfSE3A5/MN5Qh0A04vdpANTpOkXq+j1Wo5Iqfp+yBfgxszQBKmk7Tkfz2192Eg817GNEUWTGO6NWf2qrk43UuaFvsB9s9MHhLoKrlbVayfZNdV/H4mRD9Pt9/vR7vdVgk++jn9bFY3avawBNWPqeg2sZPZsBvoN3/d9nc7V50J8Hwnv0o/Rj1o3k4mXz9z0bKsBy6IdgsfeyYgQV/0fsQrfx/GbJDX8pies+/1euH3+43EbxprkMTXEXAQQg/SFiSjkfZ1vw698jed8TrdR5+/03cTOL0bqaabxmUGopynmzk63cP0m9tjlrU/Uow//Bk8ZOAL0BnAIE7tBG6lnp7zz3v5/X5sbW05eoqdJJQkMD2kaXJO9fMFDEL2fkQpNQJmxPGz/rwmQgkEAvD7/WqurBnQY+j8XWYJupmrCXTmyMpJr9fbU9486Ho3x3ejFbEnwYcFH3smAJhDM4PUNzcv08350vYHtjfCNDmnnK41EbR+fwmmun0nB5l0JJLAZbgNAMLhMDKZDAAgFAqpIiWZtsyEK71keWtrC1tbW9jc3FRSmKXMtt0tEgoEAqjX68bncePUk2tExuFkc8t7SKeuyW6X1zjBIE3C6R1J+LCTiT72TMDJ7hokHQfZdqZrdCBxEDGdVFQnhJOed4mounNOZ0ZSC3HyXuvPT2cpcxNkKjIlJu/l8/kQDod3mAz6WrZaLQSDQaRSKUQiEaytranyYcls3K7tMGaDG+LT12NYKT5sZKgffJiNST62TMCt6ivPd8v9TeOakKgfA+hnv5ueQf45qefyd8kwdImn+yj0Nlk0WwidTgebm5s9z9VoNHpMk2azqdR63odMIBAIAMCOc2QJs1SJ3fgsdIYo18zE4PXP/cbQ178fDMIvkzZnmhP/HzCBBwhOqraT6ufWJnQ610SYZAAS6Ye5N6+hNsNn8nq9yrYOBoMIhUI9qjjVdJYWy74ClPKS4Px+P5rNppLSzFvgfBnB4L3b7Tbq9XqPvc7rJYF1Oh34/X54vV4Eg0HU6/UeU4PaBx1k0lEqNRxTo1TJyMhkpWki34lJu+r3Wb9GfjcxKJ2hDDJp9PH2qpHsFT7WTMBkCvQjuEEq86CXrY8n23cNmqtJ6gPdMGI4HIbP50MsFkMoFEIsFkMymUQkEkE0GkUsFlM9BYDtdl31eh1bW1toNBqoVqvKPq/Vatja2lLHms0mtra21HORmE357uVyWR0zSVX5DFLFrVarPUyDTKLdbiMQCMDr9aruRwQyNdkHUc/RJxPhWJ1ORzE6EzjZ8PJZ3BKi07P3u6fTtfxPJvtBMoKPFROQL7EfA3B6+YNepuk+OlPgfZvNZt9xTJqKHINE7ff7cejQIczMzODUqVMYGxtDKpVCIBDYIbn0noF6GSu1g2q1ilKphPn5eVy9ehXz8/OwLEsxB0lQcj2lmeGkaQHbqc+cl6x61J+31WrB4/EgHA4jGo2iXC6rsYPBILxer9IydLOKGksikUAikYDf70en08Hq6irK5TKazeYOn4lbP4G+tqbrBhWeDdLynL5/0IzA+qBVDxOEQiF7ZmZmz+NQehHx3ap0+kvW7Wp5nOPoY3U6HYWURD75uzxPn4McPxAIIBQKIRQKYWZmBidOnMAzzzyDdDqNQCCgJLwJSaR0lmNKdVp68ZmrsLGxgbNnz+L69eu4fPkyNjc30Wg0+vYpoKSVzIJA5iVND8k0dXWfc4lEIspkoVnDZ5XvTwev14tEIoHR0VEVgahUKtjY2FAaiAn6+Q+k+ca/QUTv5IfSccwtzT1oRnDlypV3bNt+Vj/+sWICQK8ZYJK0/a5zAt3RpoefbNt2TP4ZJHkAKNs5mUzi2LFjeOaZZ3Dy5Enk83llf8sOQibHmp4r70arkWMEAgElvVdXV/H222/j/fffx507d1AqlXao1/JaqXVxfiyAkvenNkF/g97mHNhujkq1Xj4HGQr/5Hug9kHtgtpBo9FAsVhEvV7vmYuTdO/nJzCda1pzRj5MPgPTfUznWJblqEHtFj4RTEBPCXXDBNzadE5cngQZCASUXd2vPNl0r0gkgtHRUTz99NN4/vnnMTk5Cdu2HSW+SRWXDki5HvKeugQ2MTPLspQmUqvVcOnSJfz4xz/GuXPnUC6XVVRAn4++z4DH40EgEEAikUA8Hsfo6ChGRkZ6/BmBQKDH70DfxObmJsrlMlZWVrCxsYHV1VVUKhXU63XFXGSnYt5PPp/H40E0GkUikYDX60WxWMTGxsaO90J/ghxLniM1AJ6rn+ck4Qf5GPodl+/0QdHox54J9Ku1ly9DHjNBPyZgup4SaGtrq4f56OPIzUuJdKFQCKOjo/j1X/91nD59GtFoVKnD+rPJuLoulQcVo0hTQIIuqfUmHQBUBMLr9aJWq2F+fh6FQgHz8/PKyejxeJBMJjEyMoJsNotkMol4PI5AIIBgMNijGeiOMLmecv3YOVnOeWtrCxsbG1hcXMTVq1dx5coVLC4uYnl52WiC0Syh87TVamF1dVUlbOkEasITrotTQk8/aT/onch16Hf9g9rh6BPDBID+6u+g5x3EKPp5evlZz8BjKEza2clkEl/84hfxzDPPIB6Pw7ZtFUO3bbvHr8Fx6LCT95V2tbwX/RMkJF1S8nqWL9PskCm88hopcTk+gB57XTruJJPU18hkI3NsiexSAktpzz+q/0tLS7h27RreeOMNLC0t7dBYLKsbBs1kMojFYrh165YxauOkEfaT1k744eQX2A08bCbwsYgO6E4rpxfR72UOy8WJhLQ1JUhHGIlya2tLSfHR0VF86lOfwosvvohwONxDeLrEls4p/s7PTLklscu4u0nz4bxIwJ1OR+UbAF2zZGNjQ6Xw6nOQ/geONcjppktZOa7+npxMLT6/yc72eDzI5/MYGxvDyZMnce3aNfzoRz/CzZs3UalUFGOib8Dj8WB8fBxLS0s9ERx9nnJ+EnRnoY53Tv6E3TAAnZE+rLTiPWsClmXdArAJoA2gZdv2s5ZlZQD8PwBm0W02+tu2ba87jbFbTYASU3ptdQeNW8nfj3E42eR6vzjdTvT5fEpasWjoyJEj+K3f+i1MTk4iEon0SFAnpJHPxEShYDConGi8D2PtUh3WGYZ0AJrW8/bt20ojkHF7Kb2lY7LfdmOcq34tzzNpDk4S0+n9MKWZYbVWq4VarYa5uTlcuXIF58+fx507d1TEIRQKKT8FjzuB6R04aQBOc3TSLnYDe9UIHrYm8GnbtlfF9z8G8A+2bf+pZVl/fP/7v3lA9xoIw0h3J+RyGofIT2mo29lyHKqvzWYTXq8Xo6Oj+OxnP4sTJ07A4/EoSeQkfSQzsywL0WhUhdO8Xi/C4bBysEnG1Gg0lDpsWRYikYiS9lIjoK0v78cchPX1daUV6KqzjuT62tEWZ40Bj0kpKv0EjAQQwXWTR/e1SCks91QEulWK0WgU6XQa09PTGBsbwzvvvIObN29ifX0dtVpNCY9MJoPV1VVHwjI98zAC5kGb2ntlIk7wsMyBLwD4Z/c//0cA/4SHwASkzUpwy3l1qT3oHHmu3vJav59OJNFoFCdOnMATTzyBRx99tCepRy/20SWpz+dDOp1GOBxGKBRS99za2kIwGEQkElHSUBb+UErLij6q1JR+lI62basGJ8lkUlUKxmIxNBoNlV0oE3ZMlYperxfxeBzJZFIxylgsBgAqbVnOhck9shqRn+mj0N+nE7Pm2jGs5vV6kU6n8cQTT2BkZAQ3b97E2bNncffuXVQqFaytrSGXy6koyLAE7fTdyRwYNK6bc03a14OAB8EEbAB/b1mWDeDP7e7OQnm7u2EpACyiu315D1iW9WUAXwZ2v2WTdMC54bpufARO1/FcUyhOB2n/hkIhHDlyBJ/97Gdx/Phx5YijBJbnSubCNOFkMolsNquq9lqtlkr/lfn7tHvZnFQyqFqtpp5DJxjOgXkOo6OjaDabKj2Zpgwz8FhHsLW11bPJKaV+NptFJpNRiT7hcFiNwRZqtVpNMSA5T2ku8HcWHXGeTu9MX0MylWg0ilOnTmF2dhb5fB6/+MUv8NZbb6FarWJjYwPRaFTN1fRedWeq/o5N5z0oia3jtWQCwzCPQfAgmMCnbNu+Z1nWKIDXLcu6JH+0bdu+zyCgHd/zNmROC+BGAxh0njxXz8k3IYouGemNPnr0KJ5++mmcPHlSqcA6YvGlkvhDoZCSqLlcThG4jKMTaWXiDP9oLpAZEHn0vAEJjUZDSd5yuay0EDKOVCql5s2aBBIn0O05wBoGrkEwGFQMjz6MarWKarWqQnp6VIfHeD7rHmT2oa7FyXF0gqVmEAgE8PTTTyOXy6HVauHGjRtYXl5WORFMqXbCDV1r038z+Y36/a6f56SZmrREalkPCvbMBGzbvnf//7JlWX8L4DkAS5Zljdu2vWB1tyVb3ut9xP0AbGefOangOjiZDW7VMJ0hOJ1DqZpOp/HCCy/g137t15DP51Gr1dSLkxWGfAav16ucVplMRjXxIANYXl7G+vr6DlWw2WwqdZYMIBwOKz8BQ3qyX4BuRvE8YJsh1et1ZUNTKwkEAmg0GggEAojH42re0tanY9Lj8ShmEYlEAACbm5s9GgHvp79DMjWuSSgUQqfTUbUNekqzfCaOJ/0YzL3wer2YmprCV77yFfz0pz/FN77xDaytrSEWiyEYDKpznfCI9+unUQ7K2XACk5/FZGJwXfYNE7AsKwrAY9v25v3P/w2A/wXA3wH4VwD+9P7//7rXiQLDqe/D/Oakwkmk1L3gJqbDlzQyMoInnngCTz/9NPL5/I7ae53TM28+HA4jlUohnU73aAnXr19HvV5XkrrVamFzc7Mniw6Aqt1PJpPKwSfDgZSI0pkoGRfn5vP5lFkRDAZ71oFVhKVSCa1WC5FIBPF4XJkHlmUpZlapVGDbtupFIPMQSExSOzEhvW3bylyMRqMIh8M9FZC8hjUDvFbP2pT+Br/fj09/+tOYn5/HG2+8gVqthmg0CsvqFlHJtu8STAQuGYPpGQbhmBvQxxxkjg4Le9UE8gD+9v6EfAD+b9u2v2dZ1lsA/otlWb8P4DaA397jfXqAaipBLtJubSUndY5hN9Nvunbh8/kwMjKC1157DU899RT8fj8qlYqj9CDx0/6mClyv11Wx0OrqKkqlEpaXl/Gzn/0M9+7dQ6VSQbVaBbCtGlLaybBhIpHAqVOnkE6nkUwmlY9hdHQUuVwOpVJJzVs+VyKRcFynXC63Yy1IqJTQUjPQ6wPIEG17OzmKeQ6S8XGN6PDku/B4PIjFYohGo6jX60q7kNfpwPnIsRqNBn7v934P8Xgcr7/+OqrVqgohsnJRMne5Pk5jO0UMdsMAnPCRayqjS3uFj2TGoF4lqDMEN4xgkM3GhaZ0c1INpTPoiSeewOc//3nMzs4q1VSqqcB2tWEgEFASf2xsDO12G5ubmwgEAkilUlhdXcV7772Ht99+Gz/60Y961Gw6CIFthsjiJRKTTOihZJcmDasVp6am8MQTT+DkyZNqbU0SWdeGeF8yl5GREYRCIfU7TRgyQJYwU5NhBIDz0yW37n/huX6/XyU4cf0bjQZKpdKOugaTvS3fH6sPV1ZW8NWvfhXXr19XRAZsO1T74Y9uYjrlrJiuc9JK9bmafgewQzgNgo9NxqBJfQScs86cNAYdnAjdibvrKms6ncbnP/95nD59ukciyvsSKMkAYHx8XDm/QqEQotEoFhYW8Prrr+O73/2u8gNQAlarVUUM9BTT+cbko1qtphgGiYlEK9OEa7UaCoUCLl26hFOnTmF8fBxHjhxBOBzuCUWWSiXljKxUKvB4PDhz5gwOHz6snH3Xr19Hp9NRuQudTqcnS7JWq6k2Y3wOmSfA6IdkBjI9mesom5dSapNxsoeh7meQarxOtOVyGSMjI3j11Vfh8Xhw7dq1nuiNEzhpCfr4g36X+KT/1s/3QBx4ENrAR04TMLUOd2IAJtvd6VwdnLQLffxOp4NIJILf//3fx7PPPttT3SalGlXZVCqFTqdbOjwzM4NarYZSqYRkMgmfz4cf/OAH+Ku/+iuVqNNqtVQxDkNzzBCUHm064qR6Kp+B1wBQUpj1DLyez0TnE7/LRB72Ofid3/kdHDp0qKfMWJoAure+2WwiEAio56E2IDMeeZ5Mo+baUWOQadGNRkP9TqbYaDRUPwT9/esCRL6jcDiMcrmM73znO/iHf/gHNBoN1RJNZjaacExqWA8LdA2C9xzUvEbCx0ITMPW2HyTpTeqXGwYgHU2m80lkfr8fs7OzeO6555QdqXN1GfprNpvw+XyYnp5GsVhErVZDJpNBOBzG3//93+Ob3/wmVlZWelR+dvthpSLvKxGQhCjTfOX9KVmlBlEoFBAIBNScGo2GMgd472azqZhLPB7H0aNH8cgjjyCZTKJcLu8oIOJnaULw/pw/fRGU0pIp+f1+lbgkzRqZYUgNIRgMqnN4nt/vRyKRQLVa7YkiyHemd08Guk7LWCyGZ555BqVSCWfPnkW9XkcoFFJmjBOu9MNJntPv+mFBjsXmLXuBjxQTcFpcp4XsZ5MNuoeTWUHk5n1PnTqF1157Tf0mpQLPoeOL3Xenp6dRqVRQKBQwMTGB9fV1fPOb38Qbb7yBO3fuKA+2zA7TfRMkQBPT4XxNXnjWDxSLxR5fRbvdVuo1j9dqNXQ6HSQSCYyNjSGXy+H555/HxMREj3SXDIDrwDXgH8ut5fuit15KMxmOlFoEz+dW7VKSM0WZ59PWZ2ISmaj8Y4mzzNys1+uYmprC5z//eUxNTeFb3/qWysyk81Hi1SDccwNu8dPJrzHoHDfwkd+GzK3zbxhwE4KxbRvBYBAvvvgiTpw4saNGnfenFCSyTU5OIhwOY21tDSMjIwiHw/jud7+L119/HXfu3OnxStP5JW16KdX0fgC8l1T9eQ2Rn0lB+hj8jc9OEyAajWJychJHjx7FY489hsnJSRU9oESmBKeDT66DrHQMh8OqXkGujT6/Wq3WMxdZ/MS8Acb1ZQKRniLNe7JRK9+bFB7SRKD2Nzo6iqeeegqPP/64CqXqIVUn/NJ9RfrnfmAyQZ2uc/JT7QbnPzKawCB7y4lTyuNuFsjkbORnORbTgZ966qkdL0wSASVhMBhU1Wu1Wg2JRAK3bt3CxYsX8cMf/lB50umUk+YAkVw3h3RnE0Nb0taW/gEmzHAcFvfYtt2T22/bNsLhMBKJBHK5HI4ePYrp6WnkcjlEIhGVlMRndMqilMyp3W4rQpQaANODeZ58BkmkulZBxkKm2Ww2d3RcJjBfgqnRMtLB85lZSI0rk8ng1VdfRTKZxBtvvKHmL6MuJiefjidupfMgjaLf9XutJ/hIMAEix16dmP3UN3mOfr7+MgOBAMbGxvCrv/qrSKfTKmav+x2YqUfvdTabBdBF0Lm5Ofzd3/0dfvGLX6ie/pbVTbVlvv3y8rKScLTbZddf3e/AY2QCwWBQIbUMufE7QzMzZ1UAACAASURBVG2VSgVA13lIZpVIJJBOpzE1NYUjR44gk8koaS/3E+SYfEcyPVkyKZm+LNVv2u0k+mAwqBx80gfC+bElGcupmUrN55MSXppLPJ/j8p5cO5mTT4Zy/PhxJBIJrK+v46233oJtbxc9DbLDnQSI0zlujus4Kd898Wo3sO+ZgJQGg84DdmoEepjOyXnoZFsTpJobj8fx2GOP4YUXXkClUulJYZaOMIavIpEIUqkUQqEQbNvG4uIi/vqv/xrnzp1TUpWI1Ww2EY/HEYlEsLi4qAiI0pNEIDPvZC8BmVZKM4KIzaSiRqOhJDkdgp1OB6FQCPl8HiMjI4hGoxgdHcXU1JRiAJxrJBJREpiEwznItZJMUdYxEOgnqdVqqFQqan6RSERpLhLpZR8F2aug0WgoKc3PUnBw7Vh+LWsSJNMEtrUuJl+NjIzgC1/4gqo14H2lKTUIhmUAg8wH3aThmrs1O3TY90zACZzUrEGL6wTSSaYf1yVcMpnE008/jUgkglKptCODUdrDTG5htdrq6iq++tWv4vz58z058mQC9XodtVoNsVhMIbEMi7I/P5GXSN5oNHqcf3TCMX2XfQGk+qvH8pvNJiKRCHw+Hw4fPoyxsbGePQc9Hg8SiQQ8Ho+qV+D8pM3NNZDMW5deuu+i1WqhWq2iXq8bGQA1FPoCAKi+CZVKBT6fT+Ve6L4SAs0h+iUYgtU1FalRAcDs7CxefPFF/OAHP0CxWOzpxuTUCNYJ55yEkJMU181bHfaqHQMfISagS+ph7Xu3v+njMgxFCRoOhzE9PY1HH31UbZQhVWIivgx3UTJXq1X8+Z//Oc6dO4dIJKLU2FarhVAoBMuyVCos89lJbBx/fX0d+Xxe1RLooTRKeDrSZmZmcOPGDaXyA1CmAsuD+QzxeBz1eh0zMzM4fPgwYrFYD7EGAgHEYjEUCgVUq1XF4KjCSz8Ix6QDVfeZ8JkonXkf1kRw7ckgZcRAhkEbjQbK5bLqf0ATSJo/MueBYVrmbBQKhR3Vg1KD6HS6+zB+6UtfQiKRwDe+8Q3U6/WeRi1OGoGTr8QNLg7SCB4E8RP2ZXRAhl/67SQ0zHhu7yePyYw82tb5fB6nTp3qaQqq94enysjr6/U6FhYW8NZbb+HnP/85gG4BDpGZCTLSmUfCpglB6dhut3H58mV4PN0Ov9LTTpudjEOPv9OZJm1rElkwGEQ8Hsfx48cVA+A+h5R8Pp8PlUoFm5ubPSq6LsVIQDK6QTA5Ei2rG+YLhUI9eyDKFGvZGYnXSJOLyVQy+Ukfh/c5dOiQup7ErCc3yfnRTHn11VcxMjKimI+On7pT2clZamIE0vdkcjCbztdBdxy7hX3JBJyARAe4i4lKtcx0HNhGFp0BEEFkwsnIyAheeOEFvPDCCz3qMMfhf3qjyUBKpRLefPNN/MVf/IVCHJoYbOktO/6EQiGk0+keG5kEQu2iWq2q+gMiO52HbF5arVaxsLCgVGyPp1sgk0qlEIvF1HWRSASzs7N4/PHH8fLLL2NkZERpL3IzU0YRms2mqljkf1murJtQ+nvQ/TTUAlgByeeliQCgJ2Ig1yESiSCTySizZ3V1FcvLy1hdXcXGxobyi/B+Gxsb8Pv9OHbsmEqAYts2GWJkNIBEyJyJP/iDP8DU1BRs21brKoufeB+JqyZC7qeJ6ucPYwbshgl8ZMwBYPgEDDfn6xl2uiSQJsj09DROnTqlvMzyXjyXklY689bX1/Hmm29idXVVJamQKJl2nEgklOpJz73sn5fL5VTCClVeSsFKpaIcYpT49IBT4/D5fGpcahe23S0tzuVyOH36ND7zmc8glUr1eP11DYJmAKU8sDOVm0AfhMy+lBEK/b1Krz8z/uT4ZAIch0xWzsPn8/W0WZPMiAlQhOnpady7d68ndKhHNmQ0ptFo4NFHH8WnP/1pfOtb30KhUFBrbsrb6AeDzjHh5V5NYCfYl0yABDKoiGO3Y+tMwqTGSacW1fKjR4/i8OHDxuoySj+qz0TAZrOJO3fu4NKlS0pKhkIh5aH3er2IxWIYHR1V++ZRxWZKbzAYRKlUUogQi8VUzJ7JNTRZgsEgRkdHsbS0pJ6DmgbVYmogHo8Hx44dw/T0ND73uc8hHo8r4pJbhXF9QqEQwuFwz/Pr2pAkcJMPgL/pGYY0vzg/1kuwJwETnPg8XJtGo6HsfPY4ZDalZVlKmyEjCwQC2NjYUONHo1EVbaFpp2ucsonHxsYGXn31Vbz//vuqjJmMlY5WPp98Vifp7kTYg3wJTpoCNdBhwoX72hwwOUZ4zC3H088f5np5HkNnemIGXy6lo+yXaFkWVldXcfXq1R5pIu2/ra0tFAoF3Lt3D0tLSygWi6oHwZEjR2BZlkJiEglTjm17u46fKnm73UYmk1FScWxsTDXMiEajGBkZgd/vx9LSErLZLEZHR/Hyyy+rFlvSAUsC2traQrFYxNraGmzbRjab7XkGMkBd7ddVZH6XRCZBT8JhZaTsN0DC57OHw+EeKSy1AN6X9Q98b8ViUf3PZrM9vg89EsR5AdtJT36/H0ePHkUymVSMgz4dnbilo1SCKZ/CCUwMYFg66Af7UhMAdp/y6/ZaGReWyEwgV2fhz5kzZzAzM9NTtUcEoBmgJ8Tcu3cPv/jFL3DlypUdcXKPx6Ps7GaziXK5jHg8jlAohFKphHK5rGxdqr/SP9FqtbCysqJaibEUmWGvUCiEbDaLaDSqKv2q1aqydYPBII4fP47PfvazyOfzPW3LpdTmc5J5sZiI5opcbxPx93s/OmMgscnrZf6BNKXkPbmO9JVwXBkdoOkAQLU9Yxg2m832FEIxbCpNQ8kUKpUKfuVXfgVLS0t45513lCmmCwf52eR4lrjm5DMw4bKuuTr95hb2JRPQ1UoncJPjT9DNAF1KyXvzGNXro0eP4vHHH0cul+sJB1GFZYhMEnm73VZpwXQiUs2ORqMol8s9pb/szMukH7YQSyQSqthHX6NarYZQKITx8XF4PB7ltd/a2lIx9UKh0FMO6/F4EA6HMTU1hZdeegnT09NKwyDB6O+A11E7YBRDl5j6WpsIgesmVWUZzpPvQ2bx8Z791GfOUY6jZwVyrNXVVUxNTWFjY0OFH2UzU7mzlEwAozYwPj6OJ554AisrK7h586axdFlqVU7SnJ/7gU7YTmaF/KxrrP1gX5oDJknB76bzBo3lxE11760T9z5+/DjGx8dV+E4uOL37ElEsy8Lm5iYWFxfV1lfMB2i1Wkin0z2hQKDLNCqVito6S5bLyjAebXlmyNEfwEgC7+X3+1Eul1V/PzrcuAPy0aNHMTMz0yNpge0UbZoTMuoh4+eSKPXwmv5+TO9TSlrdP6B/1rUTqaXI0ChNBF3r4trIegT6NdiYIxqNqlAkzQIdLyTD83q9mJ2dxfj4OACzGTIMmNZOF1ZOa6nfz40AlbBrTcCyrJPobjVGOALgfwSQAvAHAFbuH/8T27a/s9v7DJjDDm7ab/ElUppsNwn8PRQK4fDhw2rHYP18hu7omKINf/fuXSwuLvb0/aNkknn70vHErD9K5EgkopCVEozOLqCLeHSC0S+QSCR6MuGi0agal12QZ2ZmMDMzo4qByDhMjj3a2xIppc3u1NnGhMByXU3al36O/N10HefJPAja68B2mE9qGjyXqnu5XEYsFlMaFcOfHEeWOOtzbDQayGazGBkZUTkKUtvop/47rcmgY27HGdaU3jUTsG37MoAz92/qBXAPwN8C+NcA/sy27X+7h7EHqjwEkwTSzzVxSafr9DlkMhmMjIyo0JMEmgK0RymxS6USLl++rHL/aQaQMFdWVhAKhVSOPENvZCQy950ZbfJ5pFrMzUhYoxCNRlWUIRqNIhKJoFgsKo0lk8lgamoK+Xxe9Q8AtsNw+trooSpdvaW/YpBNa9IA5P1M786tnUxGyrWkyk7ti74UXkumt7a2phyPlmWpmohWq9UTmdBNDZqK4XAYo6OjyGQyqoaEzIjhTTdgeh59jfvBbvwAEh6UT+AzAK7btn17WC5kAidbcjfXD/Mb0FulRZWPTjCJBHSOEeGSyaRKqrl16xbu3r2Ler2upL3P59vRe49bi1G627bdU0VYq9V6wlPMT2CoC4DaBIRps8ViUfkWarWasm25hVkwGFTJQk42tm5LSilo0qCcHGL6WCYGIO1/niNj8yRYnZnwvnJu7NQs1Xdpy0ttDNhW4ZnzIBOfaGJQC9KZAR2NIyMjOHToEObn53vG0TUkXY13YnD6muvr5XS+ae3dwoPyCfwLAN8Q3//QsqxzlmV93bKstOkCy7K+bFnW25ZlvS2dbSZbzPRZZxSm4/J3+VkPUTm9kHA4jOeffx7RaNTYR4/qtNfrxfj4uNIEfvnLX2JpaanH3rdtG8lkEocPH0Yul0Oz2cTa2hru3buH+fl55XBjz8FyuawSfaLRqArhWVbXQy+bdDBGPTc3h0KhoCQYPeUej0f1LGRFI+Pjct1lxIR/cp2kViCJW15LAqG05J8sdpIgnZL0M/AesmkJf+dnqanI98f3xJ2amHTE8zgXjsPkLB28Xq/KJOTz8h3xWKvVwtTUFE6ePKnenWmrel3rdEOs/bRUjuGkQci1dQN7ZgKWZQUAfB7AN+8f+vcAjqJrKiwA+Hem62zb/ppt28/atv2sU33AoIXQz3Vz/aAx+TvVZ90hyPAaJYVlWap6LRgMolarYX19XXn+eV6j0cDS0hLu3LmjdtEh8tH+5HFqFCQIjkNbn4yAmgMRkvYwU2NZcSfrHJjQpHfa4WfJkGWWn75mZHgkRF0b0NV9qUHxXryHHlmR7wLo9c7LMXWVWbYh9/v9qtpyeXkZV69exc2bN1X3Js6XjlhpalHDo4kn8UI2XqEWkU6nkU6n1bqYhJcJx3TtRv4m19LNeTo43dsED8Ic+A0Av7Bte+n+5JbERP4DgG/v9QaDOKhbzgq4SxPmiz927FgPsTDpJBgMqgo7AMpDHIlEcPPmTeWRZ2wd6EomSjW5JZlt22r3X+m8oh9AJqjQX8CQJE0BjitV4UqlooiACEtbV24uIj378r9MwJHrp38mSClJotJtWjIrOubkvZ3eKSMlpnfJdF35Hj0ejzLDPB4PCoUC3nnnHVy6dElFUsbGxvBHf/RHSiNbWlpSDj5qfUyPjsfjWF1d3eHToMnSbreRzWbx5JNP4vvf/75qTKozO6dnc/rNtNZOeN6PPkymhw4Pwhz4EoQpYHX3HiT8cwDnH8A9hgYSkG4fOiEzX24gEFAtvh577LGe6/lf1gdQ+pLgWbAjY85S/eR5tHXb7TZWVlaURJJ2J30R0h/Rbnc3KSmVSj0dcjwej0oQonawvr6u0lg5V6bPyueRSGKSyBKkWs//+g5BsiegNBV4jlwPvitdjaUko5ZBs0JqHVKCy3nQMVcul7G1tYVoNIqxsTEAUCnYV65cUfNiFqLMDNQLoOQz6JpTLBbD7OwsEomEa+11r2Ayh3cLD2IvwlcBfEUc/l8tyzoDwAZwS/utL7ipFXCrKvF3/Vwp8eRxxuTr9To2NjZw8uRJPPPMMz1db4kU0lYdHR0FsJ0BuLCw0NOZVqq9Xq8X5XJZESE9+jxfVq3JGnvLslCtVlWR0cbGhlJ/SXBer1dt9knNgfdmmy69UYdU0eV/KaFksg6fR6rFZFxSjZZMS64/rzeFIrlGHE8SpMxRkOtKJivvIZ2FN27cwLlz51CtVpHNZrGysqIY5re//W288MILyoTY2NhQ+QLSLGKJM9eQ95FrwmasqVQKGxsbKtSo4+VuQa6jjrfynZgYATWzfrAnJmDbdgVAVjv2L/cy5rAcTXe46Iuuj6d/p4S0bVup6Z1OBy+//LJqsiFVQRIvF52agM/nw40bN7C6uqpUWNqlXq9XSWXmCNDJV6vVEA6HFeHIuL1Uq+nEIsFRpWbhC8tqS6WSOkak5DVMLpJNTHTkkcxAMkyugVSDqVVYVreoifOR0pqEQkYrn0uX/hyf6ymfX2cAMiWYfhKGA/l/cXERy8vLqqSY8ywUCgiHwygUCqoehO9fajl8D2TUOnOS58XjceTzeayvr+/IMdDx1AQmYSafV342neskEN3Q077MGDQtmJPtqF8nzzeNZWIKlmX1IK9lWTh58uQOhxclkuyaK+H69esol8sqY49huXa73bNduGVZKhPQtm0lvXk/2u8cXzbjpDSXvgFuTy57DALA2NiYyiNgq3H6ETh/3W7XtR6pMUh/Ac9lUhMrH6XTTdcEOC4LgkwqNsfVIxHSDJFMREp+MlYAytEKbLdGz+fzyiQKBAKKYXMtbNtWVZnSNCOj51j6elEY0J+gdxoaxqcltTT5bkzvSQfJwPTr+sG+rB1wAjeLKKW2BBKj/E67TyIu0N0rb2JiQhGizOLTPcYAlLPrypUr2NraUghOW1Nm3DFpSNqanBfHlfvp0fmlS2fek5ELPZ5OYpNIwdTkUqmkIhqydbgcRz6fLmW4XixGImOS89OvkVqHjtz6+5PMxuS3Mam3cg25Z2K1WlXbvVerVWxubqpNX5vNJm7cuIFjx471aDeS0XKOMo1aLzXW8YmMcBAMYgwE0+/9BKLOsN2YI/tKEzDZkAQndWoQ93NSk4BtR5YM+wDA6OiokmwSuUlY+tz40hcWFhRhkLiZgUaVVfbJk8gm/1Mr4H1k3F7G4EmMREDa5JS0hUKhx5627W6FHO1WvbhGSn+dqMmE5I7CQJf4tra2lPSUcXwnU0xqXKZYtpNWIHFAzk++H65ztVpVGZJs8trp9HZf3tjY6LmfbvrI5yczNiVGSbzSTRgnvHUrpeU6mD6bYFiTel8xAcJuF0cfo984eniKCOnz+XDs2DFjX3lKBR3JmSBSLpd3jCW941QtLWu72QWJwWS+EFkpfcgcdLudDEeW1FqWhbW1NZVQxPErlYpykPEauQ4mpJVSklqNzozYtJSMxPRu5HX6fUz3NGkBHIf/pZYjG6fQ/1Kv17G2toZ6vY5cLqeImQ5Ap7i+zhBo5/N96fOROSFkwvK5+4HTOpgYnRyzH44PQ0P7yhwwSQ/TORJ0qWHSIPTrJDFLVZ1q/KOPPtqzuShBSkppizJPnEQgTQhqDnITEGA71CbVdx0BSaSU5LJKkIjHv3a7jXK5DNu2e+6ZzWaV5xsANjc3sbCwoObDlGNJ3CaPu2RAXq9XecClCQP02sxyjfgOTHka0hyRayQZpNQeTDhBjYRru7GxgdXVVaysrKDVaiGZTKpkLsvqOgj5viWO6JKc95ONY03E6PP5EI/HFbNgUZEJF/VjJs2pn3AbNKZ+/iCzYN9oAm5THN2CvlgSoUycl5I7HA7jyJEjRg+2VF8pgYFtxJcETi+1vIZOOplYI73hPIemAqWWZFiyqYjsmS/Dq2zZLaMMtGnr9TrW19dVkREdaBxXOijl2kiJKU0MHXH1sJ101sk1k2svv0vnqG4C6So2TRJW/pEJ3Lt3D4uLi4r5ce53795Vz1wqlZBKpdQcOabJ3qcZYcJRiQNM+pLrxufSNQcnf4i+Lvo1TpqBvI8Og0Lv+0oT2AsMawfpL4oEdurUKUxOTmJ9fX0Hl5ZaAO17y7KQy+V2OPJICEwc4j04V0rTRqOBTCYDy7JURhuAHpWyVqthfHxcbZUtbV+g12dAAg2FQlheXlZFSLFYTEUH1tfXUSgUVB8E2+724+dYJFZdrSejYk4FkY4MRvpLiLQkZklAJnNBEoBkHHL9dabDY2yawp4Aly5dQqlUUoU8jIzYto14PK6KsPL5/A71XppY+r1kMZeci2R+OuG7wcFBGkA/DdlJyuumbj/YN0zADRGbFsPpOt1ek5EBqXKyFwCR4bnnntvBdXXE5suW/QTpZGI3ImDbTCDBcwxp61M6k4hllh2Jl12IYrEYQqEQ1tfXYVmWqjZkApJMDFlaWlLSWnbvtW0bxWIR8/PzyGQyKjmKmYaybbhU3fnH3xielEQB7Oz2ZDJznAhavmNda5PXSUbF3ySjPnPmDBqNBpaXl5UjlpmajPtPTk4qRiZ9CYwAAb0mG21+KenlMwJQOR8AenoaOuGrG5wfxjzWNQu5pv1g3zCBQf4A+YC6BBlGCyCSUuWkxAuHw4jH45ienlYvUCKrCbGl6ri8vNzTEFOqtLyG87VtWzWyYHGRrm3Q405gqi3QrVGgM5JSORAIoFwuK4nOWDgZCH0Ftt2tLpybm8ORI0dUNtzq6iry+bwibonAkuA4hr728p2YnlmC7hvQiV3/T+bDY7I+gdEKACgUClhdXUUqlcJLL72EUCiEpaUl3L59G9VqFWtra8pmLxaLPX4fvu9oNIpoNIp6vY5isbjjvgR93q1WCxsbG6hUKipSwvN0ISTXwS0M0hZ0oaWbEf1g3zCBYTie02+DmIhuk1ISkxjr9TqmpqaUZ1lGEEz2W7VaRSqVUnsIyko/qVJKhsDYPZmOZW2Xt0YiEcRiMbTbbTWOx9PdLuvYsWN47733sLW1hUwmo0qI2YtQlrcmk8mekJ4+/3q9jvn5ebVusiFKrVbryaGQhCyZFOcmw4ymdQLQw2x100IyBI4jNS6dQUjbvdPpKN9LsVjE9773PdUctlwuI5VK4amnnsLLL7+MlZUVnDt3Dnfu3Onxp0i84X6H7Da0ubmpGKL0icj14B/xiJ+ZAEbNSmqcEid1JjosY9A1VR3cCMl9wwSA4e16oJcBDLKd5PnyeiIzd99leq68TqqiPEYCWFxcVLv/0P6XiEwpzNg8Y/o0HTges/l8Ph9isZhiRrFYDAsLC0ri+3w+tUmIlFa2batkJaDbJn1ra0v5EKQDrFqtYmNjA6FQSKUccx4yCiLXT9r5Muwp7V+dOIBe7Uuupe6XkWq+Hk0xvVeu9eLiIm7duoVKpYJQKIRyuYxKpaJ8Amtra7h27ZryW9A0y2az8Hg8ag+IbDarqkOp/kvJKsPD8ln5n3syxONxeL1e5aClJmMKO7sh+n64qx93YgT9YF8wgX62i5MPwEn6O6moTudLtW5qampHKis5u+wTSDuy1Wphbm5O7RMA9KrtHINSj5Vq7ANYr9fRaDRUhdv6+npP/jq7BzHOzWcjgYyOjiKVSqksOK/Xi1KppNTR1dXVnjx2Oi0Z1z937hxeeeUVtXkGsLOKUK6bKUeh37uSElRm5enX8rN0rukMQGcgHGd1dRXnz59XDI2Mk2r9hQsXVHOWeDyOlZUV2LaNdDqtbHxWGkpG7/V2N0otl8s9YU9dekvGmU6nkUwm0el0VHoyx9LXVT6LkzSXfhbT+spjTgyB95D+Kx32BRMAnNUWN6q+0zVO3/XfbNtW2WWmCjjGk6VKyxfD2DxtczIRSTRSUhMpaNNblqW2IucegiQaIrX0GfB/pVJRUp4VfFJlr9frKJVKKkGG6n69XldIubq6Co/Ho7QL3XlqWkNdNTcxTX0MuWayiIi/Ab07+0qNS2YLcj0ZQi2Xy1hZWUGtVlMt2yjlW60W1tfXAWyXecuICFu7sTCIeQBkoOy/qJsrfC4dp+jEZXNYvgf5jDwmzRonnOx3PzfMgJ+dTDQJ+4YJSOjnBHGSPibkNI0jf9MRO5fLKVuOY1OS64srHVOSSUinlUyk8Xg8yOVyqsqPv5EZtNtttUPQ5uamsidpS3KHIyb2ANvx62AwiK2tLaVu0qEoQ1aBQKCn36DP50OhUECpVMLo6KhCTmnbO627KWvO9E5oOsj/8h3Jz5LQOG8ZBuW5DOVWq1XcunULq6urynyjFsVNVrxeL5LJJBKJBGq1Gnw+H5LJJIrFInK5XE9vhUwmo0wJVlma8IjPofsn6LvJZrMol8tYWlrqCRdLM9HEVAZBv3P6aQpct36wr5jAbnwCbq7VpajpGq+3uxONfFk87sRdJULQ4SfVf6bvUtoy/MT21OwgJB1GiUQCIyMjSiKRyOl4IuMBthkIP8uEHz3Mx7l7PB7lh2Cegn6e7geRzyyde3Jt9cw7qTrzzxQtkFqBvKccRx6nE3ZtbQ137txR3Zjp1+A6NBoN5XtJJBKqySuwXSAmQ6JS8vPeknEBO1PNeR41HHYujkQiPWum9zWU1+p4KL/3Y7BOmoGcX79xJOwbJuAkeXSJM8hkcPpd/01HVLaS0u9BJ5u8Xn6W6cHUAuQmFpTkLPfVpSOwHY+maZFKpXoaiPJ6Wbgjs/ZknoPUIKSNS0IhUjI7kuWvek1EP1XVZL/q36VKz/nq70pmZepqMglLjsdsx7W1NczPz2NlZUVVRJIB0Cwi0TOT0O/3K/MpnU5jdnZW7e6sPx//ZE8A3fTRn5c4QK2P4Vn5np003EFEKq91owkPMy6wj5gA4PyAw1zD6yQnloine76pDtK21pkFE39op8t7kOh5DUN+wDZzoPSX24dzbrqKTDu32Wwq6RUOh5HP51EsFtVGmrJhCYHzp1bCzkQ0FZrNJlKpFDqdDtbX1xEOh3H48GGk0+kdJdb9klxMpgKv0dfaxFBkGjCZn9RaZN8AKTmr1SoWFxcxNzenGADDfOyZWKlUEIvFUCqVlDe+Wq2iXq+rPRlDoRDGxsYwOTmp5s17Spwi45Tzk9qMrmGRSWSzWSwtLSEYDKpmJvp5+hqZQBdcpnV/ULBvagecwEl9d1pQIphJ8gNQnnD296cayGo4nssXLpmEvK9kAkSksbEx5Xiq1+uqF2Cn08HY2BhGRkZURhmRhtoCsI1YPp8PW1tbuHjxIm7duoXp6WlMT0+rohc+G82FRCKhugszlZkq7tjYGBKJhPoej8cRiUSQzWbx4osvqj0KJCHoyCfX3mTXy/WX1+vmBCsn6/U6arWaaoRCBkhNh2bKxsaGiukvLi7i0qVLuH79OlZWVlSGJp97cXFRtWVvtVooFAooFouqnoBrs7W1hXA4rLo56So6x5NmF4HmRSQS2eHsZaHXc889pzIPZdm4jtOD7HdTNEHHI5ExXAAAIABJREFU6wfFFFwxAau7f8CyZVnnxbGMZVmvW5Z19f7/9P3jlmVZ/4dlWdes7t4DT7udjIlwpaTV7VOCtCudVC4ep01erVZVOzHpgDOVlkoPvc54eG673d2kkpt68K/dbqNYLKJQKCASiaidh8g4gO3ON2Q4VO+Z7ru0tITR0VHkcjmFoI1GA8ViEa1WC5lMRsW/LctSoal2u43Tp09jamoK0WhUMRtqAYcOHVJOQimdTZqAXGuurWS8gLl9uNwnkedJ5ymZK0N17JPIoiZ6+e/du9cTKqX5EAgEUCgUcOrUKbW5arvdVj4CplZXKhV4PN02YE888UQPfsj3GIvFVMNQPQWa58ooEIFa3cmTJ/HYY48pZi+vk/ijM1GTRuD22CDox3QA9+bA/wXg/wTwl+LYHwP4B9u2/9SyrD++//3foNuC/Pj9v+fR3Yfg+aFnjp3+AHmcoC+kyXaSL4w5+9LD7vf7kcvlAGwTJBNiSFiSOejIw/AgnU2U/iSGZrOJhYUFLC8vY3JyEqdOncKbb76pEIoNLpPJJLxer/JuW1Z3f4HFxUUsLCzgySefxIkTJ/D++++jUCgo6c15MvRFyV+tVjE7O4t4PI5gMIj5+XmUSiVkMhnMzMwgkUio0BrXgs/olNmm/9erCeX7kpqZbdvKv8L8CEYAKHVJ+LymUqmolN9AIIDDhw8r04bvsV6vY2ZmBrlcDu+9955qq8751Ot1NJtNxGIxPPvsszhz5gyi0ahKLKIH3yRc5HuXzyVj/3Kd2u1uG7nPfe5zuHDhgnE99HuYhIoJp+Vx+V8/34k2+oErJmDb9huWZc1qh78A4J/d//wfAfwTukzgCwD+0u7O4GeWZaUsyxq3bXvB1Yzug4lzmbjysGNQOktHWrPZRDwe70lZtaztJqKyH55OEHS4ZbNZVUcumQ4lPKX3zZs3cezYMbV1FRGfmYITExOYmZlBvV7HhQsXsLm5iXw+j3g8josXL2J2dhZHjhyBZVlKk1lfX4fH40EikUA4HFZqbCAQwM9//nPlRIzH42i328jlcjh06NAOW1jmSOilszoi01Y25cQTZBs1vbkKGQQZAus1qNlsbGzg3XffVbstjY6O4tixY0in0ygUCjh37hyuXr2KbDaLSqWCGzdu4Pr169jc3FR5E3xXoVAIIyMjePzxx+Hz+XDhwgV4PB4cPXpUvUsyDx1kvwkAylnr9/uNrcaY1p3P5zE/P9+zC5ST9DcJLtNx03fJWPoJx36wF8dgXhD2IoD8/c+TAO6K8+buHxuKCTxIkHYWK70SiYRiArT/U6mUcgZ5PB6VRSZNEqD3ZRDRmXM+Pj6u+ttJx1cqlUK5XEatVsPVq1cBQCUn0YHEApTV1VUcP34cn/70p3H79m3UajXMzs7i4sWLWF5exvT0NB555BFkMhnMz8/j/PnzKg25WCwiGo2qEtqLFy/iySefxOrqKorFIk6ePIlXXnkF8Xi8p5ch14LJNvSHMNGIz6z7Rgi62UYJT42IobtKpdLTFZgNWblx6tWrV3Hp0iXMzc3B4/GgWq2qfQOuX7+O9fV1eL1enD59GseOHcPm5ibefPNNrK2tqfBqqVRSGtzU1BQef/xx5HI5vPHGG7h27Rq8Xi82Nzfxla98RTGnxcVFeDzdQrJGo4H19XXEYrGe9w1s10HIUmq5jmxb9tJLL+H27dtYWVlRmoMpbVjCIGbghNd79Q08kOiAbdu2ZVlDGSuWZX0ZwJeB7WIS8Zvxu5O6r6tK/RaNUleqdnz5+XxeES5jyHy5eiENr5VeZCYE3b59WyGXPP/QoUO4efOmigKEw2EltaQ/YnNzEzdu3ECtVkMikUAikcDRo0eRyWSwubmJ9fV13L59W2U5jo2N4e7du6jVasoxyUy43/iN38CZM2dw5coVFAoFTExM9PTc43NQtebzrK+vo9PpIJlMIpVKKW1I+lbks9HOp5alNylhqjLXibs4cZPVGzdu4N1338Xt27eVz8a2u30OKpUKlpaW0Ol0cOjQIRw+fFiZM+VyGV//+tfR6XSU74S+gtnZWTz++OOIRqM4f/483n77bWV6kWClT4cNSiXzljgCdKMUsVisJxVYV9ebzSamp6eVeaf7FpzUdzf4qwOF1KBr+8FemMAS1Xyru+vQ8v3j9wBMi/Om7h/rAdu2vwbgawAQDoft+8fU77rU1T+7ASmtKHH0l2ZZ3fLRqakpJJNJlVvO6yUT0CMPRHaZ2Xb69GlcuXIFtt2NM1cqFeUcW1tbQyaTUVlpzB0gQsktyOnZ3traQiwWw9raGmzbxuTkJICut7tcLmNiYgLPPvssbNvGlStXlB+BiUWlUgmVSgWTk5OYmJhQGYdkFpIBUPJeu3ZN+R2OHz+O559/HvF4XO3MxLlKQuGaSClPJsDzWZTDnApGOkqlEm7cuIH5+Xm1cSobheZyOWQyGaWpBQIBXL58GefPn8fx48fh9XrxyiuvoFgs4uzZs8hmszh58iSmp6dhWRYuXLiAH/zgB6qgiLUUfBYpyWXXJ4aGKaB05gZAtRDTJXyr1UIul8P4+LjScAbhcT8zwXRdP0ZiggfhGDTB3wH4VwD+9P7//yqO/6FlWf8ZXYdgya0/YBgiNzlKTIvE8xg+oueY0ikQCCCZTPbEjYHt2nGeI5mHzkhkMsjJkycRj8extrYGy7KUl5hEw8QVhpBk5aCs5aezz+PxqE5Ak5OTai71eh13795Fq9XCiRMn8Oijj2J+fl7lIqRSKeRyOVWslEgkFDFWq1UVPyejonZw69YtrK+vo1wuqwgK56c/uyz4kc1FZEITGQN9J9xvQWoV0WgUhw4dQqlUAgC1qzLTfW/duqX2CZDl09VqFePj48hms5iYmMCJEydU2LfT6eDcuXO4fPmy2l+A6n04HMbGxsYOJsA/KYCIR3SAyrJg+lqA3h4KXJNsNtszHoUKx9adek54rR8fBtzQlCsmYFnWN9B1AuYsy5oD8D+hS/z/xbKs3wdwG8Bv3z/9OwD+WwDXAFQB/OvdTtbJESJ/d1oYnfPSMUdGILP6UqkUZmdneyr1KEn1Vld65pskAKr4UsvweDwKKRnz9vv9OHPmDPx+P86fP6+85Mx1DwaD2NzcVB5wbmZ68uRJlUxE+71SqWB+fh4TExN47bXXUCqVVJjS6/Vienpa5RFUq1XcvHlTtd8CugQxMjKitKa1tTX1fGxMomtBwDbS87hsi8YQJR1+jMZwzUk4lLKJRAInTpxAOp3G97//feUzIbFb1naBj3TIcft2rt+hQ4cQCARQq9Vw4cIF3Lp1C51OB5lMRuFCtVpV0ZxYLKaeW+7czPtwHXgMgNKkZP6I7qDjuoyOjhp3LpLjOuG+E047mQQSJ/Xjg8BtdOBLDj99xnCuDeC/c3X3wfcd+Hu/B9VfirRTKWXpJEyn0z0VgZRg0u4Dej2yRBb+Vq1WVfKRzCxj6K7RaCik3djYwPj4uGI+9IJPTEyo7kZ3795FqVRSuQ137twBAKRSKUVsnH82m8XU1JQqHmKYMpPJIBAI4O7du7hw4QKuXr2KxcVFNBoN5cxstVqq9JY1C/SW04QiMsuMR0pR3TNtWdsbdpDwNzc3FbFRmkpTgrURL730EjKZDC5fvqyyAkOhEAKBAOLxOOLxuCoUSiaTSpOTGXqXL1/GzZs3VTIS06TZtGVtbQ3ZbFYxFCmhKd2lP4i4xOQs1iWQyUunH3Gu3W4r5gugx4yQYBJ0JmKXuGzyI+i/647bh2UOPFBw49TYjTrE6/RwF18+95a3LKunDyCwnfxCMM1PVhhyTNqe0vlIW7hWq6nYv9/vRyqVUtpEMBhEPp/H2NhYj5OwUqmoXoDhcBiRSET9ZbNZ5HI5lQvg8XiwsrKClZUVHDp0SDnefvnLX+Ktt95S3nUibyAQUMeOHj2KpaUltNtt5QNIJpMA0LNVusyUJLOQKdVUh0lMwHaBE7UCMk/JUPx+P44fP45MJoNcLoeLFy9ic3MT9XpddWKiwzYWi6l2aPl8XmUVtttt3L17V20nLusmmC1ZKpVw+PBhAL2dhvnOJRMnXlJ4yGcn89KzDrkGzMakBurWee3ECOQ5Ok7uxiFI2DdMANi5OFKt1rkdf+8Hun0lt8ziMe4oyxdFJJDagOneHF827aDWkE6nVYsq3oeMiPv2UbrTo08NpNFo4O7dboSVEocSJRwOIxaLYWxsDKlUCqlUSvW6b7fbKBQKqNfrSg1mVeT169dx5coVFIvFHuKkhJRqdyKRQCAQQDAYRCQSUdmIrHhkHJ7PRAYg14ZEJE2pSCSCSqWikpjkGutmVyaTwZNPPomRkRFUq1UsLCyoXZN8Ph9yuRxyuZyyuX0+H0qlkqqtYENWRhho9jFEHIvF8MgjjwDADjNHZj3y3fG/ZGD8nWup4yiFAZmQjjf62E5OPxOuS4blNJ5+vJ8A3VdMADCHAZ0WxclXYDqv2WwqxJBZXslkEocOHerZn56ILfPZKQUlspCx0CdA1ZmJQLTfKTEZJjty5Ajy+TwWFxdRq9UwPz+vCnzC4bDKMYjFYsrE8Hg8iMVimJ2dVaosi4HK5bJiGI1GAysrK3j++ecRiURw+/ZtvPHGG1haWlK2Ov0PVNvZqvvOnTvI5/NIpVIqj4I+DfbNY8SC19JzLx1inC8AlVBkWd2twXw+H27fvq26/eolz5J5zMzMwO/3I5/P4+2330ar1UI6ncbx48cxOzsLn8+HSqWC5eVlRaTNZhPpdBqtVgtra2uoVCqqj4BldROsxsbGMDU11YMz1GQkcyLw2bjhSjQa7dH0ZAUptY92u626NemaqAm3nex6E10MYhjDwr5iAoMku76QgxyH8jwyAd2xFQqFkEgkesI8dProqcIyIUa+EHqOmXE2MzOjGl7Qi0xgQ4xSqYTLly8rbYNpw8z4oxSr1Wool8vY3NzEiRMn4PF4sLy8rNTzWq2mIhHHjx8H0N2N+PDhw2i1Wrhy5QqWl5eVlkNGRmnGQioi/9TUlDIrqClQBadzkevBLERdEvJ5SSwAVCfkTqeDe/fuwbIsRCIRlakpiRCAyg0YHx9HrVZTplM0Gu2pN6jX6yiXy5ifn1ea0JkzZ2DbtnJ+0nS6c+cOQqEQZmZmVJEWm63oFaESj6TPQGoBJEwZMZDEzUI17lStaxfEaTeEr2sN/Rzp8no3sC+YgBtbyS30YwT6Anq9XmVjU5UkV5dEA/SqjXzxRF4mmrA92OTkJE6fPo1isYhbt26pTsR0fvn9fly9elVpEJOTk8jlckilUj2aC1uIBQIBTE9P4/HHH8f6+jree+89eL1eTExMwOv1Ym1tTdUG1Ot1/OZv/qaa061bt5RnHthO5Y1Go8qJyZj4s88+q34nwVarVRQKBSVJ+ew8Tw/3yfcpezEwZbnT6fZbWFxcRCaTQTgc7snFoOe/Vqvhz/7sz/C7v/u7OHToEADg5s2bKJfLaoOYWq2mIiRsM9ZqtXDu3Dn4/X7MzMyg1WphaWkJKysryGazqNfreOSRRxRRT09PY35+Xml9uinA56JNH4/He/wBumAgg6WZEolEUCgUjFqt02cJUvV34wjnesv/1JKcYF8wAcCsDplUfnmek3pkYgTSSUWJJRNWZPcX/d58ofJ+sgIwEAhgaWkJ09PTilnE43GkUilEIhGl4tJjz4QYIhHTjG/evKnuR697q9VSJchnz55VIbcTJ07g6NGjCsnef/99nD9/HlNTUyolmL4BEq50aLG1uMfTLWceHx/vQVy5uSbzFKTElx50U42B6d1SY7p37x6q1Sry+TxGRkbUM/PcxcVFtFotfO5zn1Oa0dbWFubn51EsFlW6NJOuqIWRaZK5FQoFlX1If0I6nVbNRB599FFYltXjRCRDkzkQeuaorJngGuqNaPksstBIX5thNd+HBfuGCZB4ZcKO6XenY/0WmE4huRW1ZVkqVixVUdqnRADafrrtqjMaqslEIkp9erTlLkPc4oztwUqlkpqPZVmqBp5S2rZtzM3NKYQ6ffo0XnrpJUxPTyunH5H61KlT2NzcRKfTwfe+9z3lpCQDYp4+220vLS1hcnIS+Xxe2dPsYkym2Wq1UCqVVEYfVX6aNmzqYfJa8ziJamNjQ+3PsLKyonokUCOg4/D73/8+XnvtNXg8HoyMjGB8fBzNZhPvvvsuKpUKLl++jFwup0yjTqeDRx99FEtLS1hbW0On01ESmH0GU6kUXnzxRQDAyZMn1RypyktNR0p2hnAlcUuzwbIsZVbIpCn57LpwcmIAujNPntfP97UX2DdNRfSHlVJXd8oNA/QMsxW1XFj+pt+H3m9KY8ndpV0NbL8sWXFIqZHNZpHJZNSmn16vV0UHZIdgjkP7W4ao2HuAobpGo4Gnn35ahdB+/vOfq70Hrl69qlJUz549i7m5ObUFN6U6M+parRa++MUvYnZ2Vm1fHovFlFkiMwR9Ph+y2SxOnTqFmZmZnm5LdJbJ/HVKVZP0W1pawvLysqrvZ3jz2rVr+MlPfqKYZ7VaxU9/+lOsra1haWlJMbp0Og2fz4fV1VX80z/9E95//3088sgjiEQieP/991VFaKPRUM1DPB4PJiYm8MILL2BkZERVYQLomaNuLpKApTNU7t8gtQE+M4/z/en+AMk8+hG0ye7nNfKP55qu0c9zgn2nCbhxkgDOZZYS5DESNz22tLVlyayOEPzM/H5Kas6FxM6XvLm5iXg8jmq1ilarhXA4jHQ6jVKphHa7rVJxdcJnBhqZEr3cnU5HeeSZMMNinmKxiGvXrqlCIwCq602lUkGpVFLJNcxSJIIyYefy5cuoVqs9rciSyWSP6SPX0efzYXx8HB6PB9euXYNlWSriwSxF0/ukj6PRaChTIBKJoNlsYmVlBcvLy/j2t7+Nra0tPPnkk9ja2sKnPvUpVCoV/OQnP1Hh20KhgFqthmg0iomJCYyNjeGdd95BtVpV/QO5KxR7BshSYJoCUs1mSjGfTydWZmcyYsRQqR5FkM5NXsuaDGlqSJw14a9bTUH/3eSTkdCPEewbJgCY7Xw3zpN+4zEcxvAY89LpD5DmBwmaRCk5PwtF6Nij3cgXb9vdlFuG+JrNpqryK5VKWFlZUbn7ehoum4rSFGAZLNeA7cRzuZzaY+9nP/sZLl++rBKcbNtWWYmsVHzllVcwOjqK27dvY2lpSRFTq9VCJBLBysoKtra2MDo62mP/AlAtwGgS1et1RUCjo6Mol8s97b5pctHul3UD7XYbtVoNf/M3f4MLFy4gl8thdHQUFy9exJtvvomxsTGcOXMGly5dwvz8PP7xH/8R09PTmJubw+bm5o6sxFqtpnIeWq0Wvvvd76p2YSQ6mjRAd6ehY8eOqdwKiUfSTJOhTr/fr/o88vxyuax6N0iiI2OVuQ5+v1+FiIlbTp59Ha+dzCpd++RnJ4YwiKEQ9g0TIOEBzg8uVTj526CHJqHwWv0lmtR7jiPzyOU95IsicTcaDeXUopON7aoks6AzjogDoKdjMKU2K/t47cTEBBKJBBYWFnDv3j1ln3NtwuGw2kgzHA7jscceQyQSQbvdxg9/+EMUCgVly7N4CICS4jIPnsxQJkvxO9D1qlPD4bmSeZEZUJVmpKTT6SCVSqFYLGJubg7VahVjY2N49tlnkcvlUCgUMD8/j3A4rOL/XCO2DKNjT+YtEHc8nm7vgkKhgHA4jFwuh3w+r8wcr7e7SxPNK1n8xednKjKLzmhisJBKRgQ4BtdJOkErlUqPf8uEyyZ8NTkYdwO8Tla5mmDfMAHAbBKYTIRBnJTHdPufKhyzuOSutPI+VP/kC5UqJImcSCOLaWjryjASNw2hY5KIR0ZAjYWaBcfk3LizTTwex+LiIpaXl1Eul9WcGO/nfoJ3797Fk08+iWw2C5/Ph8OHD+PcuXMoFotKw6FTLxKJqHg+248ToUlYXDeq9UxcikQi2NjYQLlcVj4P6UdglAMA3nzzTQBAOp1W0ZROp4OTJ0/iueeeQz6fR6lUwrvvvqvadXNbdtmrgDX/3PqtWq2q2gT2E6QkZwg4m80qDc62bWXWFYtFxWTIuKLRKGKxGDweD4rFImq1mlojblCq4ybnJvHS7/er/AwdBqn6g/B8EFNwMjGcYF8xAcBd+eSw9g9tbR6XGXzSVuNnxsDJSKgm8hoAKvmIjkDTdVIq0qYk0knCInFx7nQ8UToHg0GMjIyovPi1tTVFdHQm0rRhkRK97vTss2SZmobP50O1WlU7HDPxhwxSxv/5jNJRCnQLmdhKnX4TaSMDUDkFN2/eVOvEsZi4w8Sm5eVlzM/PIxgMolKpYGxsDJVKRREvOxWzliKXy/Wsn0xnloyRvhH+xvJuGWLkO2L2pGzxns1mUSqVlGYmiYsMQNYG8J0vLi4a1XUTuGEM/XxmJuhnMkjYV0xg0GRNapSTliCJV6bLSvVVqreSeCmdJSFTOtPmJMHTsaf3HJC+Bna9JSNgL8D19XV1rkxNllKYzAUAisWiQkY+HzUNzm9lZUVlyNl2d0uuCxcuoFAo9GS9yfbbZDaMGshnlwlBQJeoGcLMZDJYW1tTLcJ1ByvNmNu3b6tqvkqloioV2+02VldXcfHiRQDA3Nycum5ra0up/lKa6u+D0p95+tRC2Ckqk8koRiC95ZxPq9VSzIKRBPYqkBmPrP7UQQoTPenpzp07PYzBrVrf7zyTtiBNB3mOW4axr5iA24WSZsMgDkr1l4grXwolC7CdIyBVc93jSwZBgmXYMRAI9DgTKQl4HhOHVldXEY1GlUpMdZTMinMjYct2Yevr6z297CjNLasbpmMv/IWFBRw9elRttjk3N4cbN24ox55Me2XlIlV3MjSuBcOKXAMWG5GZMN2Ym6DwfXAshvJ++ctfKu2COf3st3j27FlcvXpVee4ZsqUXPhwOKw8+/TNc71u3bim/C/NAfL7uPgwTExM4duyY6u7Dd0jta3NzUzlKo9EoMpkMvF4vyuWy2qyEGYYLCwvq+fi++J04JM03FjRdvXp1h//KjWZrwul+wk7ipwQ3WYbAPmICJttcB/mbkwdVByKeiWnQmcXzeC4ZASWsHvrhOLSrJeHL3nsyHz2TySCVSqlGp5SgUt2W+wYQ4XUvu8xUlOOzkpApxh6PB7dv38aPf/xjhcQy8YmZjPSgcz1JJLSp9dx26SjlMzKuT5Wcqdi1Wg2XL19WXXxo5zNSQYaxubkJoNtcRGpn1WoVa2trPfF4Mr5KpaKyGNkAJhKJIJ1OI5/P49ixYz1FUGRufLfFYlHNIZFIqIQhagVANwoh6y6kBgL0agESP4LBIK5du4bl5eUevHVry5uS5ZzOlWAa341Q3TdMwMlzOowNpI/HxWcFnByThMU0XCIIABXSktEKvlyqkDI8KEtWpbpKQvX7/ZiYmECtVlMpuMViEaFQSNW6U8r4fD5Eo1HFkDqdbgcd2pws85VaSSQSUSYGd0G6evUqbt68qaIVfC5ZniwlIEEvBtKZ7v/f3rfGxnmdZz6H4p3DO4fUDEmJlChRkmVLtiTbdWJbdoJ1EnvrzSbtui16S5s2QIrFLrZYNNsfW2RbYIHdosBiswVawEgbbJMUadMt7DiWL7Ut27ESO7rLNMX7nRwO77R4E8/+GD6H7xyd883wIpOU5gUIznzzXc453znv+7zXQ7VK5iKwj2wPI/+01mhvbzfhvKypyMXGcwGYugHUs7lQs7KyTIIXpS5tIbR5xONx1NTUGObU2NhoEpsWFxcxOjqKwsJCVFdX4+bNm+jv708K+pGoUetE0lYsFjNMhrYS2lyo6hE1cI4Q0RUVFeGdd94xKsRaEW46c9s2fPvul8762TZMAFidUDat1xqqdaKkGOGiPaEl9Lej9+jyoyWZ0pYW9JmZmaRnLiwsGJcSiZOVz66qqkJ5eTlmZ2eNT1/rRFgrLd/5+fkoLS3F7Oxskk7JQh9EDGRGlMylpaUm9fbs2bMYGRlBdXW1kc6yTTQ+sqAJi7DK8eaE5nuhykGDmxx7WubJWOWYTU9Pm+pFtj5O5sHxo+eBtQWpjlVVVaG/v98wQWB1A5O8vDwUFxcjFouhrKwMBw4cQCQSSYobYKDX9PS0yRPgjkfFxcWYmppCSUkJxsfHMTIyYkLA+c45HhQEzAORrmfOg5KSEiwsLODs2bNpzVGXTu8jl0qRChG7vBM2pWQCSqnnATwDYERrfXTl2P8A8K8BLABoB/DbWusJpVQDgA8BfLRy+Xta66+lbIX/2c5j0iYgyfWdUp1FPCRM5+9cTJJjc6ETkku4XFhYaCYKiVJQTmqpG9M2sHv3bvT19ZngIEa4Ma5+ZmbGPJc6OBcD0YHcoVhKserqaoyMjGB8fNz8xvuwn9KbUFlZabwCdC+yjyRKa0pdVywHLf1y7z0uekYGss2ylp80oHLssrOzjRGT72dychJVVVUmht9+H8yPKCgowJEjR0yoNc9l7gbrGdCWwz4Qnc3NzZkNXcjAqZrJUGFGIkpjMtWsvLw8XLlyxQRR2ajWJb19yDfI3pWO+pwuik4nd+DbAD5nHXsFwFGt9X0AWgF8Q/zWrrU+vvK3ZgYgB1XGn0tuycGRcfxywHktidLQLvJAg5isdCPvu7S0hKmpKeMt4ERgeC2lKp8li4jIyDsgYUmntK6trUU4HEZlZSUOHTqEaDSKmzdvmjLjSimTJltWVmbq88tgHronaZdgSO3Vq1dNUZK5uTlTYz8nJwehUMjsI1BRUYE9e/agvLwcRUVFRqfn+PKPKc3MerQndnl5OQoKCjA5OWnsKDIDLxQKYXp62qANZvJxG3iWW6OLjkyXRjqtNcbHx5O8MhxTtqesrAy1tbV4+umnUV5ebiopE2nQ7QnAICmOMysgy1LoREd0s5JhMHTZNjIDq8Khp6cHzz//vDkucypcagHneKq14Pouj/l+T4dSIgHt2IJMa31GfH0PwJfmbrfzAAAgAElEQVTX9NTg5wHwW0xdUEieK7mjRAwMjKE0ZQCPWrHyl5eXJ4W9UkrNz89DqUQBDCkFubhk1J7W2sT528ypqKjIhNuWlZUZ/TwnJwfj4+PG78+MOlquy8rKzC42wOqkkshgfn7euA7J8KgPh8NhE1N/48YNKKVMYg1tJVR/4vE4qqurDTOkS29sbAxAwtVZV1fnfG/Ly8vGjkIJmZOTgwceeADxeNzEJdCVSuYuq/JQPZLGzvLycrO7swxNpl2kqKgIJSUlOHz4sFEBKJUlouH7lLYAbhBrS2gyAds+wmQgqj3yfMYevPfee+js7ExCOXIeulRWe467EASP29ela28Ios3IIvwKgJfE90al1Hml1JtKqUfXejMXpHf9uRCCJKnH8T6Eg3w5RAGjo6NGIvBaXsOQ4Onp6SQJwAlIe4OUnIS+sn10XR08eBCRSMQgCe7CS/gZCoUQDoeNdZ5uROqj1Mu5YChhmV23e/duowqEQiHk5uaipqbGGA5ZSYjMYW5uDlNTU8YSTyYTj8dN6jD7Nzc3h66urqRx5hbjXHCs9kMV7NChQ2hoaDChvUVFRcbGAKyWHaeNgBGXRGp0E1LN4oIrKipCWVkZotEojh07lpRlaRc+5fixLgENm4wxIFHtYY1CzoWFhQWMjo4aA7B0o0qm2tPTgxdffNGoIfZcsoUT/2xE61sPElXY58j1wHPTsQcAGzQMKqX+GMASgP+7cmgQwB6tdVwpdQLAPyml7tFaTzmu9W5DFoQG7AHwcUx5Ls+R2WKUCCxNderUKRQUFJhy1pKLU7KRCcj0Y+bBMzGJUlWiCUbUcfI2Nzcb3ZK2AqWUkZJLS0tGarO9NAyGQqGk0N6srCyUlpaisrLS7HvAGHnel3CXDIPBRJTAwK0FLCYnJ3Hjxg0jfQmHp6enzU5KsjqzVI0Iv2lMPHHiBK5fv54E5xnoQ48Lr5VxDITwzIQkM2V7I5GI2XAEuDVHxLYfsW3MQaBXxrYxSLWQ74keFnsu5eXlIS8vD2NjYzh37pzJz7DnpAvCpyvBbQThQsOybWuldTMBpdRvIWEw/IxeaYnWeh7A/MrnD5RS7QAOAnjfvl6Lbcjy8/O9Sozkkq64bcd9bxkwOeh0yXGyTUxM4MqVK/jUpz5lyo8vLy+bBS991MBqRh8hP3VBpZQpKkFGQGY0PT2NsbEx1NXVGQl94MABZGVlYWBgwMBSluBiuqzUw4FE+GplZSVKSkoQi8WMN6GyshK5ubkYGRnB0tISqqurDZymdGV/QqGQySmQ0icvL88UGuFi48KjVOTE53cZp8BrOMZkZHl5eairq8PJkydNWTQyIibl0MpPZFNUVISRkRGjn7NsGBcnF/K+ffsQjUaNCsT6g4xIlJKXDICqkiwUKtOrObeAVXsFVUL5G/tbWFiIeDyO119/He+++27Sc3mePTddkYdUkWwmI+dxurTpNgEXKaU+B+A/A3hca/2xOB4GMKa1vqmU2gfgAICO9TyDHFnCKpeu5IJGrnM4EWQkoFKJoJPW1la89tprOHnyJKLRqDHCcRJIRECrM18aI+4YYCJ95fK54+PjKC4uNrX66J/Pzc017i/aKGhRp4WZFv3y8nKUlpYaCE5ozxBgWrZpdKPeOjc3B621QQnMaqRthAuINQ4BGPWCkX18D9XV1SZCkIZVYFXySlRH2J+fn49Tp06hp6cHU1NTJkQXgJGy7DurC9NHv2vXLlOfgUyGBsdoNGpCnUtLS41qNjw8bJgw3zntBDICUjIB/s4FvLS0ZMqSueYd0YTWiT0g33zzTVNAVd7btgXIeSwZhctGkC650PBaKB0XoWsLsm8AyAPwykoD6Ap8DMA3lVKLAJYBfE1rPbbmViE5OixosEQ7vUYTGT4pdSVKh/HxcfzoRz/CwMAAjh49img0mlR+iu4+mXZKqS91Ota2k3HpbM+NGzcwPDxsdFcAxltAhsEddFiMhOoBUUtWVpZx/+Xn5ycFFQEwW3CRYcmIx4qKCrNTEYNilpeXzYLkwmb/qHpMTEwgHo+bBVRTUwNJ3BmYXhEybroLaXisrKzEfffdh+7uboyOjhqExNp/ZLAca6IY3pPMhTkBDAhi6HJVVZXpl/SicIGzTTLkV84pqc4wKInuWdsVKAO6Wltbce7cOQwMDCQtdqliuGC6vJ89n32qgg/92udwbqdLaj2cY7MpPz9f7927N+kY3UWS0hkAObBBOhJfOJkNXUaVlZU4cuQIvvKVr2D37t1GwsoyWlJiUJeXYbaczLTwS7jc3Nxs9sBjG7k1N7cdI/xlPoJ0Y9LtR+lJewSh8OjoqNlDkDaA3Nxco0bIMWWILS3kUvVKl27eTOy2TMMZITcXHMetuLgYubm5iMViuHDhgqkFODMzYxgVjaSlpaUmHFuqI4WFhaivr8fRo0fR3NxsypBzo1FgtVQ5mTTfl3w/Mr6C/Wb8BSMb5XjYxUIKCwuxsLCAvr4+vPjii3j77bdvyTCUjIPPsfX3VDDfhSrkf/tc+SwXE2htbf1Aa33ylmt3AhPwGUVsSvWba/CkO4i6ZVlZGR566CF89rOfRSQSMYEydLNR8sq8BE4YhvVy8ZIZ0MiVk5ODe+6555a+AYn6ewz1pW9ea238/nl5eWZXY0o0Rg+yliBdoexTXV2dSY/l+HFfP24AAiQCZrhpBxeATA2W40gJSxdjTk4OZmdnMT4+bvYctEOsl5aWknZLGh4exqVLl0x5NLZ9fn4eoVDIbBO+vJyorxiJRHDixAns378fZWVlqKysNHYEGdnY29trFjIZgEzjBpJ3EKYhlinRNpKQ75bemcXFRZw5cwZvvPEGOjo6jHtXvk+XJV+6C20KYrw2AvAxA9k/11rYcUxAFgH1kavt9oC5bAb2+ZzwLPJBI1l9fT0efPBB3HvvvSgvL4fWq5VyZOirrWbwXqxhSEMjGUYoFML+/fuTFhnbNTs7i/7+fvT19SEWi2FhYSGpsAYRABcidXFmLZIB5eXlIRqNmk07uaiYZFNSUmJsB/L5HR0duHnzpqnOS91dwldK28bGRjN+SiUCnBivkJ2d2B2IuyMVFBRgdnYWeXl5BkozRr+rqws9PT2mOIks2lJaWopHH33UwH+2nW1hOjQA9PT0GE8MbT8UJjRYsp8yT4DQX+r+UrWh4ZZRnefPn8cLL7yAvr6+JE+QlL4u95xLiKWDumyd32VYtBHHHcEEaHyRceou8tkG+GJcTMA+h9fx//LyMoqLi81OvdXV1bj//vtx6tQp1NbWGss31QQuSHuy8Z67du0yUX8M7AGABx54IEm1kH1guOzExAR6enpM1Byt6YSphN9M6S0tLUU4HDZMwZbsdCfS1sDxlRWAuQjD4bDJMlxeXja1EhnkROlXX19v8gUYwcgsvRs3bmB6etogB5lfIVOVOQ6My5C5CyyqQsnP90iGCCSiNfv6+pKSu+Q95TtmLAfDh/m7XMTZ2dmmFsHHH3+M7u5uXLlyBS+//LKpOCRrUdgxKfK/bViUx9gW2zvgI5/nQTIv3318TGBbJRClIjmoNqxyDXIQ1/WhCEKpyclJY8memJjAwMAAYrEYHn/8cUSjURQVFZkJwuKgnLycmDI6jRlnoVDIbIPe09OD+vp6cz4nJfV75hTU1dUhGo0atDExMWEyC6mjyoAa9pPxBbK/XDz0KgDA1NSUgeQSWQAwdg1uhtrT02PiDjiRh4aGUF1dbTwkhM7yfkQidH+yTgIZlUwwInTPysoyblEZxWgnH01PT6O3t9fcwzVHpGrGd0wGQLsMAHNf1l3s7OzE66+/jkuXLmFoaMigBjJfufDtBSgFje0dYDupPs7OzjrnoySXhHcxgrXStkYChNT8bpMcTN9nORF8RhrX/ewgGranrKwMjY2NeOKJJ3D06FGEw2ED95lYItUCLjqlVt1nlMBkOvX19UlVbe32ctHTiMb72JKGf2Q+cmFTOjLtmFCfW5HLpCRKea0TOylVVlZicXERQ0NDt7g+pbSlFZ/Mh2nDXGhyAw+5RyPvJxdXdXU1du/efQtSs9/b3NwcWlpazL2ILBirIDealVCaYy+RAwuMhkIhxGIxvPLKK2hpaUFvb6/x3PDdkrm72uTS2V2fJUqhuiczGF1qretZNiPyrekdhwQkrLbr/fs6KQfadgXxd04I34sKMt4wfHR6ehoDAwPYv38/nn76aTQ1NZlSVExOsZEBYR9hPG0Pubm5GB0dRVVVlZGksj1aJ1KNOQFp9KJXg1CUklLGKMg8CAZIVVRUIBwOQ2ttYK2U6rYbzWdfIWOTBjGGDHMhEkVQt5auQE56ubAo2YuKigwD8CG8xcVFkzFJxsh2y/gOILk6tYT/tPoT2XC3qFgshp///Oe4du2a2ceQ1n66DYMksE8iuxgB2yPzJeyNTWWciov4WxADCKJtiwRIrFQDJOv76bTbfhlykHwvymXV5QDLNnDicduu06dP45FHHkkqZU0jovRV86Xyvlwc3HsgFAqZ7DUX1GScvzQ8ycg/qVvKQBmpY/M4709vAyWxjMwDVusiUHclYmBYsGynhLs8X/ab1/NcqUsTqZSWlib1nzQxMZGEtshkyHCpo9sk37lk8oTzNFZmZ2fj8uXLeOedd9De3o6BgQHjtuR9XC5Un5S3VQEX2YJLKWVcrS7bgus/n0N1yPe8HYcESIzQS0U2JHYNhMsQmO7vNhfWOlGYghVoRkZG8NOf/hSRSATHjx9HQ0ODgdyyKo60QnNxyDDb2dlZU6RDxhwwak/W+KMkphSkdOV9pYSRfmu5CCVj5T25yGRKsNy5RyIPl/FL6vZy/Pi7bJetutFwSrQkmb7sO5OBuFikPUDOCf7nbzLxJz8/36Q3j46O4vLly2ZTF3o12F6bkdrBQC5VzqcOuL5TNaFNggZnObauOSntGkEoOYh2BBMgBAOCYwHkQPGFycVrD6Rr0OyB9sE+ee3S0hKGhoYwNjZmjGd79+7FoUOH0NTUhPLycpP/zww7iQY4yakysKy2NJ5xQkrXFSeBtGrbbaSXQCIc14TifXi+PI+Tk2qZvA8XINvEY/Yit8fZ5+aid4FVgeywX2BV5SFTkP2RaEguIsmE6H6mp6K3txfvvPMOLly4gN7eXhPrwHa6hIscB/kefWqmb/5w3DlGSilUVVWZ7E6iKfs+EklxTIJQQBBteyYAJMNSSfbEkpPNNRFdATA2F/cZDOXvroGmlJ6YmMC1a9fQ09ODtrY2s+NvbW0tqqurEY1GTVYcJ7MMR+ZCkC4oLkRKOyn95SKl/cQlQYKYnTTSsS8ca5bxApDkFuT7oERmfIJrbOwxti3oRDtaJycucYykoRXALaoQSaIGyVwJ/WXBkoWFBVy+fBltbW3o6+tDS0sLRkdHkxK/bOYhx82FgORvNpOzGSL/JKqT426/B7uPwOq6kHNoPbQjmACAW1wygDu92B5kXguswicpSYHkxW0Ppg1zXZOA9yYtLi5ibGwM8Xgc165dQ2FhISKRCBoaGnDq1Ck0NzejuLjYMA65H55ss8w9oI2BE0wawijxeQ8uVgl9ZR9lP2yJKSUbny03aWGOgS0dJdN1MVr73QDJEl1uxy4rEUuUY6MyLlA5drINMuGI47W4uIjBwUH09vbi7bffRktLS1KpODta0B47F/nmpAtxyfdFY60sqjI/P4/x8fGkLerkvaVNw7YZrJd2DBMAkq289mLkZ3uiUO+jZJDFPuQ19mJ2oQwXBORz5H95X60TMQLXr19HT08Purq68NBDD6GhoQGRSMQEETE0WUp7aTOQsHR5eTlJYslna61Nn2WqL9vrmjx2HT0bTXHcZmdnjeVfli9njQNpebfHS/7xXBr25OKV3+2xlpJUQmESGZ/MQKVNaXx8HFNTUxgdHcWFCxfQ0dFhKjHbaqNNrvfr+02OHVUohhvLRDAiEyIoxkv09fWht7fXhJnbz5Hzi8fvKiYgSzqRggYgNzcXR48eRUNDg9mFiPH/U1NTZqcZWvCpe9uLALgV6tlIwOb8csLKhdTZ2Yn29nbk5OSgoaEBzc3NOHnyJOrq6swuuPRvy0CfxcXFW3ZSsuEq4xWAVSnL8Fgf3JREewCv5Yaf7AMDlZjBKMutyWIcbIucoHZyFYOdbM+BZEa2RJWSn4tGSn25C9T8/DympqbQ29uL9vZ2dHd3o7u722w9xutc1n57fvlUKtc85H0ZbRqJRFBTU4P6+nrjOWJ4NWM/uKXd4cOHMTIygu985zu3hDEHIRKJltZDO4oJAMl2AVvXBJINXHv37sXXv/51dHd3Ix6PIytrNQ5c1gwYGBhAR0eHyWyjNZ+FJlwqhnS3ceICbg8D28zzKPE6OjrQ3d2NixcvorGxEffeey8OHTqEqqqqW2IjGAAThFgIw2XBTGmwCpJoUk2ip0Ja4/lMMtL5+XlToShId5b31zoRq08GAOAWtxb7KrMveW87LViOL987s/s+/PBDdHd3Y2BgIAlekxG6kJ4L7cnPNtK0hQQXf0VFBU6dOoWmpibk5uZibGwM7e3t6O3tNcVkZdVl0rlz5/DNb34TP/zhD5M2tQlCpXaI+npoxzEB+yXY0owTpqSkBF/84hfxp3/6pyZ/nSR1PRYECYfD2L9/P770pS+hvLwcXV1duHz5MkZHRzE2NobJycmkirxS75UGGteL40tyWbKBRPbg8PAwzp07h7y8PFRUVKC+vh73338/Ghsbk9J9iQqIBuSW5lw4DEGVtQWkPcVGL/bklmMr1Sj2m5mArMYscwBsWwv1ffq+bfuMbI8dhENi/6g2ceyXlpYwPT2NtrY2tLe349q1a4jFYkkFQW0Jbd+X/20m4JKuNrNimyorK9Hc3Iz77rsPdXV16O7uxhtvvIG33nrL1CWUc8++J49PT0/jo48+SopGtFU4l9qxUdpxTIDcPBWMy8rKQnFxsYkoc53LBcUdaK9fv453330XZWVlOHHiBJ588klorXHlyhX09vaanYOY7Wbrrfwv9XBgFbG4Is1siDc3N4eBgQEMDg7i4sWLKC4uRnNzMxobG3Ho0CGUlZUZnZ/jQWkqt1sHkDQBXdCXn2U1IEp1yQQk3JcLVhZetTfioLSlOsFx4X3JECQsl/0hseCJRAMTExPo7OzElStX0NPTY9KHZa1C+51LJCfHQdpZ5Du0f7fnz65diTJpR44cwSOPPILq6mpcunQJ//iP/2hcjL6F6/tOBiuf51rknDN23cP10o5jAkAyFHe9IPmdvnkfZLIHmQFAZ86cwZkzZ1BQUIBf+IVfwC/+4i9Ca42Wlha0t7djZmYG09PTpoCF1GnZBlvCSunkQwuyP6xvF4/H8fbbbxsYXFlZidraWqNv1tTUIBQKmcIdcqJTf5aLjovQbptst+2Tp2VdLhobktqS3tVH3pPfpXuLC4sl2KiutbS0oKurC21tbejv7zeMWNZykO9WohHbUOdKqJI2B/s3W5UqKChAfX09vvCFLyASieDKlSt48cUXTV6FLQDs8XXNPz6voKAAlZWVKRd3ELJYD237sGEfSenhuSd+6Zd+CYODg/jZz34GwG/Ft8mW0DQ8sRzYfffdh8bGRrS0tOC9997DwsKCKcOVro4mJa7P4GOTDVkBGIMYjXX5+fmoqKhAeXk5wuEwqqurUVpaaiLj5Lj5pD6PuVx90r4hpatkANIYyu9csLyvDOaiukJE0d7ejpaWFly+fNmEClMFku5AG1mlY/uw3zdtIPzsOicvLw8HDx7Egw8+iL1796K1tRUffPCB2VTWtp24nufS7W217PTp0ygrK8P3v/99o9L5VLf1xAWsO2xYubch+xMAXwUQWzntv2itf7Ty2zcA/A6AmwD+vdb65TW3Ng1ywS1+poHoo48+wmc+8xlcvXrVQNcg8k0aGfY7OTmJvr4+NDU1obm5GV/+8pfR0dGB8+fPo7i4GDMzM5iYmEhKYPE9R0oNyXhcSEH2Wd6XOjefm52djf7+flMHj3Ca23TJzyUlJWZHXtYVoEGSxUEKCwsRCoWSGIJLpXAFx2itjStR7l7k6jNR1blz53Dp0iVMTk4aiC8nvL3o5Xc7OtRmmlIdkAxKeih4bVZWltH3H3/8cRw8eBDd3d34wQ9+gFgsZmokBM0b+Vn2VbaFx6qqqvDwww/je9/7XlIpd3muSwhsBqWjDnwbwP8G8LfW8b/QWv9PeUApdQTAcwDuARAF8KpS6qDWev3hTB6yQ2UlnAMSkLO9vR2/+7u/i0cffRSvvvpqUtKMvNanv9mGoOXlZVMwdGpqCq2trWZTj1OnTqGyshLd3d1oa2vD5OQkpqenzaYlronskjwuqCfb5HJZyj6TASqVSEShNZzGO2nEY/iszM2Xm3wopUxSE5kEpTgRAe8l05ulKsL/st10czJPYmpqKmkDlNnZ2VuMrS6G7xoH+3x7jFxjxvuwZmFdXR3279+PSCSCWCyG69ev4+LFi5iZmcHY2NgtdQt897bbyM82Y8rJycHTTz+NxcVFtLe3J9VW4DmyTxuJDnTRurYhC6BnAXxPJ/Yf6FRKtQF4EMBP1t3CNIiDZBt36Cc+ffo03nrrLWMtd3HSIEOMfc7NmzeNj3dkZMQkCTU1NaGiogKPPfaYqQg0MDCAyclJUxnIFesg752OGhH0XZL0pcvzXUxTwnwASWnKZBZS/6Z+LZmCPXbyfcg/BkWxyIcs4mpHxLlsPr53FPRufYuWDKympgZ79uzBnj17TKHT69evY3h4GH19fbdsQOtSKW3obj/f1b7s7GxEIhEcOnQIP/7xj41nwyUMXLaczUAFGzEM/oFS6jeQ2FjkP2mtxwHUIrE3Ialv5dhtIfq2AbfLZHFxEa+88gp+9Vd/FQ8//DA++OADs8mljQDSIfvFk/HMzs6a2PNoNIqqqioolagleOzYMWRlZSEWi5mIQRoqfUYkuQDsNrr6KcmeaLb6YD/Hl9ZrnyeDgXguPTXyWb6FaDMEG8nZbZf/XSqAS13yjYXNvBicU19fj8bGRuzatcvs+DQxMYFYLIZYLIb+/v6kTEXJkFzP9jEa+3z+npubi3379uGhhx7CBx98gPfeey/J+GqrEWudr+nSepnAXwL4bwD0yv8/R2JPwrRJBWxDli4FGV2AhL78/vvvo7a2Fk899RRu3LiBixcvJlW4SaVj2dCNz5PfgYTUHRkZQSwWw65du1BQUIBoNIqGhgaEw2FUVFQYS35XVxfGx8eTYK9Lh3RNeFe77d9s6WlLD57LxSifY8NVSZJJyLRl+x3YKIPjIyWa3X7fYrf7Zy8mF4OUz6G9g8ZTBouFw2HU1taipKQEs7OzuH79ukFtshqRC6GlUjV8jFS2l5usPPHEE6ipqcGf/dmfmb0LbIOsPSZbYRNwNWaYn5VSfw3ghZWv/QDqxal1K8dc90hrG7Igck1Ye2LcuHEDb775Jp588kl86lOfwvT0NHp6ekzwShC8TkfSEKLJtlBdaG1tRUdHB0pLS9HU1IQ9e/bg6NGjyM/PRzweN1F5MzMzZqsw29LsWih2G12MIkga2/0lopESWur1rsknGYK08vOYdJf5oLH9vnwMz5Uz4usHY/L5n4VVWe48KysLJSUlqKqqQjwexxtvvIFYLJY0H+Rz7Pa6JHo6C1MikpKSEtTV1eHgwYOoqqrC2bNnze5FLo9XKvS3UUrLRbhiE3hBr3oHIlrrwZXP/xHAQ1rr55RS9wD4OyTsAFEArwE4oFMYBtfjIpRkB3+QtF51pdx777346le/ioWFBbz77ru4du0aJiYmnFlZvI/ru096uZiJ/bvWiTqFkUgEBw4cwLFjx5CTk2OSRljMlMFIhOpSitr+fH6WiEKGJ9vtl32S4c42XLcTgagO2JDd1Ude53pP8jd7odnQN0hlk9dS4jNePxwOo6SkBEVFRaiqqkJNTQ1ycnIwNDSEtrY2kznoqhO4FiFgX5dqPnAruePHj+Pxxx9HVlYWvvWtb+HnP/95EqryPWejOQLrLjmuxDZkAIaR2IbsNIDjSKgDXQB+XzCFP0ZCNVgC8B+01i/dclOLNsoEANyiz/KFyMl9/Phx/OEf/iGWl5fx2muv4cKFC8YV5YLFtgQK0vn4e9CLZDtIJSUlCIfDOHbsGPbt24e8vDwMDAzg2rVr6Ovrw9jYmDGcaa2TIuKCpKn9LJtBuiaUlIKckLbkl9Df1Se7TUHkSsLiZ6Ipe8xlPyjpCwoKTH1GRlI2NDRg7969yM7ORk9PD1paWtDT04ORkZEkpi/b7+uTa3yD3r+rP8wpaGpqwokTJ3DgwAHMzMzgxRdfxEsvvWTGg+fbrmPpXdkI7bh9B9ZKEsa5XgZffk1NDZ566ik8++yzKC8vx09+8hO88MIL6OnpSapUAyS78oIWl002I5DBLUGUnZ2NkpIS1NbWYt++fbj//vsRCoWM2/HSpUsYGxszG2Xaksx2q8m2yjHiOfKYK3DJVgPsicrPMpTbvreNnnzjZp/DZ8u8++LiYhQXF5vYhfLycoOqCgsL0dnZidbWVly9ehUjIyNJNQ5974oMz26bzdB4ni8/xCWAdu3ahfLycjQ3N+NLX/oSKioqcPXqVbz00kt45513TNk8Vxiz/V43Y53e8UwAgPOFuxYJAITDYdx33334/Oc/j09/+tNob2/Hq6++in/5l39JSp/lfe372cd9x3w6JL/bk4ckrfb5+fkoLy/HsWPH0NjYiPr6eiwvLyMej6O7uxvt7e0YGRkxIczSsGX7lG1d3m6vhOyuklW2OmRHDkrG6UNHQRAcgJHyxcXFqKqqwsmTJ1FfX2+2HpuamkJ7ezuuXLmCwcFBI+GXl5eTgpJcZM93F7OT59rqk7SZ+JDWrl270NzcjNOnT+OZZ57BwMAAvvvd7+L111/H8PBw0tjJd+BCVJvFAIC7hAkAq/CLn4FbrfhyoobDYTz22GP4tV/7NTQ0NJ5RRQkAABRDSURBVODs2bM4c+YMLly4kJRzYOunQXA3FdNwTTjXBJRMi58JLSORCCKRCPbs2YOGhgazyenk5CTa2towNDSE7u5uY2OQiUV8niuhSbr97DbYaMhGCK7FZKMM+Rt99Pn5+SgpKUFpaSkaGxvR1NSE6upqhEIhLC4uoru7G4ODg+ju7sbw8LDJ6CSDczEcn63C9058Et71jnzvLy8vD5FIBCdPnsSv//qvY3l5Gc8//zzOnj1rXMNsr0t9tcd6MxkAcJcxATt81P6dk1UurIKCAkQiETzzzDN44oknEAqF8O677+Lll19Gd3d3klvR59+3nykXhWsB+e7hYl72Z5kMZG8AEgqFUFRUhLq6OtTX16OiogIVFRUoKiqCUsoUC5mcnMTIyAhmZmaMt4IMgzHx/JOJPnJx8ZkyqCgvLw8FBQUmTJnGusrKShO2nJWV2LH4448/xtjYGIaGhhCPx03BVpkrICseuSIm7XfuktAum4kkn64v34kLyeXm5mLPnj146KGHcPLkSUSjUVy8eBHf+c530N3dbTYtSSUMXHYKKbA2g+4aJkAitw0yjvE//7Kysow0eu6553Ds2DHMzs7i1VdfxYULFzA4OIjZ2VmvhAmCkz4EEfTdB12lYVAyGsJVGcUnfeTc24BbipWWlhorOrcGk0lGNvKhsc4VPGO3j7UOuLvy5OSksWfMzs6aDULkFu6McJQMx0Zg8rm+d2qPpWuOS2hv20Hsa1zPy8vLM6rKI488gt27d6OrqwsvvPACrl69ioGBgaQiML52uRCLUiopNHmz6K5jAlItSAfCy3O4AWZlZSX279+PY8eOob6+HnNzc2hra8Ply5fR2tqaVBGY19rj6UMArnF3/WZLC1+7Xfey9XP+yVwCFumgD91n9LLbYFur7Tbzj9JMFlSVSUG2LSFInfAtcpt8Xh3XtTaq8y1apnE3NDTg8OHDOHToEPLz8zE8PIy2tjZ0dnZidHQUQ0NDZl4EzTebZL83GwGQ7jomAKwyAtfkCVpswKrVmNbdw4cP4/Dhw4hEIvj444/x0UcfoaWlBR9++KHZFccmlw0hVUKMPGafE4Q6fJM91TFbl3YxMRfycfXNhbpsZOPzkLgWOZDMwHz94fEgfV6eJ9vmmxuyH6wh0NTUhPr6emRlZRlVqrOzE11dXYjH497Fa6OmoHNuFwMA7lImANy6VwHg17nly5fShBuRNjQ0oKGhAaWlpcjKysL8/Lx5cQMDA+jp6cHs7KyBtK6JLeMWpHSWsN63qOz2p+qP/VyXqmEvWJdEt59lS2+bObjUhCBp67uXvRjtNtjkU9Nc5/nGmYFHLDlXXl6O7OxsTE9Pm6rVU1NTGBoaMqHf9MbYNgjJmFw2C/vZALxbqW0G7dhtyDZKroXkg4e2lCDNz8+bOoDvv/++iT2vqanBvn37TEx6XV0dlEpspDozM4OpqakkpiDJzmm3kYjdDptJ2ceC+hPUb/t+Ljgu7+da3C49OohciMb3374mHWnv65uvX9wDsbi4GKFQyFRnYvjx3NwcOjo6EI/Hbwk397XX1Qa773Z/tkog3/FIAHBDPBdJHZrfXdLR/qxUYuuoI0eOoK6uDkVFRZibm8P4+DjGxsYwNTWFyclJUwMwCIm42upDBOm8O1vflmPiY5D24g6CsfI8aQW3mYc8336m/SwXcwvS64P67noW/zMWgeHF3C5ufn4e/f39aG1txcTEhKnLIMclHTRmt8Wn+vEdbXadAJvuWiQAuK2wLkoHVrsQg9YaIyMjGB0dRX5+vvHdR6NR1NbWIh6Po7e310T6yRx6lxpgt93+nM7klyqNq+2+vvl+893HXuA+5pGqzfZ5Psm9HhTAdrFaUG5urolPiEajqK6uhlIK8XjcFC8lgtP61qw+Vw4E36Mr9iKo/zz3djOAILorkICkoPh3X3iwfb5r0UpGIydEfn4+qqqq0NzcjIaGBlRWVqK/vx+Dg4MYGBjAjRs3TE18lw4u720fc33n4pcuMFd/ASS5+mzy9deW/Pb5tn5vt811fx/ET4dxyPbJa+TCZzoxt5E/cOAAcnNz0dHRgUuXLplyYXbFINeCdj1XMtygueEbo9tpB5B01xoGbeLLTYeCYLhvQtvnyXtQ8pw4cQJ79uxBdnY24vE42traMDIyYpCCzAGQ7XBBdZcESsVI1morCOqnyx3nk4r2vVz9CXqW7x58FrBqCKakD4fDqKurw+7du1FSUoLR0VGcP38efX19GBkZScqiDFIbfaqTfY5dKyEVfVIMAMgwgSRyxcy7FknQy3SpBfKzTxeXxPLaTU1NRn3Iz8/H2NgY+vv70dnZaQp+urL+XPdORycNmrx2/wF/hmbQ83xtkNenQiA+hMA2kbinH9Wv/fv3IysrC2NjY2htbcX169dNoJIvucp+tn3cx1jTOd+HGD9pFSDDBCxywVsXE5DH7evt3+17plqgcpExJ4CTeM+ePYhGo9Bao7u7G319fejr68P09LSJsLPDUYPsF6n00iA93/XZ98wgWgsT4H+70IdSq9b8yspKVFdXo7m5GWVlZWZLuUuXLqGzs9NAfNf92R6XEThIBZDHea6tEviYgFSjtmLdZZiARbb05sv0RZClszBc9+O19iST18k8AHvC5+TkIBQKmW3S6uvrUVVVheXlZUxNTWF4eBiDg4OYmJgwHgipUvD5Qf1wVej1wd4gmJtKf7bJZTNgn7OyssyGKpFIxERwFhYWYmFhAUNDQ2hvbze7NdlqlEwRlovUV8fAfieu92S33cUwXPexj29WfYC1UoYJOMiFBnjchw7kMdsAlIpsiRskEW2SuQAlJSWorq5GVVUVwuGwKaqRlZXYsScWi2F8fByjo6OYmJgwngi5570LlQQhH9l+m1wMI1WflFImdJlbdxcXFyMcDiMajaKgoADZ2dlYWlrC+Pg4xsfHMTQ0hNHRUbOjL/tkJxW5MgldqpuPXOgqaEzkfV1BWXIObRUKAO5yF6GPfC/Yx93ly1zL4rcpXfTA/ywDxvj72dlZxGIxk7XHoBYiCe5KxFp2TBbKy8tL2iNgYWEBc3NzWFhYMJFv9FYwvp9/0uAn2y93QpZ/bBe3MC8qKjJ7HGidyCVgMhEDq6anpzE0NITz58+bc7Re3WNxfn4+yZMixzBIVfEx9HQYlY98NgKXUNlqNSAV3dVMAFjlzr5UVHvSb9biZ7afnKg+/Z5toNtveXnZ7FMor+X9mRDEclvSN27H4rMtJFk0lAzDTi5iOyRjYJtYkEUWJCEjk9+Xl5fN4pbMiHszuAKP+Jlt8Kk5QYgllQ1FUjpo0DX+LiGyXRkAsP5tyL4PoHnllDIAE1rr40qpBgAfAvho5bf3tNZf2+xGbza5GEFQQo3vHjZq8EkLINjiLCeYq5oxr5ckFyVrAHCPex/JIBgyCBYU5Z9tp5AFPOQzmf5LJiVTgnksyBpuLyRb5UqlhgTdl+cGLUL72UFowYUoXPYB0nZmAMA6tyHTWv87flZK/TmASXF+u9b6+GY18JMiTmQg2CLuMvxIWitSSEfPTofsRSMXlW8CysnJ1Nd0C6bY+raLOUkEELSobHXLJfntZ/ratdbF5uqHr60u25HrOpLPjbvdaEPbkKnEKPwygCc3t1lbQy74F7TgXeRbgD4pkc5v6TIEaRgL8ofb17ikr69fQXXx7Xa4Mutc16Qr6deCnHz3kPdxLXgf8rPngc2o7GNb5QFYD6UXOuenRwEMa62vi2ONSqnzSqk3lVKPbvD+W0LyBbomq2th+qRXEG1EQqSCn3bbJJRPBbFdf/zNtgNIKe5SHXzkev5a0I/df/u39d436D7pPEOqSjuFNmoY/BUA3xXfBwHs0VrHlVInAPyTUuoerfWUfaHahG3IbifJ2vv2C/fVMPRJriDdUZ5rSyYfCkilv9rw1gdx07m/7z68lwspuZiQ/G9LapdUTdUm+zd5ra+6cTr3DDqWimnzvK1MBloPrRsJKKWyAfxbAN/nMa31vNY6vvL5AwDtAA66rtda/5XW+qTW+qSr8Md2IBc3D9IZpZS0r+HvQZRKd3bdYy33dLVLLlD5Z/fF/iz7FCSVXRS0D4CvD3Z/1iPd7fFNpW74+m/3XdLi4uKa27XVtBF14LMAWrTWfTyglAorpXatfN4H4ACAjo01cWvJx9VTTfy1LAo5ubhAXFlptgHN9SyXiuCD/0ELyfccX1ls33isVW0K0snl+PjQQroMKdW7842173zGU+xESskEVGIbsp8AaFZK9Smlfmflp+eQrAoAwGMALimlLgD4AYCvaa3HNrPBW0G3o/JrKpITcC367+1uh3zuep8dJI1dz0+nbelS0DU+1ShVW3aaDcCmdLwDv+I5/luOY/8A4B823qztR9J9GKS+pAPn7UWVKuVWftZamyAjHwIIaoPrmbauHrRIfKpFOsfS0auDFrVvzwFfO4NQyHpVCl4L3L6qwJ80bT+L3DYke/LcvHkzsJx5OpNdkm8B+wx+vmIeqdqe6nzbcCm/BxlA0+2vjznZbfItVtdz0lHJfIwiVbtpZHRJ+TuFAQAbdxHelUT454LsQdLFtZh8erfNGGwLu022YS+VlFvvBF6v9EznmdT5fbq9jSpcY+Kr8ux6lut/Om2+kxgAkEEC6ya5UF3WbluarQV+us5LFclnX5+OhAyCzD4Do/3besi+3h6rIEpXb/fd04Uy0jX+7ZQIwLVSBglsgLi4fcUkeM5a7peK0pXw6zGapfO8ILUjXSNfkCsu6Lp0VK5UTML1m60Cue6ZLsLYiZRhAptAZARBufrp3IMU5M5bi6cgFaQOolRqSyrayGJxqTS+xZnOM13npWI60li63ROANkoZJrBJ5JosLqmcaqGuZcG5GIXL87CWewS1ba3X++4TZNH3Xe87li7DSNUm3+87KQdgvZRhAptMQaXDgfVLSN9CsJlOOm66IB18rZDXd346kjsd2ghTXO89bTXvTqeMYfA2ENUCFueQZH9fi2FOfk/H8Gd/DnpuELm8Ey7mEXTPVL+lkvbp0Eb9/sD2z/2/HZRBAreRXNtwy//rJZ8LzWdLWC/kD1Jj7PNSnZPq2rXcYyNqigs9AUgqo3a3UQYJfAKktTalr7OzswOlpst9FwS31xMPkErCpoLyNkoJul+QOzCojfb5qWwKvmtTLeqdGu+/mZRhAp8wkRn4pHZQTMFaYg2CaK1GNNf1dlt8jGAz2rtZJJlCZvGvUoYJbBFxEspaCmuR0EEMJB2yn5XKT25f5/t9Lc/2tcX3nHTPsc+X58nFv1abw51KGSawxSR3yFFKrTnXntels5Dkgk8VbLMRndvXjnQDfNbibUjFUO4GF99GKWMY3EbkcktxwUobwHqYBO/l063TlbDreX46lI7P3v6cilIxgNvVl51GGSSwTciGrJRy6eygnK5FPdWET8eQJp+XblCOS+VYrxU+lavRF7WZIT9lmMA2JCnt7ESltZBtaNyoYdFevHbU3kZ0bBuJ2OjHZTCVRj6b0WT0/fQpwwS2CaXj15aLQdoOfEY+1zM2wgjSDVDayH3kOZKx2IjCLnOWjisyQ27KMIEdRLZbK5U7LigSz2WM26gEl99TLXQp7X12CtsOIBlDxti3eZRhAjuUfJWQfcfWq4/b6CEoaCfItWkjGpLPuu9z62Vo8ynDBO4gku5GYDUGIcglmC4FLX6fS89e0D6d3r7O7keGbi9lmMAdTL7FpJRy1kj0Sep0bA0+spGE9N9nLPjbgzJM4C4kmctgH1/LblA243AZMG0on9Hltx+p7cCNlVIxALMARre6LbeBqnBn9gu4c/t2p/Zrr9Y6bB/cFkwAAJRS72utT251Ozab7tR+AXdu3+7UfvkoEzacoQzd5ZRhAhnK0F1O24kJ/NVWN+A20Z3aL+DO7dud2i8nbRubQIYylKGtoe2EBDKUoQxtAW05E1BKfU4p9ZFSqk0p9Udb3Z6NklKqSyl1WSl1QSn1/sqxCqXUK0qp6yv/y7e6nalIKfW8UmpEKXVFHHP2QyXof628w0tKqQe2ruWpydO3P1FK9a+8twtKqS+I376x0rePlFJPbU2rbx9tKRNQSu0C8C0AnwdwBMCvKKWObGWbNome0FofF26mPwLwmtb6AIDXVr5vd/o2gM9Zx3z9+DyAAyt/vwfgLz+hNq6Xvo1b+wYAf7Hy3o5rrX8EACvz8TkA96xc839W5u0dQ1uNBB4E0Ka17tBaLwD4HoBnt7hNt4OeBfA3K5//BsC/2cK2pEVa67cAjFmHff14FsDf6gS9B6BMKRX5ZFq6dvL0zUfPAvie1npea90JoA2JeXvH0FYzgVoAveJ738qxnUwawBml1AdKqd9bOVajtR5c+TwEoGZrmrZh8vXjTnmPf7CizjwvVLY7pW9e2momcCfSp7XWDyABkb+ulHpM/qgT7pgd75K5U/oh6C8B7AdwHMAggD/f2uZ8crTVTKAfQL34XrdybMeS1rp/5f8IgB8iAR2HCY9X/o9sXQs3RL5+7Pj3qLUe1lrf1FovA/hrrEL+Hd+3VLTVTOBnAA4opRqVUrlIGGD+eYvbtG5SShUppYr5GcC/AnAFiT795sppvwng/21NCzdMvn78M4DfWPESPAxgUqgNO4IsG8YXkXhvQKJvzyml8pRSjUgYP3/6SbfvdtKWphJrrZeUUn8A4GUAuwA8r7W+upVt2iDVAPjhSs58NoC/01r/WCn1MwB/r5T6HQDdAH55C9uYFimlvgvgNIAqpVQfgP8K4L/D3Y8fAfgCEkazjwH89ife4DWQp2+nlVLHkVBxugD8PgBora8qpf4ewDUASwC+rrW+o0odZSIGM5Shu5y2Wh3IUIYytMWUYQIZytBdThkmkKEM3eWUYQIZytBdThkmkKEM3eWUYQIZytBdThkmkKEM3eWUYQIZytBdTv8fdq/X6tuXEnsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: Malignant Case\n",
      "Prediction: Malignant cases\n"
     ]
    }
   ],
   "source": [
    "from skimage import io\n",
    "from keras.preprocessing import image\n",
    "\n",
    "best_model=models.load_model(\"DenseNet201.h5\")\n",
    "\n",
    "img = image.load_img(lngdir+'/Malignant cases/Malignant case (1).jpg', grayscale=False, target_size=(imsz,imsz))\n",
    "show_img=image.load_img(lngdir+'/Malignant cases/Malignant case (1).jpg', grayscale=False, target_size=(200, 200))\n",
    "disease_class = classes\n",
    "x = image.img_to_array(img)\n",
    "x = numpy.expand_dims(x, axis = 0)\n",
    "x /= 255\n",
    "\n",
    "custom = best_model.predict(x)\n",
    "print(custom[0])\n",
    "pyplot.imshow(show_img)\n",
    "pyplot.show()\n",
    "\n",
    "a=custom[0]\n",
    "ind=numpy.argmax(a)\n",
    "print(\"Actual: Malignant Case\")    \n",
    "print('Prediction:',disease_class[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWnBbHm9KHLO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "duration": 27470.503163,
   "end_time": "2020-10-22T22:51:12.074002",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-22T15:13:21.570839",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
